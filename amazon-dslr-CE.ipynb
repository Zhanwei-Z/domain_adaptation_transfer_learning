{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "step: 0,\t,class_loss:3.6211,\t\n",
      "step: 1,\t,class_loss:3.5711,\t\n",
      "step: 2,\t,class_loss:3.4839,\t\n",
      "step: 3,\t,class_loss:3.3255,\t\n",
      "step: 4,\t,class_loss:3.6087,\t\n",
      "step: 5,\t,class_loss:3.3213,\t\n",
      "step: 6,\t,class_loss:3.5033,\t\n",
      "step: 7,\t,class_loss:3.5279,\t\n",
      "step: 8,\t,class_loss:3.3490,\t\n",
      "step: 9,\t,class_loss:3.5655,\t\n",
      "step: 10,\t,class_loss:3.4399,\t\n",
      "step: 11,\t,class_loss:3.4043,\t\n",
      "step: 12,\t,class_loss:3.4197,\t\n",
      "step: 13,\t,class_loss:3.3221,\t\n",
      "step: 14,\t,class_loss:3.4284,\t\n",
      "step: 15,\t,class_loss:3.2974,\t\n",
      "step: 16,\t,class_loss:3.2481,\t\n",
      "step: 17,\t,class_loss:3.2491,\t\n",
      "step: 18,\t,class_loss:3.0499,\t\n",
      "step: 19,\t,class_loss:3.2074,\t\n",
      "step: 20,\t,class_loss:3.1360,\t\n",
      "step: 21,\t,class_loss:3.1376,\t\n",
      "step: 22,\t,class_loss:2.9847,\t\n",
      "step: 23,\t,class_loss:3.3060,\t\n",
      "step: 24,\t,class_loss:3.0913,\t\n",
      "step: 25,\t,class_loss:3.1680,\t\n",
      "step: 26,\t,class_loss:3.0748,\t\n",
      "step: 27,\t,class_loss:3.1650,\t\n",
      "step: 28,\t,class_loss:2.9017,\t\n",
      "step: 29,\t,class_loss:3.0316,\t\n",
      "step: 30,\t,class_loss:3.0308,\t\n",
      "step: 31,\t,class_loss:2.9781,\t\n",
      "step: 32,\t,class_loss:3.0668,\t\n",
      "step: 33,\t,class_loss:2.7924,\t\n",
      "step: 34,\t,class_loss:2.9125,\t\n",
      "step: 35,\t,class_loss:3.0063,\t\n",
      "step: 36,\t,class_loss:2.7515,\t\n",
      "step: 37,\t,class_loss:3.0582,\t\n",
      "step: 38,\t,class_loss:2.7016,\t\n",
      "step: 39,\t,class_loss:2.7088,\t\n",
      "step: 40,\t,class_loss:3.0423,\t\n",
      "step: 41,\t,class_loss:2.8240,\t\n",
      "step: 42,\t,class_loss:2.6312,\t\n",
      "step: 43,\t,class_loss:2.8911,\t\n",
      "step: 44,\t,class_loss:2.6684,\t\n",
      "step: 45,\t,class_loss:2.6321,\t\n",
      "step: 46,\t,class_loss:2.6547,\t\n",
      "step: 47,\t,class_loss:2.5819,\t\n",
      "step: 48,\t,class_loss:2.6142,\t\n",
      "iter: 00049, \t precision: 0.3655,\t best_acc:0.3655\n",
      "step: 49,\t,class_loss:2.4602,\t\n",
      "step: 50,\t,class_loss:2.6356,\t\n",
      "step: 51,\t,class_loss:2.4620,\t\n",
      "step: 52,\t,class_loss:2.6620,\t\n",
      "step: 53,\t,class_loss:2.7783,\t\n",
      "step: 54,\t,class_loss:2.2241,\t\n",
      "step: 55,\t,class_loss:2.2595,\t\n",
      "step: 56,\t,class_loss:2.3186,\t\n",
      "step: 57,\t,class_loss:2.3323,\t\n",
      "step: 58,\t,class_loss:2.5902,\t\n",
      "step: 59,\t,class_loss:2.6598,\t\n",
      "step: 60,\t,class_loss:2.3360,\t\n",
      "step: 61,\t,class_loss:2.4611,\t\n",
      "step: 62,\t,class_loss:2.6352,\t\n",
      "step: 63,\t,class_loss:2.4541,\t\n",
      "step: 64,\t,class_loss:2.1525,\t\n",
      "step: 65,\t,class_loss:2.0323,\t\n",
      "step: 66,\t,class_loss:2.1538,\t\n",
      "step: 67,\t,class_loss:2.1247,\t\n",
      "step: 68,\t,class_loss:2.4470,\t\n",
      "step: 69,\t,class_loss:2.1094,\t\n",
      "step: 70,\t,class_loss:1.9580,\t\n",
      "step: 71,\t,class_loss:2.2745,\t\n",
      "step: 72,\t,class_loss:2.0148,\t\n",
      "step: 73,\t,class_loss:2.1496,\t\n",
      "step: 74,\t,class_loss:2.0004,\t\n",
      "step: 75,\t,class_loss:2.4087,\t\n",
      "step: 76,\t,class_loss:2.0713,\t\n",
      "step: 77,\t,class_loss:2.0726,\t\n",
      "step: 78,\t,class_loss:2.0372,\t\n",
      "step: 79,\t,class_loss:2.1186,\t\n",
      "step: 80,\t,class_loss:2.0870,\t\n",
      "step: 81,\t,class_loss:2.1660,\t\n",
      "step: 82,\t,class_loss:1.9101,\t\n",
      "step: 83,\t,class_loss:2.1349,\t\n",
      "step: 84,\t,class_loss:1.5528,\t\n",
      "step: 85,\t,class_loss:1.7587,\t\n",
      "step: 86,\t,class_loss:2.2807,\t\n",
      "step: 87,\t,class_loss:2.0218,\t\n",
      "step: 88,\t,class_loss:1.4623,\t\n",
      "step: 89,\t,class_loss:1.3570,\t\n",
      "step: 90,\t,class_loss:1.6891,\t\n",
      "step: 91,\t,class_loss:1.5289,\t\n",
      "step: 92,\t,class_loss:1.5500,\t\n",
      "step: 93,\t,class_loss:1.8713,\t\n",
      "step: 94,\t,class_loss:1.5317,\t\n",
      "step: 95,\t,class_loss:1.8387,\t\n",
      "step: 96,\t,class_loss:1.5196,\t\n",
      "step: 97,\t,class_loss:1.3784,\t\n",
      "step: 98,\t,class_loss:1.6505,\t\n",
      "iter: 00099, \t precision: 0.5482,\t best_acc:0.5482\n",
      "step: 99,\t,class_loss:1.3633,\t\n",
      "step: 100,\t,class_loss:1.8184,\t\n",
      "step: 101,\t,class_loss:1.8536,\t\n",
      "step: 102,\t,class_loss:1.4469,\t\n",
      "step: 103,\t,class_loss:1.7815,\t\n",
      "step: 104,\t,class_loss:1.1780,\t\n",
      "step: 105,\t,class_loss:1.3084,\t\n",
      "step: 106,\t,class_loss:1.2677,\t\n",
      "step: 107,\t,class_loss:1.4970,\t\n",
      "step: 108,\t,class_loss:1.3823,\t\n",
      "step: 109,\t,class_loss:1.3596,\t\n",
      "step: 110,\t,class_loss:1.2255,\t\n",
      "step: 111,\t,class_loss:1.5589,\t\n",
      "step: 112,\t,class_loss:1.5669,\t\n",
      "step: 113,\t,class_loss:1.9924,\t\n",
      "step: 114,\t,class_loss:1.6386,\t\n",
      "step: 115,\t,class_loss:1.3157,\t\n",
      "step: 116,\t,class_loss:1.4564,\t\n",
      "step: 117,\t,class_loss:1.4779,\t\n",
      "step: 118,\t,class_loss:2.2151,\t\n",
      "step: 119,\t,class_loss:1.3172,\t\n",
      "step: 120,\t,class_loss:1.5634,\t\n",
      "step: 121,\t,class_loss:1.5374,\t\n",
      "step: 122,\t,class_loss:1.6502,\t\n",
      "step: 123,\t,class_loss:1.1430,\t\n",
      "step: 124,\t,class_loss:1.5504,\t\n",
      "step: 125,\t,class_loss:2.0264,\t\n",
      "step: 126,\t,class_loss:1.3884,\t\n",
      "step: 127,\t,class_loss:1.4278,\t\n",
      "step: 128,\t,class_loss:1.2934,\t\n",
      "step: 129,\t,class_loss:1.6411,\t\n",
      "step: 130,\t,class_loss:1.2661,\t\n",
      "step: 131,\t,class_loss:0.8967,\t\n",
      "step: 132,\t,class_loss:1.4596,\t\n",
      "step: 133,\t,class_loss:1.1462,\t\n",
      "step: 134,\t,class_loss:0.8213,\t\n",
      "step: 135,\t,class_loss:1.6041,\t\n",
      "step: 136,\t,class_loss:1.2667,\t\n",
      "step: 137,\t,class_loss:1.4460,\t\n",
      "step: 138,\t,class_loss:1.6866,\t\n",
      "step: 139,\t,class_loss:1.0138,\t\n",
      "step: 140,\t,class_loss:1.4316,\t\n",
      "step: 141,\t,class_loss:0.9788,\t\n",
      "step: 142,\t,class_loss:1.0321,\t\n",
      "step: 143,\t,class_loss:0.9494,\t\n",
      "step: 144,\t,class_loss:1.2753,\t\n",
      "step: 145,\t,class_loss:1.2238,\t\n",
      "step: 146,\t,class_loss:1.7355,\t\n",
      "step: 147,\t,class_loss:1.2272,\t\n",
      "step: 148,\t,class_loss:1.1052,\t\n",
      "iter: 00149, \t precision: 0.6546,\t best_acc:0.6546\n",
      "step: 149,\t,class_loss:1.0972,\t\n",
      "step: 150,\t,class_loss:0.7288,\t\n",
      "step: 151,\t,class_loss:0.6032,\t\n",
      "step: 152,\t,class_loss:1.4755,\t\n",
      "step: 153,\t,class_loss:1.1933,\t\n",
      "step: 154,\t,class_loss:0.8209,\t\n",
      "step: 155,\t,class_loss:0.7099,\t\n",
      "step: 156,\t,class_loss:1.0671,\t\n",
      "step: 157,\t,class_loss:0.8001,\t\n",
      "step: 158,\t,class_loss:1.0791,\t\n",
      "step: 159,\t,class_loss:1.2850,\t\n",
      "step: 160,\t,class_loss:1.4056,\t\n",
      "step: 161,\t,class_loss:1.0339,\t\n",
      "step: 162,\t,class_loss:0.9825,\t\n",
      "step: 163,\t,class_loss:0.8973,\t\n",
      "step: 164,\t,class_loss:0.5621,\t\n",
      "step: 165,\t,class_loss:1.0676,\t\n",
      "step: 166,\t,class_loss:1.1334,\t\n",
      "step: 167,\t,class_loss:1.1509,\t\n",
      "step: 168,\t,class_loss:0.9592,\t\n",
      "step: 169,\t,class_loss:1.2848,\t\n",
      "step: 170,\t,class_loss:0.7876,\t\n",
      "step: 171,\t,class_loss:0.6297,\t\n",
      "step: 172,\t,class_loss:0.7807,\t\n",
      "step: 173,\t,class_loss:0.8147,\t\n",
      "step: 174,\t,class_loss:1.0869,\t\n",
      "step: 175,\t,class_loss:0.9950,\t\n",
      "step: 176,\t,class_loss:0.4609,\t\n",
      "step: 177,\t,class_loss:0.5998,\t\n",
      "step: 178,\t,class_loss:0.9195,\t\n",
      "step: 179,\t,class_loss:0.4586,\t\n",
      "step: 180,\t,class_loss:0.9785,\t\n",
      "step: 181,\t,class_loss:0.5839,\t\n",
      "step: 182,\t,class_loss:1.1353,\t\n",
      "step: 183,\t,class_loss:0.8998,\t\n",
      "step: 184,\t,class_loss:0.9735,\t\n",
      "step: 185,\t,class_loss:0.8787,\t\n",
      "step: 186,\t,class_loss:0.8112,\t\n",
      "step: 187,\t,class_loss:1.0725,\t\n",
      "step: 188,\t,class_loss:0.6791,\t\n",
      "step: 189,\t,class_loss:1.4323,\t\n",
      "step: 190,\t,class_loss:0.7005,\t\n",
      "step: 191,\t,class_loss:0.9255,\t\n",
      "step: 192,\t,class_loss:0.7040,\t\n",
      "step: 193,\t,class_loss:1.2896,\t\n",
      "step: 194,\t,class_loss:1.3810,\t\n",
      "step: 195,\t,class_loss:0.5912,\t\n",
      "step: 196,\t,class_loss:1.0121,\t\n",
      "step: 197,\t,class_loss:0.8284,\t\n",
      "step: 198,\t,class_loss:0.7712,\t\n",
      "iter: 00199, \t precision: 0.6847,\t best_acc:0.6847\n",
      "step: 199,\t,class_loss:0.8121,\t\n",
      "step: 200,\t,class_loss:0.6092,\t\n",
      "step: 201,\t,class_loss:1.5356,\t\n",
      "step: 202,\t,class_loss:1.0159,\t\n",
      "step: 203,\t,class_loss:1.0107,\t\n",
      "step: 204,\t,class_loss:0.8024,\t\n",
      "step: 205,\t,class_loss:0.6331,\t\n",
      "step: 206,\t,class_loss:0.8207,\t\n",
      "step: 207,\t,class_loss:0.7919,\t\n",
      "step: 208,\t,class_loss:0.8946,\t\n",
      "step: 209,\t,class_loss:0.5322,\t\n",
      "step: 210,\t,class_loss:0.8879,\t\n",
      "step: 211,\t,class_loss:0.8014,\t\n",
      "step: 212,\t,class_loss:0.6466,\t\n",
      "step: 213,\t,class_loss:0.9004,\t\n",
      "step: 214,\t,class_loss:0.9674,\t\n",
      "step: 215,\t,class_loss:1.0318,\t\n",
      "step: 216,\t,class_loss:0.3546,\t\n",
      "step: 217,\t,class_loss:0.8738,\t\n",
      "step: 218,\t,class_loss:1.0210,\t\n",
      "step: 219,\t,class_loss:0.5043,\t\n",
      "step: 220,\t,class_loss:0.4162,\t\n",
      "step: 221,\t,class_loss:1.0656,\t\n",
      "step: 222,\t,class_loss:0.6443,\t\n",
      "step: 223,\t,class_loss:0.6920,\t\n",
      "step: 224,\t,class_loss:0.9454,\t\n",
      "step: 225,\t,class_loss:0.7683,\t\n",
      "step: 226,\t,class_loss:0.8083,\t\n",
      "step: 227,\t,class_loss:1.3377,\t\n",
      "step: 228,\t,class_loss:0.2595,\t\n",
      "step: 229,\t,class_loss:0.5804,\t\n",
      "step: 230,\t,class_loss:1.1654,\t\n",
      "step: 231,\t,class_loss:0.8154,\t\n",
      "step: 232,\t,class_loss:0.4207,\t\n",
      "step: 233,\t,class_loss:0.7201,\t\n",
      "step: 234,\t,class_loss:0.8363,\t\n",
      "step: 235,\t,class_loss:0.7820,\t\n",
      "step: 236,\t,class_loss:0.8557,\t\n",
      "step: 237,\t,class_loss:0.9098,\t\n",
      "step: 238,\t,class_loss:0.5632,\t\n",
      "step: 239,\t,class_loss:0.9855,\t\n",
      "step: 240,\t,class_loss:0.6183,\t\n",
      "step: 241,\t,class_loss:1.2615,\t\n",
      "step: 242,\t,class_loss:1.0890,\t\n",
      "step: 243,\t,class_loss:0.5770,\t\n",
      "step: 244,\t,class_loss:0.9459,\t\n",
      "step: 245,\t,class_loss:0.8989,\t\n",
      "step: 246,\t,class_loss:0.6693,\t\n",
      "step: 247,\t,class_loss:0.7166,\t\n",
      "step: 248,\t,class_loss:0.5806,\t\n",
      "iter: 00249, \t precision: 0.6807,\t best_acc:0.6847\n",
      "step: 249,\t,class_loss:0.5893,\t\n",
      "step: 250,\t,class_loss:1.1177,\t\n",
      "step: 251,\t,class_loss:0.7446,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 252,\t,class_loss:0.4355,\t\n",
      "step: 253,\t,class_loss:0.9386,\t\n",
      "step: 254,\t,class_loss:0.7386,\t\n",
      "step: 255,\t,class_loss:1.3248,\t\n",
      "step: 256,\t,class_loss:1.4425,\t\n",
      "step: 257,\t,class_loss:0.7961,\t\n",
      "step: 258,\t,class_loss:0.6981,\t\n",
      "step: 259,\t,class_loss:0.8180,\t\n",
      "step: 260,\t,class_loss:0.5777,\t\n",
      "step: 261,\t,class_loss:0.9423,\t\n",
      "step: 262,\t,class_loss:0.8110,\t\n",
      "step: 263,\t,class_loss:0.6615,\t\n",
      "step: 264,\t,class_loss:0.7535,\t\n",
      "step: 265,\t,class_loss:0.6083,\t\n",
      "step: 266,\t,class_loss:0.4698,\t\n",
      "step: 267,\t,class_loss:0.5188,\t\n",
      "step: 268,\t,class_loss:0.7607,\t\n",
      "step: 269,\t,class_loss:0.9340,\t\n",
      "step: 270,\t,class_loss:0.7027,\t\n",
      "step: 271,\t,class_loss:0.7894,\t\n",
      "step: 272,\t,class_loss:0.4854,\t\n",
      "step: 273,\t,class_loss:0.8080,\t\n",
      "step: 274,\t,class_loss:0.6794,\t\n",
      "step: 275,\t,class_loss:1.0757,\t\n",
      "step: 276,\t,class_loss:0.6576,\t\n",
      "step: 277,\t,class_loss:0.7945,\t\n",
      "step: 278,\t,class_loss:1.0014,\t\n",
      "step: 279,\t,class_loss:0.9217,\t\n",
      "step: 280,\t,class_loss:0.3704,\t\n",
      "step: 281,\t,class_loss:0.3575,\t\n",
      "step: 282,\t,class_loss:0.4505,\t\n",
      "step: 283,\t,class_loss:0.5128,\t\n",
      "step: 284,\t,class_loss:0.5720,\t\n",
      "step: 285,\t,class_loss:0.7137,\t\n",
      "step: 286,\t,class_loss:0.5767,\t\n",
      "step: 287,\t,class_loss:0.6849,\t\n",
      "step: 288,\t,class_loss:0.6581,\t\n",
      "step: 289,\t,class_loss:0.6428,\t\n",
      "step: 290,\t,class_loss:0.3789,\t\n",
      "step: 291,\t,class_loss:0.8290,\t\n",
      "step: 292,\t,class_loss:1.1433,\t\n",
      "step: 293,\t,class_loss:0.6643,\t\n",
      "step: 294,\t,class_loss:0.3764,\t\n",
      "step: 295,\t,class_loss:0.4825,\t\n",
      "step: 296,\t,class_loss:1.0381,\t\n",
      "step: 297,\t,class_loss:0.9169,\t\n",
      "step: 298,\t,class_loss:0.8290,\t\n",
      "iter: 00299, \t precision: 0.6988,\t best_acc:0.6988\n",
      "step: 299,\t,class_loss:1.1303,\t\n",
      "step: 300,\t,class_loss:0.7955,\t\n",
      "step: 301,\t,class_loss:0.5232,\t\n",
      "step: 302,\t,class_loss:1.0804,\t\n",
      "step: 303,\t,class_loss:0.8617,\t\n",
      "step: 304,\t,class_loss:0.7680,\t\n",
      "step: 305,\t,class_loss:0.7679,\t\n",
      "step: 306,\t,class_loss:0.8161,\t\n",
      "step: 307,\t,class_loss:0.5275,\t\n",
      "step: 308,\t,class_loss:0.8415,\t\n",
      "step: 309,\t,class_loss:0.6439,\t\n",
      "step: 310,\t,class_loss:0.9935,\t\n",
      "step: 311,\t,class_loss:0.8885,\t\n",
      "step: 312,\t,class_loss:0.8457,\t\n",
      "step: 313,\t,class_loss:0.6599,\t\n",
      "step: 314,\t,class_loss:0.7194,\t\n",
      "step: 315,\t,class_loss:0.4807,\t\n",
      "step: 316,\t,class_loss:0.5190,\t\n",
      "step: 317,\t,class_loss:0.6413,\t\n",
      "step: 318,\t,class_loss:0.7477,\t\n",
      "step: 319,\t,class_loss:0.5849,\t\n",
      "step: 320,\t,class_loss:0.7795,\t\n",
      "step: 321,\t,class_loss:0.4181,\t\n",
      "step: 322,\t,class_loss:1.1167,\t\n",
      "step: 323,\t,class_loss:0.8873,\t\n",
      "step: 324,\t,class_loss:0.6525,\t\n",
      "step: 325,\t,class_loss:0.2134,\t\n",
      "step: 326,\t,class_loss:0.7429,\t\n",
      "step: 327,\t,class_loss:0.4315,\t\n",
      "step: 328,\t,class_loss:0.9523,\t\n",
      "step: 329,\t,class_loss:0.6104,\t\n",
      "step: 330,\t,class_loss:0.6074,\t\n",
      "step: 331,\t,class_loss:0.5542,\t\n",
      "step: 332,\t,class_loss:0.4561,\t\n",
      "step: 333,\t,class_loss:0.6976,\t\n",
      "step: 334,\t,class_loss:0.5370,\t\n",
      "step: 335,\t,class_loss:0.7054,\t\n",
      "step: 336,\t,class_loss:0.5976,\t\n",
      "step: 337,\t,class_loss:0.4365,\t\n",
      "step: 338,\t,class_loss:0.4852,\t\n",
      "step: 339,\t,class_loss:0.9460,\t\n",
      "step: 340,\t,class_loss:0.4917,\t\n",
      "step: 341,\t,class_loss:0.8427,\t\n",
      "step: 342,\t,class_loss:0.2616,\t\n",
      "step: 343,\t,class_loss:0.4917,\t\n",
      "step: 344,\t,class_loss:0.3852,\t\n",
      "step: 345,\t,class_loss:0.6717,\t\n",
      "step: 346,\t,class_loss:0.2727,\t\n",
      "step: 347,\t,class_loss:0.6448,\t\n",
      "step: 348,\t,class_loss:0.4028,\t\n",
      "iter: 00349, \t precision: 0.7209,\t best_acc:0.7209\n",
      "step: 349,\t,class_loss:0.3893,\t\n",
      "step: 350,\t,class_loss:0.3762,\t\n",
      "step: 351,\t,class_loss:0.3155,\t\n",
      "step: 352,\t,class_loss:0.8315,\t\n",
      "step: 353,\t,class_loss:0.8991,\t\n",
      "step: 354,\t,class_loss:0.4044,\t\n",
      "step: 355,\t,class_loss:0.6678,\t\n",
      "step: 356,\t,class_loss:0.5505,\t\n",
      "step: 357,\t,class_loss:0.4027,\t\n",
      "step: 358,\t,class_loss:0.7840,\t\n",
      "step: 359,\t,class_loss:0.3114,\t\n",
      "step: 360,\t,class_loss:0.7390,\t\n",
      "step: 361,\t,class_loss:0.6640,\t\n",
      "step: 362,\t,class_loss:0.4200,\t\n",
      "step: 363,\t,class_loss:0.9171,\t\n",
      "step: 364,\t,class_loss:0.3279,\t\n",
      "step: 365,\t,class_loss:0.6232,\t\n",
      "step: 366,\t,class_loss:1.1001,\t\n",
      "step: 367,\t,class_loss:0.4737,\t\n",
      "step: 368,\t,class_loss:0.2190,\t\n",
      "step: 369,\t,class_loss:0.6031,\t\n",
      "step: 370,\t,class_loss:0.3145,\t\n",
      "step: 371,\t,class_loss:0.2363,\t\n",
      "step: 372,\t,class_loss:0.8014,\t\n",
      "step: 373,\t,class_loss:0.6453,\t\n",
      "step: 374,\t,class_loss:0.7797,\t\n",
      "step: 375,\t,class_loss:0.4635,\t\n",
      "step: 376,\t,class_loss:0.5006,\t\n",
      "step: 377,\t,class_loss:0.6917,\t\n",
      "step: 378,\t,class_loss:0.6296,\t\n",
      "step: 379,\t,class_loss:0.7382,\t\n",
      "step: 380,\t,class_loss:0.3750,\t\n",
      "step: 381,\t,class_loss:0.5798,\t\n",
      "step: 382,\t,class_loss:0.3954,\t\n",
      "step: 383,\t,class_loss:0.8705,\t\n",
      "step: 384,\t,class_loss:0.8853,\t\n",
      "step: 385,\t,class_loss:0.6846,\t\n",
      "step: 386,\t,class_loss:0.5904,\t\n",
      "step: 387,\t,class_loss:0.8547,\t\n",
      "step: 388,\t,class_loss:0.3542,\t\n",
      "step: 389,\t,class_loss:0.5023,\t\n",
      "step: 390,\t,class_loss:0.3037,\t\n",
      "step: 391,\t,class_loss:0.2436,\t\n",
      "step: 392,\t,class_loss:0.7217,\t\n",
      "step: 393,\t,class_loss:0.7791,\t\n",
      "step: 394,\t,class_loss:0.6100,\t\n",
      "step: 395,\t,class_loss:0.6267,\t\n",
      "step: 396,\t,class_loss:0.1799,\t\n",
      "step: 397,\t,class_loss:0.1898,\t\n",
      "step: 398,\t,class_loss:0.2564,\t\n",
      "iter: 00399, \t precision: 0.7369,\t best_acc:0.7369\n",
      "step: 399,\t,class_loss:0.2335,\t\n",
      "step: 400,\t,class_loss:0.3333,\t\n",
      "step: 401,\t,class_loss:0.5206,\t\n",
      "step: 402,\t,class_loss:0.5338,\t\n",
      "step: 403,\t,class_loss:0.5840,\t\n",
      "step: 404,\t,class_loss:0.5797,\t\n",
      "step: 405,\t,class_loss:0.2948,\t\n",
      "step: 406,\t,class_loss:0.7778,\t\n",
      "step: 407,\t,class_loss:0.5629,\t\n",
      "step: 408,\t,class_loss:0.2364,\t\n",
      "step: 409,\t,class_loss:0.6273,\t\n",
      "step: 410,\t,class_loss:0.8260,\t\n",
      "step: 411,\t,class_loss:0.6118,\t\n",
      "step: 412,\t,class_loss:0.7334,\t\n",
      "step: 413,\t,class_loss:0.6824,\t\n",
      "step: 414,\t,class_loss:0.5826,\t\n",
      "step: 415,\t,class_loss:0.4563,\t\n",
      "step: 416,\t,class_loss:0.3435,\t\n",
      "step: 417,\t,class_loss:0.3908,\t\n",
      "step: 418,\t,class_loss:0.3308,\t\n",
      "step: 419,\t,class_loss:0.7782,\t\n",
      "step: 420,\t,class_loss:0.3541,\t\n",
      "step: 421,\t,class_loss:0.9933,\t\n",
      "step: 422,\t,class_loss:0.5668,\t\n",
      "step: 423,\t,class_loss:0.8503,\t\n",
      "step: 424,\t,class_loss:0.3312,\t\n",
      "step: 425,\t,class_loss:0.4990,\t\n",
      "step: 426,\t,class_loss:0.3889,\t\n",
      "step: 427,\t,class_loss:0.4038,\t\n",
      "step: 428,\t,class_loss:0.5353,\t\n",
      "step: 429,\t,class_loss:0.5047,\t\n",
      "step: 430,\t,class_loss:0.6371,\t\n",
      "step: 431,\t,class_loss:0.6604,\t\n",
      "step: 432,\t,class_loss:0.4662,\t\n",
      "step: 433,\t,class_loss:0.3529,\t\n",
      "step: 434,\t,class_loss:0.5116,\t\n",
      "step: 435,\t,class_loss:0.4374,\t\n",
      "step: 436,\t,class_loss:0.8209,\t\n",
      "step: 437,\t,class_loss:0.3166,\t\n",
      "step: 438,\t,class_loss:0.3193,\t\n",
      "step: 439,\t,class_loss:0.2386,\t\n",
      "step: 440,\t,class_loss:0.6297,\t\n",
      "step: 441,\t,class_loss:0.5355,\t\n",
      "step: 442,\t,class_loss:0.5757,\t\n",
      "step: 443,\t,class_loss:0.2017,\t\n",
      "step: 444,\t,class_loss:0.6595,\t\n",
      "step: 445,\t,class_loss:0.3487,\t\n",
      "step: 446,\t,class_loss:0.3482,\t\n",
      "step: 447,\t,class_loss:0.8987,\t\n",
      "step: 448,\t,class_loss:0.1998,\t\n",
      "iter: 00449, \t precision: 0.7490,\t best_acc:0.7490\n",
      "step: 449,\t,class_loss:0.4161,\t\n",
      "step: 450,\t,class_loss:0.6855,\t\n",
      "step: 451,\t,class_loss:0.3959,\t\n",
      "step: 452,\t,class_loss:0.6929,\t\n",
      "step: 453,\t,class_loss:0.5064,\t\n",
      "step: 454,\t,class_loss:0.3950,\t\n",
      "step: 455,\t,class_loss:0.4090,\t\n",
      "step: 456,\t,class_loss:0.3025,\t\n",
      "step: 457,\t,class_loss:0.2972,\t\n",
      "step: 458,\t,class_loss:0.4130,\t\n",
      "step: 459,\t,class_loss:0.9081,\t\n",
      "step: 460,\t,class_loss:0.6636,\t\n",
      "step: 461,\t,class_loss:0.1241,\t\n",
      "step: 462,\t,class_loss:0.4473,\t\n",
      "step: 463,\t,class_loss:0.1933,\t\n",
      "step: 464,\t,class_loss:0.3964,\t\n",
      "step: 465,\t,class_loss:0.6299,\t\n",
      "step: 466,\t,class_loss:0.7283,\t\n",
      "step: 467,\t,class_loss:0.4923,\t\n",
      "step: 468,\t,class_loss:0.6536,\t\n",
      "step: 469,\t,class_loss:0.5102,\t\n",
      "step: 470,\t,class_loss:0.3388,\t\n",
      "step: 471,\t,class_loss:0.2458,\t\n",
      "step: 472,\t,class_loss:0.6049,\t\n",
      "step: 473,\t,class_loss:0.5562,\t\n",
      "step: 474,\t,class_loss:0.3293,\t\n",
      "step: 475,\t,class_loss:0.7407,\t\n",
      "step: 476,\t,class_loss:0.5214,\t\n",
      "step: 477,\t,class_loss:0.2603,\t\n",
      "step: 478,\t,class_loss:0.5104,\t\n",
      "step: 479,\t,class_loss:0.4278,\t\n",
      "step: 480,\t,class_loss:0.3840,\t\n",
      "step: 481,\t,class_loss:0.6505,\t\n",
      "step: 482,\t,class_loss:0.2468,\t\n",
      "step: 483,\t,class_loss:0.5501,\t\n",
      "step: 484,\t,class_loss:0.4615,\t\n",
      "step: 485,\t,class_loss:0.2651,\t\n",
      "step: 486,\t,class_loss:0.2153,\t\n",
      "step: 487,\t,class_loss:0.4098,\t\n",
      "step: 488,\t,class_loss:0.4870,\t\n",
      "step: 489,\t,class_loss:0.3690,\t\n",
      "step: 490,\t,class_loss:0.4314,\t\n",
      "step: 491,\t,class_loss:0.6458,\t\n",
      "step: 492,\t,class_loss:0.4587,\t\n",
      "step: 493,\t,class_loss:0.3763,\t\n",
      "step: 494,\t,class_loss:0.5548,\t\n",
      "step: 495,\t,class_loss:0.2585,\t\n",
      "step: 496,\t,class_loss:0.1767,\t\n",
      "step: 497,\t,class_loss:0.2500,\t\n",
      "step: 498,\t,class_loss:0.4598,\t\n",
      "iter: 00499, \t precision: 0.7530,\t best_acc:0.7530\n",
      "step: 499,\t,class_loss:0.5782,\t\n",
      "step: 500,\t,class_loss:0.1562,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 501,\t,class_loss:0.3628,\t\n",
      "step: 502,\t,class_loss:0.4334,\t\n",
      "step: 503,\t,class_loss:0.6476,\t\n",
      "step: 504,\t,class_loss:0.1916,\t\n",
      "step: 505,\t,class_loss:0.3245,\t\n",
      "step: 506,\t,class_loss:0.3387,\t\n",
      "step: 507,\t,class_loss:0.6786,\t\n",
      "step: 508,\t,class_loss:0.2499,\t\n",
      "step: 509,\t,class_loss:0.3741,\t\n",
      "step: 510,\t,class_loss:0.5270,\t\n",
      "step: 511,\t,class_loss:0.2662,\t\n",
      "step: 512,\t,class_loss:0.3192,\t\n",
      "step: 513,\t,class_loss:0.2665,\t\n",
      "step: 514,\t,class_loss:0.2344,\t\n",
      "step: 515,\t,class_loss:0.4299,\t\n",
      "step: 516,\t,class_loss:0.3148,\t\n",
      "step: 517,\t,class_loss:0.5271,\t\n",
      "step: 518,\t,class_loss:0.1887,\t\n",
      "step: 519,\t,class_loss:0.1865,\t\n",
      "step: 520,\t,class_loss:0.3826,\t\n",
      "step: 521,\t,class_loss:0.4122,\t\n",
      "step: 522,\t,class_loss:0.4862,\t\n",
      "step: 523,\t,class_loss:0.4347,\t\n",
      "step: 524,\t,class_loss:0.5995,\t\n",
      "step: 525,\t,class_loss:0.6023,\t\n",
      "step: 526,\t,class_loss:0.2890,\t\n",
      "step: 527,\t,class_loss:0.2143,\t\n",
      "step: 528,\t,class_loss:0.1981,\t\n",
      "step: 529,\t,class_loss:0.5415,\t\n",
      "step: 530,\t,class_loss:0.2809,\t\n",
      "step: 531,\t,class_loss:0.2010,\t\n",
      "step: 532,\t,class_loss:0.3970,\t\n",
      "step: 533,\t,class_loss:0.2454,\t\n",
      "step: 534,\t,class_loss:0.3957,\t\n",
      "step: 535,\t,class_loss:0.3193,\t\n",
      "step: 536,\t,class_loss:0.3351,\t\n",
      "step: 537,\t,class_loss:0.4982,\t\n",
      "step: 538,\t,class_loss:0.4536,\t\n",
      "step: 539,\t,class_loss:0.1356,\t\n",
      "step: 540,\t,class_loss:0.6239,\t\n",
      "step: 541,\t,class_loss:0.5293,\t\n",
      "step: 542,\t,class_loss:0.7878,\t\n",
      "step: 543,\t,class_loss:0.1200,\t\n",
      "step: 544,\t,class_loss:0.2565,\t\n",
      "step: 545,\t,class_loss:0.3429,\t\n",
      "step: 546,\t,class_loss:0.5321,\t\n",
      "step: 547,\t,class_loss:0.3608,\t\n",
      "step: 548,\t,class_loss:0.4747,\t\n",
      "iter: 00549, \t precision: 0.7631,\t best_acc:0.7631\n",
      "step: 549,\t,class_loss:0.3041,\t\n",
      "step: 550,\t,class_loss:0.1477,\t\n",
      "step: 551,\t,class_loss:0.4099,\t\n",
      "step: 552,\t,class_loss:0.8225,\t\n",
      "step: 553,\t,class_loss:0.4501,\t\n",
      "step: 554,\t,class_loss:0.4779,\t\n",
      "step: 555,\t,class_loss:0.3830,\t\n",
      "step: 556,\t,class_loss:0.3916,\t\n",
      "step: 557,\t,class_loss:0.3450,\t\n",
      "step: 558,\t,class_loss:0.3871,\t\n",
      "step: 559,\t,class_loss:0.3539,\t\n",
      "step: 560,\t,class_loss:0.2419,\t\n",
      "step: 561,\t,class_loss:0.4473,\t\n",
      "step: 562,\t,class_loss:0.2840,\t\n",
      "step: 563,\t,class_loss:0.1287,\t\n",
      "step: 564,\t,class_loss:0.2276,\t\n",
      "step: 565,\t,class_loss:0.4854,\t\n",
      "step: 566,\t,class_loss:0.3490,\t\n",
      "step: 567,\t,class_loss:0.4766,\t\n",
      "step: 568,\t,class_loss:0.4852,\t\n",
      "step: 569,\t,class_loss:0.2924,\t\n",
      "step: 570,\t,class_loss:0.5079,\t\n",
      "step: 571,\t,class_loss:0.0780,\t\n",
      "step: 572,\t,class_loss:0.3116,\t\n",
      "step: 573,\t,class_loss:0.2584,\t\n",
      "step: 574,\t,class_loss:0.4644,\t\n",
      "step: 575,\t,class_loss:0.6257,\t\n",
      "step: 576,\t,class_loss:0.2039,\t\n",
      "step: 577,\t,class_loss:0.2273,\t\n",
      "step: 578,\t,class_loss:0.2872,\t\n",
      "step: 579,\t,class_loss:0.0668,\t\n",
      "step: 580,\t,class_loss:0.2152,\t\n",
      "step: 581,\t,class_loss:0.4019,\t\n",
      "step: 582,\t,class_loss:0.3003,\t\n",
      "step: 583,\t,class_loss:0.4517,\t\n",
      "step: 584,\t,class_loss:0.2014,\t\n",
      "step: 585,\t,class_loss:0.5803,\t\n",
      "step: 586,\t,class_loss:0.1749,\t\n",
      "step: 587,\t,class_loss:0.4009,\t\n",
      "step: 588,\t,class_loss:0.5801,\t\n",
      "step: 589,\t,class_loss:0.4099,\t\n",
      "step: 590,\t,class_loss:0.5126,\t\n",
      "step: 591,\t,class_loss:0.1356,\t\n",
      "step: 592,\t,class_loss:0.1793,\t\n",
      "step: 593,\t,class_loss:0.2645,\t\n",
      "step: 594,\t,class_loss:0.2913,\t\n",
      "step: 595,\t,class_loss:0.3220,\t\n",
      "step: 596,\t,class_loss:0.9290,\t\n",
      "step: 597,\t,class_loss:0.1008,\t\n",
      "step: 598,\t,class_loss:0.2492,\t\n",
      "iter: 00599, \t precision: 0.7570,\t best_acc:0.7631\n",
      "step: 599,\t,class_loss:0.4122,\t\n",
      "step: 600,\t,class_loss:0.3944,\t\n",
      "step: 601,\t,class_loss:0.2817,\t\n",
      "step: 602,\t,class_loss:0.4895,\t\n",
      "step: 603,\t,class_loss:0.4386,\t\n",
      "step: 604,\t,class_loss:0.1405,\t\n",
      "step: 605,\t,class_loss:0.6881,\t\n",
      "step: 606,\t,class_loss:0.4010,\t\n",
      "step: 607,\t,class_loss:0.3322,\t\n",
      "step: 608,\t,class_loss:0.4141,\t\n",
      "step: 609,\t,class_loss:0.1386,\t\n",
      "step: 610,\t,class_loss:0.5854,\t\n",
      "step: 611,\t,class_loss:0.3556,\t\n",
      "step: 612,\t,class_loss:0.4216,\t\n",
      "step: 613,\t,class_loss:0.3426,\t\n",
      "step: 614,\t,class_loss:0.3538,\t\n",
      "step: 615,\t,class_loss:0.3370,\t\n",
      "step: 616,\t,class_loss:0.5407,\t\n",
      "step: 617,\t,class_loss:0.1771,\t\n",
      "step: 618,\t,class_loss:0.7841,\t\n",
      "step: 619,\t,class_loss:0.6234,\t\n",
      "step: 620,\t,class_loss:0.2340,\t\n",
      "step: 621,\t,class_loss:0.4262,\t\n",
      "step: 622,\t,class_loss:0.1999,\t\n",
      "step: 623,\t,class_loss:0.5717,\t\n",
      "step: 624,\t,class_loss:0.1931,\t\n",
      "step: 625,\t,class_loss:0.3072,\t\n",
      "step: 626,\t,class_loss:0.3338,\t\n",
      "step: 627,\t,class_loss:0.1532,\t\n",
      "step: 628,\t,class_loss:0.7243,\t\n",
      "step: 629,\t,class_loss:0.1763,\t\n",
      "step: 630,\t,class_loss:0.4302,\t\n",
      "step: 631,\t,class_loss:0.4505,\t\n",
      "step: 632,\t,class_loss:0.3318,\t\n",
      "step: 633,\t,class_loss:0.1679,\t\n",
      "step: 634,\t,class_loss:0.1185,\t\n",
      "step: 635,\t,class_loss:0.3844,\t\n",
      "step: 636,\t,class_loss:0.4401,\t\n",
      "step: 637,\t,class_loss:0.2369,\t\n",
      "step: 638,\t,class_loss:0.6277,\t\n",
      "step: 639,\t,class_loss:0.1582,\t\n",
      "step: 640,\t,class_loss:0.6535,\t\n",
      "step: 641,\t,class_loss:0.2268,\t\n",
      "step: 642,\t,class_loss:0.0940,\t\n",
      "step: 643,\t,class_loss:0.3164,\t\n",
      "step: 644,\t,class_loss:0.4862,\t\n",
      "step: 645,\t,class_loss:0.0902,\t\n",
      "step: 646,\t,class_loss:0.3162,\t\n",
      "step: 647,\t,class_loss:0.3723,\t\n",
      "step: 648,\t,class_loss:0.5793,\t\n",
      "iter: 00649, \t precision: 0.7390,\t best_acc:0.7631\n",
      "step: 649,\t,class_loss:0.3146,\t\n",
      "step: 650,\t,class_loss:0.4303,\t\n",
      "step: 651,\t,class_loss:0.3401,\t\n",
      "step: 652,\t,class_loss:0.3291,\t\n",
      "step: 653,\t,class_loss:0.1979,\t\n",
      "step: 654,\t,class_loss:0.1751,\t\n",
      "step: 655,\t,class_loss:0.2386,\t\n",
      "step: 656,\t,class_loss:0.1712,\t\n",
      "step: 657,\t,class_loss:0.4813,\t\n",
      "step: 658,\t,class_loss:0.1502,\t\n",
      "step: 659,\t,class_loss:0.4946,\t\n",
      "step: 660,\t,class_loss:0.2743,\t\n",
      "step: 661,\t,class_loss:0.2838,\t\n",
      "step: 662,\t,class_loss:0.1438,\t\n",
      "step: 663,\t,class_loss:0.0841,\t\n",
      "step: 664,\t,class_loss:0.1877,\t\n",
      "step: 665,\t,class_loss:0.0873,\t\n",
      "step: 666,\t,class_loss:0.2399,\t\n",
      "step: 667,\t,class_loss:0.1749,\t\n",
      "step: 668,\t,class_loss:0.2037,\t\n",
      "step: 669,\t,class_loss:0.2485,\t\n",
      "step: 670,\t,class_loss:0.1364,\t\n",
      "step: 671,\t,class_loss:0.2843,\t\n",
      "step: 672,\t,class_loss:0.3296,\t\n",
      "step: 673,\t,class_loss:0.2234,\t\n",
      "step: 674,\t,class_loss:0.1970,\t\n",
      "step: 675,\t,class_loss:0.0937,\t\n",
      "step: 676,\t,class_loss:0.1594,\t\n",
      "step: 677,\t,class_loss:0.2105,\t\n",
      "step: 678,\t,class_loss:0.4231,\t\n",
      "step: 679,\t,class_loss:0.3648,\t\n",
      "step: 680,\t,class_loss:0.6406,\t\n",
      "step: 681,\t,class_loss:0.4262,\t\n",
      "step: 682,\t,class_loss:0.1566,\t\n",
      "step: 683,\t,class_loss:0.6138,\t\n",
      "step: 684,\t,class_loss:0.3318,\t\n",
      "step: 685,\t,class_loss:0.2005,\t\n",
      "step: 686,\t,class_loss:0.2756,\t\n",
      "step: 687,\t,class_loss:0.2599,\t\n",
      "step: 688,\t,class_loss:0.3098,\t\n",
      "step: 689,\t,class_loss:0.3431,\t\n",
      "step: 690,\t,class_loss:0.3495,\t\n",
      "step: 691,\t,class_loss:0.2677,\t\n",
      "step: 692,\t,class_loss:0.1969,\t\n",
      "step: 693,\t,class_loss:0.1045,\t\n",
      "step: 694,\t,class_loss:0.3267,\t\n",
      "step: 695,\t,class_loss:0.2719,\t\n",
      "step: 696,\t,class_loss:0.1584,\t\n",
      "step: 697,\t,class_loss:0.1086,\t\n",
      "step: 698,\t,class_loss:0.3087,\t\n",
      "iter: 00699, \t precision: 0.7751,\t best_acc:0.7751\n",
      "step: 699,\t,class_loss:0.1310,\t\n",
      "step: 700,\t,class_loss:0.2378,\t\n",
      "step: 701,\t,class_loss:0.7208,\t\n",
      "step: 702,\t,class_loss:0.5774,\t\n",
      "step: 703,\t,class_loss:0.1359,\t\n",
      "step: 704,\t,class_loss:0.1626,\t\n",
      "step: 705,\t,class_loss:0.2560,\t\n",
      "step: 706,\t,class_loss:0.1650,\t\n",
      "step: 707,\t,class_loss:0.0496,\t\n",
      "step: 708,\t,class_loss:0.0832,\t\n",
      "step: 709,\t,class_loss:0.2292,\t\n",
      "step: 710,\t,class_loss:0.3597,\t\n",
      "step: 711,\t,class_loss:0.1012,\t\n",
      "step: 712,\t,class_loss:0.2677,\t\n",
      "step: 713,\t,class_loss:0.2729,\t\n",
      "step: 714,\t,class_loss:0.4266,\t\n",
      "step: 715,\t,class_loss:0.2595,\t\n",
      "step: 716,\t,class_loss:0.1966,\t\n",
      "step: 717,\t,class_loss:0.1552,\t\n",
      "step: 718,\t,class_loss:0.1951,\t\n",
      "step: 719,\t,class_loss:0.2814,\t\n",
      "step: 720,\t,class_loss:0.2772,\t\n",
      "step: 721,\t,class_loss:0.1228,\t\n",
      "step: 722,\t,class_loss:0.3587,\t\n",
      "step: 723,\t,class_loss:0.5393,\t\n",
      "step: 724,\t,class_loss:0.1565,\t\n",
      "step: 725,\t,class_loss:0.0720,\t\n",
      "step: 726,\t,class_loss:0.3438,\t\n",
      "step: 727,\t,class_loss:0.2636,\t\n",
      "step: 728,\t,class_loss:0.0943,\t\n",
      "step: 729,\t,class_loss:0.2045,\t\n",
      "step: 730,\t,class_loss:0.7312,\t\n",
      "step: 731,\t,class_loss:0.1944,\t\n",
      "step: 732,\t,class_loss:0.0612,\t\n",
      "step: 733,\t,class_loss:0.1357,\t\n",
      "step: 734,\t,class_loss:0.3511,\t\n",
      "step: 735,\t,class_loss:0.2898,\t\n",
      "step: 736,\t,class_loss:0.5577,\t\n",
      "step: 737,\t,class_loss:0.2943,\t\n",
      "step: 738,\t,class_loss:0.3555,\t\n",
      "step: 739,\t,class_loss:0.4742,\t\n",
      "step: 740,\t,class_loss:0.3718,\t\n",
      "step: 741,\t,class_loss:0.1697,\t\n",
      "step: 742,\t,class_loss:0.3220,\t\n",
      "step: 743,\t,class_loss:0.1309,\t\n",
      "step: 744,\t,class_loss:0.1476,\t\n",
      "step: 745,\t,class_loss:0.0725,\t\n",
      "step: 746,\t,class_loss:0.3702,\t\n",
      "step: 747,\t,class_loss:0.3933,\t\n",
      "step: 748,\t,class_loss:0.1418,\t\n",
      "iter: 00749, \t precision: 0.7691,\t best_acc:0.7751\n",
      "step: 749,\t,class_loss:0.2313,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 750,\t,class_loss:0.2242,\t\n",
      "step: 751,\t,class_loss:0.4814,\t\n",
      "step: 752,\t,class_loss:0.4077,\t\n",
      "step: 753,\t,class_loss:0.2368,\t\n",
      "step: 754,\t,class_loss:0.2348,\t\n",
      "step: 755,\t,class_loss:0.1341,\t\n",
      "step: 756,\t,class_loss:0.2043,\t\n",
      "step: 757,\t,class_loss:0.7372,\t\n",
      "step: 758,\t,class_loss:0.2811,\t\n",
      "step: 759,\t,class_loss:0.3056,\t\n",
      "step: 760,\t,class_loss:0.5513,\t\n",
      "step: 761,\t,class_loss:0.1652,\t\n",
      "step: 762,\t,class_loss:0.2404,\t\n",
      "step: 763,\t,class_loss:0.5008,\t\n",
      "step: 764,\t,class_loss:0.1065,\t\n",
      "step: 765,\t,class_loss:0.1975,\t\n",
      "step: 766,\t,class_loss:0.2561,\t\n",
      "step: 767,\t,class_loss:0.3225,\t\n",
      "step: 768,\t,class_loss:0.6613,\t\n",
      "step: 769,\t,class_loss:0.1474,\t\n",
      "step: 770,\t,class_loss:0.1534,\t\n",
      "step: 771,\t,class_loss:0.2637,\t\n",
      "step: 772,\t,class_loss:0.3502,\t\n",
      "step: 773,\t,class_loss:0.4227,\t\n",
      "step: 774,\t,class_loss:0.1324,\t\n",
      "step: 775,\t,class_loss:0.4348,\t\n",
      "step: 776,\t,class_loss:0.2685,\t\n",
      "step: 777,\t,class_loss:0.4049,\t\n",
      "step: 778,\t,class_loss:0.4275,\t\n",
      "step: 779,\t,class_loss:0.1435,\t\n",
      "step: 780,\t,class_loss:0.1067,\t\n",
      "step: 781,\t,class_loss:0.2880,\t\n",
      "step: 782,\t,class_loss:0.4327,\t\n",
      "step: 783,\t,class_loss:0.2024,\t\n",
      "step: 784,\t,class_loss:0.1722,\t\n",
      "step: 785,\t,class_loss:0.1603,\t\n",
      "step: 786,\t,class_loss:0.2797,\t\n",
      "step: 787,\t,class_loss:0.2361,\t\n",
      "step: 788,\t,class_loss:0.3397,\t\n",
      "step: 789,\t,class_loss:0.2627,\t\n",
      "step: 790,\t,class_loss:0.1038,\t\n",
      "step: 791,\t,class_loss:0.3540,\t\n",
      "step: 792,\t,class_loss:0.1371,\t\n",
      "step: 793,\t,class_loss:0.0435,\t\n",
      "step: 794,\t,class_loss:0.2032,\t\n",
      "step: 795,\t,class_loss:0.0464,\t\n",
      "step: 796,\t,class_loss:0.1636,\t\n",
      "step: 797,\t,class_loss:0.1738,\t\n",
      "step: 798,\t,class_loss:0.2749,\t\n",
      "iter: 00799, \t precision: 0.7871,\t best_acc:0.7871\n",
      "step: 799,\t,class_loss:0.2906,\t\n",
      "step: 800,\t,class_loss:0.4220,\t\n",
      "step: 801,\t,class_loss:0.2639,\t\n",
      "step: 802,\t,class_loss:0.1235,\t\n",
      "step: 803,\t,class_loss:0.0540,\t\n",
      "step: 804,\t,class_loss:0.1371,\t\n",
      "step: 805,\t,class_loss:0.5601,\t\n",
      "step: 806,\t,class_loss:0.0503,\t\n",
      "step: 807,\t,class_loss:0.2184,\t\n",
      "step: 808,\t,class_loss:0.3175,\t\n",
      "step: 809,\t,class_loss:0.2610,\t\n",
      "step: 810,\t,class_loss:0.2595,\t\n",
      "step: 811,\t,class_loss:0.3242,\t\n",
      "step: 812,\t,class_loss:0.2396,\t\n",
      "step: 813,\t,class_loss:0.1280,\t\n",
      "step: 814,\t,class_loss:0.2014,\t\n",
      "step: 815,\t,class_loss:0.2446,\t\n",
      "step: 816,\t,class_loss:0.2829,\t\n",
      "step: 817,\t,class_loss:0.1682,\t\n",
      "step: 818,\t,class_loss:0.1813,\t\n",
      "step: 819,\t,class_loss:0.2336,\t\n",
      "step: 820,\t,class_loss:0.2147,\t\n",
      "step: 821,\t,class_loss:0.1937,\t\n",
      "step: 822,\t,class_loss:0.1194,\t\n",
      "step: 823,\t,class_loss:0.1800,\t\n",
      "step: 824,\t,class_loss:0.0838,\t\n",
      "step: 825,\t,class_loss:0.2543,\t\n",
      "step: 826,\t,class_loss:0.0741,\t\n",
      "step: 827,\t,class_loss:0.1570,\t\n",
      "step: 828,\t,class_loss:0.1406,\t\n",
      "step: 829,\t,class_loss:0.2953,\t\n",
      "step: 830,\t,class_loss:0.1419,\t\n",
      "step: 831,\t,class_loss:0.0996,\t\n",
      "step: 832,\t,class_loss:0.2239,\t\n",
      "step: 833,\t,class_loss:0.1138,\t\n",
      "step: 834,\t,class_loss:0.3077,\t\n",
      "step: 835,\t,class_loss:0.4579,\t\n",
      "step: 836,\t,class_loss:0.1948,\t\n",
      "step: 837,\t,class_loss:0.3216,\t\n",
      "step: 838,\t,class_loss:0.2159,\t\n",
      "step: 839,\t,class_loss:0.3464,\t\n",
      "step: 840,\t,class_loss:0.3414,\t\n",
      "step: 841,\t,class_loss:0.1644,\t\n",
      "step: 842,\t,class_loss:0.0764,\t\n",
      "step: 843,\t,class_loss:0.2241,\t\n",
      "step: 844,\t,class_loss:0.0305,\t\n",
      "step: 845,\t,class_loss:0.4113,\t\n",
      "step: 846,\t,class_loss:0.2419,\t\n",
      "step: 847,\t,class_loss:0.1427,\t\n",
      "step: 848,\t,class_loss:0.1815,\t\n",
      "iter: 00849, \t precision: 0.7631,\t best_acc:0.7871\n",
      "step: 849,\t,class_loss:0.2029,\t\n",
      "step: 850,\t,class_loss:0.3012,\t\n",
      "step: 851,\t,class_loss:0.7044,\t\n",
      "step: 852,\t,class_loss:0.0553,\t\n",
      "step: 853,\t,class_loss:0.1274,\t\n",
      "step: 854,\t,class_loss:0.1803,\t\n",
      "step: 855,\t,class_loss:0.1141,\t\n",
      "step: 856,\t,class_loss:0.1605,\t\n",
      "step: 857,\t,class_loss:0.0567,\t\n",
      "step: 858,\t,class_loss:0.2364,\t\n",
      "step: 859,\t,class_loss:0.1453,\t\n",
      "step: 860,\t,class_loss:0.0692,\t\n",
      "step: 861,\t,class_loss:0.1258,\t\n",
      "step: 862,\t,class_loss:0.1103,\t\n",
      "step: 863,\t,class_loss:0.0383,\t\n",
      "step: 864,\t,class_loss:0.2038,\t\n",
      "step: 865,\t,class_loss:0.4143,\t\n",
      "step: 866,\t,class_loss:0.2594,\t\n",
      "step: 867,\t,class_loss:0.1826,\t\n",
      "step: 868,\t,class_loss:0.2943,\t\n",
      "step: 869,\t,class_loss:0.1901,\t\n",
      "step: 870,\t,class_loss:0.0784,\t\n",
      "step: 871,\t,class_loss:0.2441,\t\n",
      "step: 872,\t,class_loss:0.0976,\t\n",
      "step: 873,\t,class_loss:0.1205,\t\n",
      "step: 874,\t,class_loss:0.1079,\t\n",
      "step: 875,\t,class_loss:0.2820,\t\n",
      "step: 876,\t,class_loss:0.2367,\t\n",
      "step: 877,\t,class_loss:0.1599,\t\n",
      "step: 878,\t,class_loss:0.0757,\t\n",
      "step: 879,\t,class_loss:0.1821,\t\n",
      "step: 880,\t,class_loss:0.3634,\t\n",
      "step: 881,\t,class_loss:0.1823,\t\n",
      "step: 882,\t,class_loss:0.1485,\t\n",
      "step: 883,\t,class_loss:0.2489,\t\n",
      "step: 884,\t,class_loss:0.4610,\t\n",
      "step: 885,\t,class_loss:0.2351,\t\n",
      "step: 886,\t,class_loss:0.0830,\t\n",
      "step: 887,\t,class_loss:0.1974,\t\n",
      "step: 888,\t,class_loss:0.1395,\t\n",
      "step: 889,\t,class_loss:0.1804,\t\n",
      "step: 890,\t,class_loss:0.4112,\t\n",
      "step: 891,\t,class_loss:0.3681,\t\n",
      "step: 892,\t,class_loss:0.6151,\t\n",
      "step: 893,\t,class_loss:0.0929,\t\n",
      "step: 894,\t,class_loss:0.0956,\t\n",
      "step: 895,\t,class_loss:0.1614,\t\n",
      "step: 896,\t,class_loss:0.3180,\t\n",
      "step: 897,\t,class_loss:0.2600,\t\n",
      "step: 898,\t,class_loss:0.2091,\t\n",
      "iter: 00899, \t precision: 0.7691,\t best_acc:0.7871\n",
      "step: 899,\t,class_loss:0.3239,\t\n",
      "step: 900,\t,class_loss:0.1913,\t\n",
      "step: 901,\t,class_loss:0.2981,\t\n",
      "step: 902,\t,class_loss:0.1722,\t\n",
      "step: 903,\t,class_loss:0.2594,\t\n",
      "step: 904,\t,class_loss:0.1683,\t\n",
      "step: 905,\t,class_loss:0.3853,\t\n",
      "step: 906,\t,class_loss:0.3422,\t\n",
      "step: 907,\t,class_loss:0.2095,\t\n",
      "step: 908,\t,class_loss:0.0448,\t\n",
      "step: 909,\t,class_loss:0.3848,\t\n",
      "step: 910,\t,class_loss:0.2105,\t\n",
      "step: 911,\t,class_loss:0.3450,\t\n",
      "step: 912,\t,class_loss:0.0766,\t\n",
      "step: 913,\t,class_loss:0.2970,\t\n",
      "step: 914,\t,class_loss:0.1847,\t\n",
      "step: 915,\t,class_loss:0.2851,\t\n",
      "step: 916,\t,class_loss:0.2599,\t\n",
      "step: 917,\t,class_loss:0.1752,\t\n",
      "step: 918,\t,class_loss:0.2182,\t\n",
      "step: 919,\t,class_loss:0.0753,\t\n",
      "step: 920,\t,class_loss:0.1664,\t\n",
      "step: 921,\t,class_loss:0.1933,\t\n",
      "step: 922,\t,class_loss:0.3383,\t\n",
      "step: 923,\t,class_loss:0.1420,\t\n",
      "step: 924,\t,class_loss:0.2032,\t\n",
      "step: 925,\t,class_loss:0.3262,\t\n",
      "step: 926,\t,class_loss:0.2247,\t\n",
      "step: 927,\t,class_loss:0.0763,\t\n",
      "step: 928,\t,class_loss:0.1262,\t\n",
      "step: 929,\t,class_loss:0.1689,\t\n",
      "step: 930,\t,class_loss:0.1708,\t\n",
      "step: 931,\t,class_loss:0.1825,\t\n",
      "step: 932,\t,class_loss:0.1614,\t\n",
      "step: 933,\t,class_loss:0.1547,\t\n",
      "step: 934,\t,class_loss:0.3396,\t\n",
      "step: 935,\t,class_loss:0.1526,\t\n",
      "step: 936,\t,class_loss:0.3298,\t\n",
      "step: 937,\t,class_loss:0.1283,\t\n",
      "step: 938,\t,class_loss:0.1265,\t\n",
      "step: 939,\t,class_loss:0.1171,\t\n",
      "step: 940,\t,class_loss:0.0877,\t\n",
      "step: 941,\t,class_loss:0.3200,\t\n",
      "step: 942,\t,class_loss:0.1259,\t\n",
      "step: 943,\t,class_loss:0.2443,\t\n",
      "step: 944,\t,class_loss:0.5458,\t\n",
      "step: 945,\t,class_loss:0.3113,\t\n",
      "step: 946,\t,class_loss:0.1561,\t\n",
      "step: 947,\t,class_loss:0.0479,\t\n",
      "step: 948,\t,class_loss:0.2144,\t\n",
      "iter: 00949, \t precision: 0.7671,\t best_acc:0.7871\n",
      "step: 949,\t,class_loss:0.0399,\t\n",
      "step: 950,\t,class_loss:0.2097,\t\n",
      "step: 951,\t,class_loss:0.2560,\t\n",
      "step: 952,\t,class_loss:0.1797,\t\n",
      "step: 953,\t,class_loss:0.2329,\t\n",
      "step: 954,\t,class_loss:0.1325,\t\n",
      "step: 955,\t,class_loss:0.0597,\t\n",
      "step: 956,\t,class_loss:0.2393,\t\n",
      "step: 957,\t,class_loss:0.3592,\t\n",
      "step: 958,\t,class_loss:0.1435,\t\n",
      "step: 959,\t,class_loss:0.0633,\t\n",
      "step: 960,\t,class_loss:0.0891,\t\n",
      "step: 961,\t,class_loss:0.0882,\t\n",
      "step: 962,\t,class_loss:0.1949,\t\n",
      "step: 963,\t,class_loss:0.2524,\t\n",
      "step: 964,\t,class_loss:0.0320,\t\n",
      "step: 965,\t,class_loss:0.3750,\t\n",
      "step: 966,\t,class_loss:0.0987,\t\n",
      "step: 967,\t,class_loss:0.3627,\t\n",
      "step: 968,\t,class_loss:0.3169,\t\n",
      "step: 969,\t,class_loss:0.0842,\t\n",
      "step: 970,\t,class_loss:0.0871,\t\n",
      "step: 971,\t,class_loss:0.1529,\t\n",
      "step: 972,\t,class_loss:0.1368,\t\n",
      "step: 973,\t,class_loss:0.2463,\t\n",
      "step: 974,\t,class_loss:0.1865,\t\n",
      "step: 975,\t,class_loss:0.2385,\t\n",
      "step: 976,\t,class_loss:0.1186,\t\n",
      "step: 977,\t,class_loss:0.1716,\t\n",
      "step: 978,\t,class_loss:0.4146,\t\n",
      "step: 979,\t,class_loss:0.0729,\t\n",
      "step: 980,\t,class_loss:0.2341,\t\n",
      "step: 981,\t,class_loss:0.3119,\t\n",
      "step: 982,\t,class_loss:0.1841,\t\n",
      "step: 983,\t,class_loss:0.1159,\t\n",
      "step: 984,\t,class_loss:0.3478,\t\n",
      "step: 985,\t,class_loss:0.1490,\t\n",
      "step: 986,\t,class_loss:0.1155,\t\n",
      "step: 987,\t,class_loss:0.2120,\t\n",
      "step: 988,\t,class_loss:0.1206,\t\n",
      "step: 989,\t,class_loss:0.1803,\t\n",
      "step: 990,\t,class_loss:0.1250,\t\n",
      "step: 991,\t,class_loss:0.0954,\t\n",
      "step: 992,\t,class_loss:0.0386,\t\n",
      "step: 993,\t,class_loss:0.1095,\t\n",
      "step: 994,\t,class_loss:0.0819,\t\n",
      "step: 995,\t,class_loss:0.2780,\t\n",
      "step: 996,\t,class_loss:0.2292,\t\n",
      "step: 997,\t,class_loss:0.2452,\t\n",
      "step: 998,\t,class_loss:0.4649,\t\n",
      "iter: 00999, \t precision: 0.7851,\t best_acc:0.7871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 999,\t,class_loss:0.1114,\t\n",
      "step: 1000,\t,class_loss:0.2702,\t\n",
      "step: 1001,\t,class_loss:0.2015,\t\n",
      "step: 1002,\t,class_loss:0.2125,\t\n",
      "step: 1003,\t,class_loss:0.2203,\t\n",
      "step: 1004,\t,class_loss:0.0639,\t\n",
      "step: 1005,\t,class_loss:0.0713,\t\n",
      "step: 1006,\t,class_loss:0.0511,\t\n",
      "step: 1007,\t,class_loss:0.2296,\t\n",
      "step: 1008,\t,class_loss:0.0798,\t\n",
      "step: 1009,\t,class_loss:0.1187,\t\n",
      "step: 1010,\t,class_loss:0.0250,\t\n",
      "step: 1011,\t,class_loss:0.2705,\t\n",
      "step: 1012,\t,class_loss:0.0991,\t\n",
      "step: 1013,\t,class_loss:0.1491,\t\n",
      "step: 1014,\t,class_loss:0.0572,\t\n",
      "step: 1015,\t,class_loss:0.1819,\t\n",
      "step: 1016,\t,class_loss:0.1397,\t\n",
      "step: 1017,\t,class_loss:0.2009,\t\n",
      "step: 1018,\t,class_loss:0.0953,\t\n",
      "step: 1019,\t,class_loss:0.1654,\t\n",
      "step: 1020,\t,class_loss:0.1834,\t\n",
      "step: 1021,\t,class_loss:0.1299,\t\n",
      "step: 1022,\t,class_loss:0.0870,\t\n",
      "step: 1023,\t,class_loss:0.2833,\t\n",
      "step: 1024,\t,class_loss:0.4132,\t\n",
      "step: 1025,\t,class_loss:0.1197,\t\n",
      "step: 1026,\t,class_loss:0.0625,\t\n",
      "step: 1027,\t,class_loss:0.0439,\t\n",
      "step: 1028,\t,class_loss:0.3241,\t\n",
      "step: 1029,\t,class_loss:0.2141,\t\n",
      "step: 1030,\t,class_loss:0.3399,\t\n",
      "step: 1031,\t,class_loss:0.0441,\t\n",
      "step: 1032,\t,class_loss:0.1219,\t\n",
      "step: 1033,\t,class_loss:0.3786,\t\n",
      "step: 1034,\t,class_loss:0.0665,\t\n",
      "step: 1035,\t,class_loss:0.1299,\t\n",
      "step: 1036,\t,class_loss:0.0741,\t\n",
      "step: 1037,\t,class_loss:0.1460,\t\n",
      "step: 1038,\t,class_loss:0.0427,\t\n",
      "step: 1039,\t,class_loss:0.2028,\t\n",
      "step: 1040,\t,class_loss:0.1642,\t\n",
      "step: 1041,\t,class_loss:0.0937,\t\n",
      "step: 1042,\t,class_loss:0.0660,\t\n",
      "step: 1043,\t,class_loss:0.3214,\t\n",
      "step: 1044,\t,class_loss:0.0463,\t\n",
      "step: 1045,\t,class_loss:0.0341,\t\n",
      "step: 1046,\t,class_loss:0.1004,\t\n",
      "step: 1047,\t,class_loss:0.1354,\t\n",
      "step: 1048,\t,class_loss:0.1095,\t\n",
      "iter: 01049, \t precision: 0.7711,\t best_acc:0.7871\n",
      "step: 1049,\t,class_loss:0.1092,\t\n",
      "step: 1050,\t,class_loss:0.1463,\t\n",
      "step: 1051,\t,class_loss:0.2924,\t\n",
      "step: 1052,\t,class_loss:0.1141,\t\n",
      "step: 1053,\t,class_loss:0.1107,\t\n",
      "step: 1054,\t,class_loss:0.0550,\t\n",
      "step: 1055,\t,class_loss:0.1432,\t\n",
      "step: 1056,\t,class_loss:0.2893,\t\n",
      "step: 1057,\t,class_loss:0.3604,\t\n",
      "step: 1058,\t,class_loss:0.4502,\t\n",
      "step: 1059,\t,class_loss:0.2800,\t\n",
      "step: 1060,\t,class_loss:0.1786,\t\n",
      "step: 1061,\t,class_loss:0.1534,\t\n",
      "step: 1062,\t,class_loss:0.3305,\t\n",
      "step: 1063,\t,class_loss:0.0994,\t\n",
      "step: 1064,\t,class_loss:0.0447,\t\n",
      "step: 1065,\t,class_loss:0.2985,\t\n",
      "step: 1066,\t,class_loss:0.1515,\t\n",
      "step: 1067,\t,class_loss:0.1166,\t\n",
      "step: 1068,\t,class_loss:0.1572,\t\n",
      "step: 1069,\t,class_loss:0.2584,\t\n",
      "step: 1070,\t,class_loss:0.1366,\t\n",
      "step: 1071,\t,class_loss:0.0596,\t\n",
      "step: 1072,\t,class_loss:0.0918,\t\n",
      "step: 1073,\t,class_loss:0.2603,\t\n",
      "step: 1074,\t,class_loss:0.0433,\t\n",
      "step: 1075,\t,class_loss:0.1596,\t\n",
      "step: 1076,\t,class_loss:0.0858,\t\n",
      "step: 1077,\t,class_loss:0.2928,\t\n",
      "step: 1078,\t,class_loss:0.2288,\t\n",
      "step: 1079,\t,class_loss:0.2154,\t\n",
      "step: 1080,\t,class_loss:0.0876,\t\n",
      "step: 1081,\t,class_loss:0.0611,\t\n",
      "step: 1082,\t,class_loss:0.0837,\t\n",
      "step: 1083,\t,class_loss:0.0751,\t\n",
      "step: 1084,\t,class_loss:0.2061,\t\n",
      "step: 1085,\t,class_loss:0.0627,\t\n",
      "step: 1086,\t,class_loss:0.1531,\t\n",
      "step: 1087,\t,class_loss:0.0910,\t\n",
      "step: 1088,\t,class_loss:0.3191,\t\n",
      "step: 1089,\t,class_loss:0.3795,\t\n",
      "step: 1090,\t,class_loss:0.1384,\t\n",
      "step: 1091,\t,class_loss:0.0784,\t\n",
      "step: 1092,\t,class_loss:0.1377,\t\n",
      "step: 1093,\t,class_loss:0.0676,\t\n",
      "step: 1094,\t,class_loss:0.1119,\t\n",
      "step: 1095,\t,class_loss:0.0425,\t\n",
      "step: 1096,\t,class_loss:0.1772,\t\n",
      "step: 1097,\t,class_loss:0.1642,\t\n",
      "step: 1098,\t,class_loss:0.0634,\t\n",
      "iter: 01099, \t precision: 0.7791,\t best_acc:0.7871\n",
      "step: 1099,\t,class_loss:0.1380,\t\n",
      "step: 1100,\t,class_loss:0.0710,\t\n",
      "step: 1101,\t,class_loss:0.0713,\t\n",
      "step: 1102,\t,class_loss:0.1874,\t\n",
      "step: 1103,\t,class_loss:0.2573,\t\n",
      "step: 1104,\t,class_loss:0.0603,\t\n",
      "step: 1105,\t,class_loss:0.0871,\t\n",
      "step: 1106,\t,class_loss:0.1533,\t\n",
      "step: 1107,\t,class_loss:0.0292,\t\n",
      "step: 1108,\t,class_loss:0.2340,\t\n",
      "step: 1109,\t,class_loss:0.4971,\t\n",
      "step: 1110,\t,class_loss:0.1022,\t\n",
      "step: 1111,\t,class_loss:0.1657,\t\n",
      "step: 1112,\t,class_loss:0.1231,\t\n",
      "step: 1113,\t,class_loss:0.1536,\t\n",
      "step: 1114,\t,class_loss:0.1481,\t\n",
      "step: 1115,\t,class_loss:0.1064,\t\n",
      "step: 1116,\t,class_loss:0.1380,\t\n",
      "step: 1117,\t,class_loss:0.0876,\t\n",
      "step: 1118,\t,class_loss:0.0552,\t\n",
      "step: 1119,\t,class_loss:0.0469,\t\n",
      "step: 1120,\t,class_loss:0.0777,\t\n",
      "step: 1121,\t,class_loss:0.3964,\t\n",
      "step: 1122,\t,class_loss:0.1706,\t\n",
      "step: 1123,\t,class_loss:0.0443,\t\n",
      "step: 1124,\t,class_loss:0.3208,\t\n",
      "step: 1125,\t,class_loss:0.1536,\t\n",
      "step: 1126,\t,class_loss:0.0596,\t\n",
      "step: 1127,\t,class_loss:0.0745,\t\n",
      "step: 1128,\t,class_loss:0.2101,\t\n",
      "step: 1129,\t,class_loss:0.0692,\t\n",
      "step: 1130,\t,class_loss:0.0855,\t\n",
      "step: 1131,\t,class_loss:0.1265,\t\n",
      "step: 1132,\t,class_loss:0.0402,\t\n",
      "step: 1133,\t,class_loss:0.0898,\t\n",
      "step: 1134,\t,class_loss:0.0627,\t\n",
      "step: 1135,\t,class_loss:0.1513,\t\n",
      "step: 1136,\t,class_loss:0.1682,\t\n",
      "step: 1137,\t,class_loss:0.0702,\t\n",
      "step: 1138,\t,class_loss:0.1945,\t\n",
      "step: 1139,\t,class_loss:0.0680,\t\n",
      "step: 1140,\t,class_loss:0.1340,\t\n",
      "step: 1141,\t,class_loss:0.0323,\t\n",
      "step: 1142,\t,class_loss:0.1096,\t\n",
      "step: 1143,\t,class_loss:0.3124,\t\n",
      "step: 1144,\t,class_loss:0.1312,\t\n",
      "step: 1145,\t,class_loss:0.0958,\t\n",
      "step: 1146,\t,class_loss:0.2052,\t\n",
      "step: 1147,\t,class_loss:0.1792,\t\n",
      "step: 1148,\t,class_loss:0.2897,\t\n",
      "iter: 01149, \t precision: 0.7651,\t best_acc:0.7871\n",
      "step: 1149,\t,class_loss:0.1629,\t\n",
      "step: 1150,\t,class_loss:0.1044,\t\n",
      "step: 1151,\t,class_loss:0.2266,\t\n",
      "step: 1152,\t,class_loss:0.1108,\t\n",
      "step: 1153,\t,class_loss:0.1132,\t\n",
      "step: 1154,\t,class_loss:0.0364,\t\n",
      "step: 1155,\t,class_loss:0.0831,\t\n",
      "step: 1156,\t,class_loss:0.0939,\t\n",
      "step: 1157,\t,class_loss:0.2527,\t\n",
      "step: 1158,\t,class_loss:0.1009,\t\n",
      "step: 1159,\t,class_loss:0.2194,\t\n",
      "step: 1160,\t,class_loss:0.0640,\t\n",
      "step: 1161,\t,class_loss:0.1148,\t\n",
      "step: 1162,\t,class_loss:0.1321,\t\n",
      "step: 1163,\t,class_loss:0.1265,\t\n",
      "step: 1164,\t,class_loss:0.2624,\t\n",
      "step: 1165,\t,class_loss:0.0848,\t\n",
      "step: 1166,\t,class_loss:0.0320,\t\n",
      "step: 1167,\t,class_loss:0.0713,\t\n",
      "step: 1168,\t,class_loss:0.0726,\t\n",
      "step: 1169,\t,class_loss:0.2013,\t\n",
      "step: 1170,\t,class_loss:0.1598,\t\n",
      "step: 1171,\t,class_loss:0.1319,\t\n",
      "step: 1172,\t,class_loss:0.1183,\t\n",
      "step: 1173,\t,class_loss:0.0852,\t\n",
      "step: 1174,\t,class_loss:0.2138,\t\n",
      "step: 1175,\t,class_loss:0.1922,\t\n",
      "step: 1176,\t,class_loss:0.0445,\t\n",
      "step: 1177,\t,class_loss:0.4236,\t\n",
      "step: 1178,\t,class_loss:0.2052,\t\n",
      "step: 1179,\t,class_loss:0.1307,\t\n",
      "step: 1180,\t,class_loss:0.0837,\t\n",
      "step: 1181,\t,class_loss:0.2266,\t\n",
      "step: 1182,\t,class_loss:0.3069,\t\n",
      "step: 1183,\t,class_loss:0.0985,\t\n",
      "step: 1184,\t,class_loss:0.1157,\t\n",
      "step: 1185,\t,class_loss:0.1313,\t\n",
      "step: 1186,\t,class_loss:0.1437,\t\n",
      "step: 1187,\t,class_loss:0.0927,\t\n",
      "step: 1188,\t,class_loss:0.0940,\t\n",
      "step: 1189,\t,class_loss:0.0886,\t\n",
      "step: 1190,\t,class_loss:0.1387,\t\n",
      "step: 1191,\t,class_loss:0.1358,\t\n",
      "step: 1192,\t,class_loss:0.0734,\t\n",
      "step: 1193,\t,class_loss:0.1459,\t\n",
      "step: 1194,\t,class_loss:0.0304,\t\n",
      "step: 1195,\t,class_loss:0.1781,\t\n",
      "step: 1196,\t,class_loss:0.0433,\t\n",
      "step: 1197,\t,class_loss:0.2638,\t\n",
      "step: 1198,\t,class_loss:0.1013,\t\n",
      "iter: 01199, \t precision: 0.7530,\t best_acc:0.7871\n",
      "step: 1199,\t,class_loss:0.0632,\t\n",
      "step: 1200,\t,class_loss:0.0458,\t\n",
      "step: 1201,\t,class_loss:0.1480,\t\n",
      "step: 1202,\t,class_loss:0.0902,\t\n",
      "step: 1203,\t,class_loss:0.1281,\t\n",
      "step: 1204,\t,class_loss:0.1110,\t\n",
      "step: 1205,\t,class_loss:0.0975,\t\n",
      "step: 1206,\t,class_loss:0.0856,\t\n",
      "step: 1207,\t,class_loss:0.1055,\t\n",
      "step: 1208,\t,class_loss:0.0503,\t\n",
      "step: 1209,\t,class_loss:0.0735,\t\n",
      "step: 1210,\t,class_loss:0.0393,\t\n",
      "step: 1211,\t,class_loss:0.2335,\t\n",
      "step: 1212,\t,class_loss:0.0808,\t\n",
      "step: 1213,\t,class_loss:0.1245,\t\n",
      "step: 1214,\t,class_loss:0.2651,\t\n",
      "step: 1215,\t,class_loss:0.0754,\t\n",
      "step: 1216,\t,class_loss:0.0739,\t\n",
      "step: 1217,\t,class_loss:0.0970,\t\n",
      "step: 1218,\t,class_loss:0.0869,\t\n",
      "step: 1219,\t,class_loss:0.0347,\t\n",
      "step: 1220,\t,class_loss:0.2433,\t\n",
      "step: 1221,\t,class_loss:0.1459,\t\n",
      "step: 1222,\t,class_loss:0.0736,\t\n",
      "step: 1223,\t,class_loss:0.1057,\t\n",
      "step: 1224,\t,class_loss:0.1603,\t\n",
      "step: 1225,\t,class_loss:0.1050,\t\n",
      "step: 1226,\t,class_loss:0.1503,\t\n",
      "step: 1227,\t,class_loss:0.0597,\t\n",
      "step: 1228,\t,class_loss:0.0813,\t\n",
      "step: 1229,\t,class_loss:0.2568,\t\n",
      "step: 1230,\t,class_loss:0.0998,\t\n",
      "step: 1231,\t,class_loss:0.1602,\t\n",
      "step: 1232,\t,class_loss:0.1494,\t\n",
      "step: 1233,\t,class_loss:0.0816,\t\n",
      "step: 1234,\t,class_loss:0.1727,\t\n",
      "step: 1235,\t,class_loss:0.2332,\t\n",
      "step: 1236,\t,class_loss:0.0531,\t\n",
      "step: 1237,\t,class_loss:0.0592,\t\n",
      "step: 1238,\t,class_loss:0.0974,\t\n",
      "step: 1239,\t,class_loss:0.2210,\t\n",
      "step: 1240,\t,class_loss:0.2130,\t\n",
      "step: 1241,\t,class_loss:0.2693,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1242,\t,class_loss:0.1683,\t\n",
      "step: 1243,\t,class_loss:0.1423,\t\n",
      "step: 1244,\t,class_loss:0.1099,\t\n",
      "step: 1245,\t,class_loss:0.3299,\t\n",
      "step: 1246,\t,class_loss:0.1544,\t\n",
      "step: 1247,\t,class_loss:0.0367,\t\n",
      "step: 1248,\t,class_loss:0.0756,\t\n",
      "iter: 01249, \t precision: 0.7610,\t best_acc:0.7871\n",
      "step: 1249,\t,class_loss:0.0950,\t\n",
      "step: 1250,\t,class_loss:0.0810,\t\n",
      "step: 1251,\t,class_loss:0.1501,\t\n",
      "step: 1252,\t,class_loss:0.0305,\t\n",
      "step: 1253,\t,class_loss:0.0737,\t\n",
      "step: 1254,\t,class_loss:0.0389,\t\n",
      "step: 1255,\t,class_loss:0.0724,\t\n",
      "step: 1256,\t,class_loss:0.3096,\t\n",
      "step: 1257,\t,class_loss:0.1260,\t\n",
      "step: 1258,\t,class_loss:0.1345,\t\n",
      "step: 1259,\t,class_loss:0.0883,\t\n",
      "step: 1260,\t,class_loss:0.0262,\t\n",
      "step: 1261,\t,class_loss:0.0464,\t\n",
      "step: 1262,\t,class_loss:0.1968,\t\n",
      "step: 1263,\t,class_loss:0.1003,\t\n",
      "step: 1264,\t,class_loss:0.0661,\t\n",
      "step: 1265,\t,class_loss:0.1603,\t\n",
      "step: 1266,\t,class_loss:0.1573,\t\n",
      "step: 1267,\t,class_loss:0.1114,\t\n",
      "step: 1268,\t,class_loss:0.2406,\t\n",
      "step: 1269,\t,class_loss:0.1104,\t\n",
      "step: 1270,\t,class_loss:0.0866,\t\n",
      "step: 1271,\t,class_loss:0.1137,\t\n",
      "step: 1272,\t,class_loss:0.0688,\t\n",
      "step: 1273,\t,class_loss:0.2775,\t\n",
      "step: 1274,\t,class_loss:0.1011,\t\n",
      "step: 1275,\t,class_loss:0.1185,\t\n",
      "step: 1276,\t,class_loss:0.2218,\t\n",
      "step: 1277,\t,class_loss:0.0735,\t\n",
      "step: 1278,\t,class_loss:0.0522,\t\n",
      "step: 1279,\t,class_loss:0.1173,\t\n",
      "step: 1280,\t,class_loss:0.0988,\t\n",
      "step: 1281,\t,class_loss:0.3115,\t\n",
      "step: 1282,\t,class_loss:0.0737,\t\n",
      "step: 1283,\t,class_loss:0.0458,\t\n",
      "step: 1284,\t,class_loss:0.0715,\t\n",
      "step: 1285,\t,class_loss:0.0328,\t\n",
      "step: 1286,\t,class_loss:0.0497,\t\n",
      "step: 1287,\t,class_loss:0.1885,\t\n",
      "step: 1288,\t,class_loss:0.1799,\t\n",
      "step: 1289,\t,class_loss:0.0768,\t\n",
      "step: 1290,\t,class_loss:0.0497,\t\n",
      "step: 1291,\t,class_loss:0.0667,\t\n",
      "step: 1292,\t,class_loss:0.0707,\t\n",
      "step: 1293,\t,class_loss:0.1443,\t\n",
      "step: 1294,\t,class_loss:0.1099,\t\n",
      "step: 1295,\t,class_loss:0.1313,\t\n",
      "step: 1296,\t,class_loss:0.0732,\t\n",
      "step: 1297,\t,class_loss:0.0905,\t\n",
      "step: 1298,\t,class_loss:0.0474,\t\n",
      "iter: 01299, \t precision: 0.7871,\t best_acc:0.7871\n",
      "step: 1299,\t,class_loss:0.1650,\t\n",
      "step: 1300,\t,class_loss:0.0908,\t\n",
      "step: 1301,\t,class_loss:0.0911,\t\n",
      "step: 1302,\t,class_loss:0.0676,\t\n",
      "step: 1303,\t,class_loss:0.4503,\t\n",
      "step: 1304,\t,class_loss:0.2509,\t\n",
      "step: 1305,\t,class_loss:0.0714,\t\n",
      "step: 1306,\t,class_loss:0.0958,\t\n",
      "step: 1307,\t,class_loss:0.0350,\t\n",
      "step: 1308,\t,class_loss:0.1310,\t\n",
      "step: 1309,\t,class_loss:0.1112,\t\n",
      "step: 1310,\t,class_loss:0.1826,\t\n",
      "step: 1311,\t,class_loss:0.0370,\t\n",
      "step: 1312,\t,class_loss:0.3144,\t\n",
      "step: 1313,\t,class_loss:0.0602,\t\n",
      "step: 1314,\t,class_loss:0.0912,\t\n",
      "step: 1315,\t,class_loss:0.1062,\t\n",
      "step: 1316,\t,class_loss:0.0395,\t\n",
      "step: 1317,\t,class_loss:0.0511,\t\n",
      "step: 1318,\t,class_loss:0.1053,\t\n",
      "step: 1319,\t,class_loss:0.0584,\t\n",
      "step: 1320,\t,class_loss:0.2409,\t\n",
      "step: 1321,\t,class_loss:0.1210,\t\n",
      "step: 1322,\t,class_loss:0.0312,\t\n",
      "step: 1323,\t,class_loss:0.1087,\t\n",
      "step: 1324,\t,class_loss:0.1104,\t\n",
      "step: 1325,\t,class_loss:0.1399,\t\n",
      "step: 1326,\t,class_loss:0.1529,\t\n",
      "step: 1327,\t,class_loss:0.1034,\t\n",
      "step: 1328,\t,class_loss:0.1671,\t\n",
      "step: 1329,\t,class_loss:0.0576,\t\n",
      "step: 1330,\t,class_loss:0.0555,\t\n",
      "step: 1331,\t,class_loss:0.1202,\t\n",
      "step: 1332,\t,class_loss:0.0292,\t\n",
      "step: 1333,\t,class_loss:0.0307,\t\n",
      "step: 1334,\t,class_loss:0.0692,\t\n",
      "step: 1335,\t,class_loss:0.0629,\t\n",
      "step: 1336,\t,class_loss:0.1025,\t\n",
      "step: 1337,\t,class_loss:0.1167,\t\n",
      "step: 1338,\t,class_loss:0.2891,\t\n",
      "step: 1339,\t,class_loss:0.0837,\t\n",
      "step: 1340,\t,class_loss:0.1588,\t\n",
      "step: 1341,\t,class_loss:0.2000,\t\n",
      "step: 1342,\t,class_loss:0.0381,\t\n",
      "step: 1343,\t,class_loss:0.0370,\t\n",
      "step: 1344,\t,class_loss:0.1283,\t\n",
      "step: 1345,\t,class_loss:0.0456,\t\n",
      "step: 1346,\t,class_loss:0.1134,\t\n",
      "step: 1347,\t,class_loss:0.0682,\t\n",
      "step: 1348,\t,class_loss:0.0753,\t\n",
      "iter: 01349, \t precision: 0.7811,\t best_acc:0.7871\n",
      "step: 1349,\t,class_loss:0.2356,\t\n",
      "step: 1350,\t,class_loss:0.3854,\t\n",
      "step: 1351,\t,class_loss:0.1660,\t\n",
      "step: 1352,\t,class_loss:0.1516,\t\n",
      "step: 1353,\t,class_loss:0.0962,\t\n",
      "step: 1354,\t,class_loss:0.0391,\t\n",
      "step: 1355,\t,class_loss:0.0627,\t\n",
      "step: 1356,\t,class_loss:0.0223,\t\n",
      "step: 1357,\t,class_loss:0.0705,\t\n",
      "step: 1358,\t,class_loss:0.1027,\t\n",
      "step: 1359,\t,class_loss:0.0270,\t\n",
      "step: 1360,\t,class_loss:0.1437,\t\n",
      "step: 1361,\t,class_loss:0.0343,\t\n",
      "step: 1362,\t,class_loss:0.0953,\t\n",
      "step: 1363,\t,class_loss:0.1712,\t\n",
      "step: 1364,\t,class_loss:0.0283,\t\n",
      "step: 1365,\t,class_loss:0.1664,\t\n",
      "step: 1366,\t,class_loss:0.1751,\t\n",
      "step: 1367,\t,class_loss:0.0463,\t\n",
      "step: 1368,\t,class_loss:0.0868,\t\n",
      "step: 1369,\t,class_loss:0.2146,\t\n",
      "step: 1370,\t,class_loss:0.2151,\t\n",
      "step: 1371,\t,class_loss:0.0734,\t\n",
      "step: 1372,\t,class_loss:0.0722,\t\n",
      "step: 1373,\t,class_loss:0.1698,\t\n",
      "step: 1374,\t,class_loss:0.1098,\t\n",
      "step: 1375,\t,class_loss:0.0927,\t\n",
      "step: 1376,\t,class_loss:0.1549,\t\n",
      "step: 1377,\t,class_loss:0.1393,\t\n",
      "step: 1378,\t,class_loss:0.0553,\t\n",
      "step: 1379,\t,class_loss:0.2353,\t\n",
      "step: 1380,\t,class_loss:0.0852,\t\n",
      "step: 1381,\t,class_loss:0.2399,\t\n",
      "step: 1382,\t,class_loss:0.1381,\t\n",
      "step: 1383,\t,class_loss:0.1278,\t\n",
      "step: 1384,\t,class_loss:0.0380,\t\n",
      "step: 1385,\t,class_loss:0.1596,\t\n",
      "step: 1386,\t,class_loss:0.0539,\t\n",
      "step: 1387,\t,class_loss:0.0376,\t\n",
      "step: 1388,\t,class_loss:0.0608,\t\n",
      "step: 1389,\t,class_loss:0.3704,\t\n",
      "step: 1390,\t,class_loss:0.1245,\t\n",
      "step: 1391,\t,class_loss:0.1455,\t\n",
      "step: 1392,\t,class_loss:0.2649,\t\n",
      "step: 1393,\t,class_loss:0.0501,\t\n",
      "step: 1394,\t,class_loss:0.3218,\t\n",
      "step: 1395,\t,class_loss:0.0518,\t\n",
      "step: 1396,\t,class_loss:0.0270,\t\n",
      "step: 1397,\t,class_loss:0.2325,\t\n",
      "step: 1398,\t,class_loss:0.0609,\t\n",
      "iter: 01399, \t precision: 0.7751,\t best_acc:0.7871\n",
      "step: 1399,\t,class_loss:0.0662,\t\n",
      "step: 1400,\t,class_loss:0.1062,\t\n",
      "step: 1401,\t,class_loss:0.2072,\t\n",
      "step: 1402,\t,class_loss:0.0364,\t\n",
      "step: 1403,\t,class_loss:0.1018,\t\n",
      "step: 1404,\t,class_loss:0.0372,\t\n",
      "step: 1405,\t,class_loss:0.0403,\t\n",
      "step: 1406,\t,class_loss:0.0861,\t\n",
      "step: 1407,\t,class_loss:0.1375,\t\n",
      "step: 1408,\t,class_loss:0.0237,\t\n",
      "step: 1409,\t,class_loss:0.0615,\t\n",
      "step: 1410,\t,class_loss:0.0584,\t\n",
      "step: 1411,\t,class_loss:0.0788,\t\n",
      "step: 1412,\t,class_loss:0.0409,\t\n",
      "step: 1413,\t,class_loss:0.1143,\t\n",
      "step: 1414,\t,class_loss:0.1866,\t\n",
      "step: 1415,\t,class_loss:0.0240,\t\n",
      "step: 1416,\t,class_loss:0.0597,\t\n",
      "step: 1417,\t,class_loss:0.1985,\t\n",
      "step: 1418,\t,class_loss:0.0984,\t\n",
      "step: 1419,\t,class_loss:0.1226,\t\n",
      "step: 1420,\t,class_loss:0.0331,\t\n",
      "step: 1421,\t,class_loss:0.2273,\t\n",
      "step: 1422,\t,class_loss:0.0526,\t\n",
      "step: 1423,\t,class_loss:0.0391,\t\n",
      "step: 1424,\t,class_loss:0.0569,\t\n",
      "step: 1425,\t,class_loss:0.0138,\t\n",
      "step: 1426,\t,class_loss:0.0224,\t\n",
      "step: 1427,\t,class_loss:0.0192,\t\n",
      "step: 1428,\t,class_loss:0.0558,\t\n",
      "step: 1429,\t,class_loss:0.0779,\t\n",
      "step: 1430,\t,class_loss:0.0392,\t\n",
      "step: 1431,\t,class_loss:0.0609,\t\n",
      "step: 1432,\t,class_loss:0.0560,\t\n",
      "step: 1433,\t,class_loss:0.0706,\t\n",
      "step: 1434,\t,class_loss:0.1753,\t\n",
      "step: 1435,\t,class_loss:0.0771,\t\n",
      "step: 1436,\t,class_loss:0.0329,\t\n",
      "step: 1437,\t,class_loss:0.0378,\t\n",
      "step: 1438,\t,class_loss:0.1170,\t\n",
      "step: 1439,\t,class_loss:0.0185,\t\n",
      "step: 1440,\t,class_loss:0.1781,\t\n",
      "step: 1441,\t,class_loss:0.0441,\t\n",
      "step: 1442,\t,class_loss:0.1289,\t\n",
      "step: 1443,\t,class_loss:0.1592,\t\n",
      "step: 1444,\t,class_loss:0.0696,\t\n",
      "step: 1445,\t,class_loss:0.0394,\t\n",
      "step: 1446,\t,class_loss:0.0117,\t\n",
      "step: 1447,\t,class_loss:0.2082,\t\n",
      "step: 1448,\t,class_loss:0.1016,\t\n",
      "iter: 01449, \t precision: 0.7651,\t best_acc:0.7871\n",
      "step: 1449,\t,class_loss:0.0329,\t\n",
      "step: 1450,\t,class_loss:0.0919,\t\n",
      "step: 1451,\t,class_loss:0.0502,\t\n",
      "step: 1452,\t,class_loss:0.0144,\t\n",
      "step: 1453,\t,class_loss:0.0233,\t\n",
      "step: 1454,\t,class_loss:0.1768,\t\n",
      "step: 1455,\t,class_loss:0.0506,\t\n",
      "step: 1456,\t,class_loss:0.0505,\t\n",
      "step: 1457,\t,class_loss:0.0537,\t\n",
      "step: 1458,\t,class_loss:0.0629,\t\n",
      "step: 1459,\t,class_loss:0.0381,\t\n",
      "step: 1460,\t,class_loss:0.0507,\t\n",
      "step: 1461,\t,class_loss:0.1552,\t\n",
      "step: 1462,\t,class_loss:0.0503,\t\n",
      "step: 1463,\t,class_loss:0.0887,\t\n",
      "step: 1464,\t,class_loss:0.0949,\t\n",
      "step: 1465,\t,class_loss:0.0486,\t\n",
      "step: 1466,\t,class_loss:0.1409,\t\n",
      "step: 1467,\t,class_loss:0.0736,\t\n",
      "step: 1468,\t,class_loss:0.0383,\t\n",
      "step: 1469,\t,class_loss:0.1669,\t\n",
      "step: 1470,\t,class_loss:0.0660,\t\n",
      "step: 1471,\t,class_loss:0.0286,\t\n",
      "step: 1472,\t,class_loss:0.0353,\t\n",
      "step: 1473,\t,class_loss:0.0644,\t\n",
      "step: 1474,\t,class_loss:0.0578,\t\n",
      "step: 1475,\t,class_loss:0.0847,\t\n",
      "step: 1476,\t,class_loss:0.1052,\t\n",
      "step: 1477,\t,class_loss:0.1066,\t\n",
      "step: 1478,\t,class_loss:0.1643,\t\n",
      "step: 1479,\t,class_loss:0.0316,\t\n",
      "step: 1480,\t,class_loss:0.0315,\t\n",
      "step: 1481,\t,class_loss:0.0930,\t\n",
      "step: 1482,\t,class_loss:0.2447,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1483,\t,class_loss:0.0657,\t\n",
      "step: 1484,\t,class_loss:0.1427,\t\n",
      "step: 1485,\t,class_loss:0.1874,\t\n",
      "step: 1486,\t,class_loss:0.1691,\t\n",
      "step: 1487,\t,class_loss:0.0541,\t\n",
      "step: 1488,\t,class_loss:0.0706,\t\n",
      "step: 1489,\t,class_loss:0.1566,\t\n",
      "step: 1490,\t,class_loss:0.0615,\t\n",
      "step: 1491,\t,class_loss:0.0557,\t\n",
      "step: 1492,\t,class_loss:0.1868,\t\n",
      "step: 1493,\t,class_loss:0.0905,\t\n",
      "step: 1494,\t,class_loss:0.0937,\t\n",
      "step: 1495,\t,class_loss:0.0635,\t\n",
      "step: 1496,\t,class_loss:0.2347,\t\n",
      "step: 1497,\t,class_loss:0.0327,\t\n",
      "step: 1498,\t,class_loss:0.1114,\t\n",
      "iter: 01499, \t precision: 0.7671,\t best_acc:0.7871\n",
      "step: 1499,\t,class_loss:0.1085,\t\n",
      "step: 1500,\t,class_loss:0.0938,\t\n",
      "step: 1501,\t,class_loss:0.0709,\t\n",
      "step: 1502,\t,class_loss:0.0685,\t\n",
      "step: 1503,\t,class_loss:0.0517,\t\n",
      "step: 1504,\t,class_loss:0.0450,\t\n",
      "step: 1505,\t,class_loss:0.1911,\t\n",
      "step: 1506,\t,class_loss:0.0752,\t\n",
      "step: 1507,\t,class_loss:0.1525,\t\n",
      "step: 1508,\t,class_loss:0.1958,\t\n",
      "step: 1509,\t,class_loss:0.1182,\t\n",
      "step: 1510,\t,class_loss:0.0267,\t\n",
      "step: 1511,\t,class_loss:0.1149,\t\n",
      "step: 1512,\t,class_loss:0.0328,\t\n",
      "step: 1513,\t,class_loss:0.0431,\t\n",
      "step: 1514,\t,class_loss:0.0568,\t\n",
      "step: 1515,\t,class_loss:0.0821,\t\n",
      "step: 1516,\t,class_loss:0.1582,\t\n",
      "step: 1517,\t,class_loss:0.0735,\t\n",
      "step: 1518,\t,class_loss:0.1296,\t\n",
      "step: 1519,\t,class_loss:0.0358,\t\n",
      "step: 1520,\t,class_loss:0.0598,\t\n",
      "step: 1521,\t,class_loss:0.2106,\t\n",
      "step: 1522,\t,class_loss:0.1343,\t\n",
      "step: 1523,\t,class_loss:0.0718,\t\n",
      "step: 1524,\t,class_loss:0.1029,\t\n",
      "step: 1525,\t,class_loss:0.0385,\t\n",
      "step: 1526,\t,class_loss:0.0759,\t\n",
      "step: 1527,\t,class_loss:0.0661,\t\n",
      "step: 1528,\t,class_loss:0.0675,\t\n",
      "step: 1529,\t,class_loss:0.0863,\t\n",
      "step: 1530,\t,class_loss:0.1155,\t\n",
      "step: 1531,\t,class_loss:0.1522,\t\n",
      "step: 1532,\t,class_loss:0.0601,\t\n",
      "step: 1533,\t,class_loss:0.1262,\t\n",
      "step: 1534,\t,class_loss:0.0166,\t\n",
      "step: 1535,\t,class_loss:0.3039,\t\n",
      "step: 1536,\t,class_loss:0.0336,\t\n",
      "step: 1537,\t,class_loss:0.1347,\t\n",
      "step: 1538,\t,class_loss:0.0914,\t\n",
      "step: 1539,\t,class_loss:0.0851,\t\n",
      "step: 1540,\t,class_loss:0.0878,\t\n",
      "step: 1541,\t,class_loss:0.0529,\t\n",
      "step: 1542,\t,class_loss:0.2782,\t\n",
      "step: 1543,\t,class_loss:0.0451,\t\n",
      "step: 1544,\t,class_loss:0.0666,\t\n",
      "step: 1545,\t,class_loss:0.1177,\t\n",
      "step: 1546,\t,class_loss:0.1164,\t\n",
      "step: 1547,\t,class_loss:0.1696,\t\n",
      "step: 1548,\t,class_loss:0.0235,\t\n",
      "iter: 01549, \t precision: 0.7731,\t best_acc:0.7871\n",
      "step: 1549,\t,class_loss:0.0702,\t\n",
      "step: 1550,\t,class_loss:0.0857,\t\n",
      "step: 1551,\t,class_loss:0.0441,\t\n",
      "step: 1552,\t,class_loss:0.0856,\t\n",
      "step: 1553,\t,class_loss:0.1366,\t\n",
      "step: 1554,\t,class_loss:0.1725,\t\n",
      "step: 1555,\t,class_loss:0.0652,\t\n",
      "step: 1556,\t,class_loss:0.0355,\t\n",
      "step: 1557,\t,class_loss:0.0927,\t\n",
      "step: 1558,\t,class_loss:0.2757,\t\n",
      "step: 1559,\t,class_loss:0.0712,\t\n",
      "step: 1560,\t,class_loss:0.0457,\t\n",
      "step: 1561,\t,class_loss:0.0443,\t\n",
      "step: 1562,\t,class_loss:0.0919,\t\n",
      "step: 1563,\t,class_loss:0.0846,\t\n",
      "step: 1564,\t,class_loss:0.0647,\t\n",
      "step: 1565,\t,class_loss:0.0740,\t\n",
      "step: 1566,\t,class_loss:0.2159,\t\n",
      "step: 1567,\t,class_loss:0.0613,\t\n",
      "step: 1568,\t,class_loss:0.0408,\t\n",
      "step: 1569,\t,class_loss:0.0501,\t\n",
      "step: 1570,\t,class_loss:0.0568,\t\n",
      "step: 1571,\t,class_loss:0.0161,\t\n",
      "step: 1572,\t,class_loss:0.1420,\t\n",
      "step: 1573,\t,class_loss:0.0299,\t\n",
      "step: 1574,\t,class_loss:0.0585,\t\n",
      "step: 1575,\t,class_loss:0.1029,\t\n",
      "step: 1576,\t,class_loss:0.1772,\t\n",
      "step: 1577,\t,class_loss:0.0594,\t\n",
      "step: 1578,\t,class_loss:0.1235,\t\n",
      "step: 1579,\t,class_loss:0.0521,\t\n",
      "step: 1580,\t,class_loss:0.0407,\t\n",
      "step: 1581,\t,class_loss:0.0489,\t\n",
      "step: 1582,\t,class_loss:0.0950,\t\n",
      "step: 1583,\t,class_loss:0.2007,\t\n",
      "step: 1584,\t,class_loss:0.0742,\t\n",
      "step: 1585,\t,class_loss:0.0538,\t\n",
      "step: 1586,\t,class_loss:0.0988,\t\n",
      "step: 1587,\t,class_loss:0.0294,\t\n",
      "step: 1588,\t,class_loss:0.0949,\t\n",
      "step: 1589,\t,class_loss:0.0320,\t\n",
      "step: 1590,\t,class_loss:0.0466,\t\n",
      "step: 1591,\t,class_loss:0.1138,\t\n",
      "step: 1592,\t,class_loss:0.1358,\t\n",
      "step: 1593,\t,class_loss:0.0487,\t\n",
      "step: 1594,\t,class_loss:0.0467,\t\n",
      "step: 1595,\t,class_loss:0.0971,\t\n",
      "step: 1596,\t,class_loss:0.1551,\t\n",
      "step: 1597,\t,class_loss:0.0512,\t\n",
      "step: 1598,\t,class_loss:0.0417,\t\n",
      "iter: 01599, \t precision: 0.7671,\t best_acc:0.7871\n",
      "step: 1599,\t,class_loss:0.1003,\t\n",
      "step: 1600,\t,class_loss:0.0542,\t\n",
      "step: 1601,\t,class_loss:0.0478,\t\n",
      "step: 1602,\t,class_loss:0.0509,\t\n",
      "step: 1603,\t,class_loss:0.0366,\t\n",
      "step: 1604,\t,class_loss:0.0738,\t\n",
      "step: 1605,\t,class_loss:0.0457,\t\n",
      "step: 1606,\t,class_loss:0.0368,\t\n",
      "step: 1607,\t,class_loss:0.0366,\t\n",
      "step: 1608,\t,class_loss:0.0680,\t\n",
      "step: 1609,\t,class_loss:0.0344,\t\n",
      "step: 1610,\t,class_loss:0.0584,\t\n",
      "step: 1611,\t,class_loss:0.2271,\t\n",
      "step: 1612,\t,class_loss:0.1034,\t\n",
      "step: 1613,\t,class_loss:0.1541,\t\n",
      "step: 1614,\t,class_loss:0.0500,\t\n",
      "step: 1615,\t,class_loss:0.3545,\t\n",
      "step: 1616,\t,class_loss:0.0814,\t\n",
      "step: 1617,\t,class_loss:0.1031,\t\n",
      "step: 1618,\t,class_loss:0.0637,\t\n",
      "step: 1619,\t,class_loss:0.0852,\t\n",
      "step: 1620,\t,class_loss:0.0992,\t\n",
      "step: 1621,\t,class_loss:0.2302,\t\n",
      "step: 1622,\t,class_loss:0.0695,\t\n",
      "step: 1623,\t,class_loss:0.2830,\t\n",
      "step: 1624,\t,class_loss:0.0394,\t\n",
      "step: 1625,\t,class_loss:0.0784,\t\n",
      "step: 1626,\t,class_loss:0.0174,\t\n",
      "step: 1627,\t,class_loss:0.0635,\t\n",
      "step: 1628,\t,class_loss:0.0282,\t\n",
      "step: 1629,\t,class_loss:0.1650,\t\n",
      "step: 1630,\t,class_loss:0.0567,\t\n",
      "step: 1631,\t,class_loss:0.0329,\t\n",
      "step: 1632,\t,class_loss:0.1482,\t\n",
      "step: 1633,\t,class_loss:0.0480,\t\n",
      "step: 1634,\t,class_loss:0.0439,\t\n",
      "step: 1635,\t,class_loss:0.0554,\t\n",
      "step: 1636,\t,class_loss:0.0647,\t\n",
      "step: 1637,\t,class_loss:0.0174,\t\n",
      "step: 1638,\t,class_loss:0.0377,\t\n",
      "step: 1639,\t,class_loss:0.3002,\t\n",
      "step: 1640,\t,class_loss:0.0407,\t\n",
      "step: 1641,\t,class_loss:0.0479,\t\n",
      "step: 1642,\t,class_loss:0.1137,\t\n",
      "step: 1643,\t,class_loss:0.1153,\t\n",
      "step: 1644,\t,class_loss:0.0477,\t\n",
      "step: 1645,\t,class_loss:0.0425,\t\n",
      "step: 1646,\t,class_loss:0.3319,\t\n",
      "step: 1647,\t,class_loss:0.0866,\t\n",
      "step: 1648,\t,class_loss:0.1252,\t\n",
      "iter: 01649, \t precision: 0.7691,\t best_acc:0.7871\n",
      "step: 1649,\t,class_loss:0.0312,\t\n",
      "step: 1650,\t,class_loss:0.0336,\t\n",
      "step: 1651,\t,class_loss:0.0301,\t\n",
      "step: 1652,\t,class_loss:0.0296,\t\n",
      "step: 1653,\t,class_loss:0.1033,\t\n",
      "step: 1654,\t,class_loss:0.0477,\t\n",
      "step: 1655,\t,class_loss:0.0442,\t\n",
      "step: 1656,\t,class_loss:0.1115,\t\n",
      "step: 1657,\t,class_loss:0.0154,\t\n",
      "step: 1658,\t,class_loss:0.0562,\t\n",
      "step: 1659,\t,class_loss:0.0338,\t\n",
      "step: 1660,\t,class_loss:0.1676,\t\n",
      "step: 1661,\t,class_loss:0.0628,\t\n",
      "step: 1662,\t,class_loss:0.1055,\t\n",
      "step: 1663,\t,class_loss:0.0654,\t\n",
      "step: 1664,\t,class_loss:0.1501,\t\n",
      "step: 1665,\t,class_loss:0.0308,\t\n",
      "step: 1666,\t,class_loss:0.0371,\t\n",
      "step: 1667,\t,class_loss:0.0253,\t\n",
      "step: 1668,\t,class_loss:0.0239,\t\n",
      "step: 1669,\t,class_loss:0.0347,\t\n",
      "step: 1670,\t,class_loss:0.1860,\t\n",
      "step: 1671,\t,class_loss:0.1536,\t\n",
      "step: 1672,\t,class_loss:0.0685,\t\n",
      "step: 1673,\t,class_loss:0.0822,\t\n",
      "step: 1674,\t,class_loss:0.0815,\t\n",
      "step: 1675,\t,class_loss:0.0460,\t\n",
      "step: 1676,\t,class_loss:0.0207,\t\n",
      "step: 1677,\t,class_loss:0.0477,\t\n",
      "step: 1678,\t,class_loss:0.0284,\t\n",
      "step: 1679,\t,class_loss:0.1195,\t\n",
      "step: 1680,\t,class_loss:0.0385,\t\n",
      "step: 1681,\t,class_loss:0.1823,\t\n",
      "step: 1682,\t,class_loss:0.1108,\t\n",
      "step: 1683,\t,class_loss:0.0484,\t\n",
      "step: 1684,\t,class_loss:0.0717,\t\n",
      "step: 1685,\t,class_loss:0.1547,\t\n",
      "step: 1686,\t,class_loss:0.0557,\t\n",
      "step: 1687,\t,class_loss:0.1285,\t\n",
      "step: 1688,\t,class_loss:0.0847,\t\n",
      "step: 1689,\t,class_loss:0.0547,\t\n",
      "step: 1690,\t,class_loss:0.1585,\t\n",
      "step: 1691,\t,class_loss:0.1323,\t\n",
      "step: 1692,\t,class_loss:0.0696,\t\n",
      "step: 1693,\t,class_loss:0.0314,\t\n",
      "step: 1694,\t,class_loss:0.0834,\t\n",
      "step: 1695,\t,class_loss:0.0239,\t\n",
      "step: 1696,\t,class_loss:0.1233,\t\n",
      "step: 1697,\t,class_loss:0.0422,\t\n",
      "step: 1698,\t,class_loss:0.0634,\t\n",
      "iter: 01699, \t precision: 0.7671,\t best_acc:0.7871\n",
      "step: 1699,\t,class_loss:0.1292,\t\n",
      "step: 1700,\t,class_loss:0.1519,\t\n",
      "step: 1701,\t,class_loss:0.0543,\t\n",
      "step: 1702,\t,class_loss:0.0294,\t\n",
      "step: 1703,\t,class_loss:0.1488,\t\n",
      "step: 1704,\t,class_loss:0.1985,\t\n",
      "step: 1705,\t,class_loss:0.0444,\t\n",
      "step: 1706,\t,class_loss:0.1046,\t\n",
      "step: 1707,\t,class_loss:0.0586,\t\n",
      "step: 1708,\t,class_loss:0.0579,\t\n",
      "step: 1709,\t,class_loss:0.1116,\t\n",
      "step: 1710,\t,class_loss:0.0261,\t\n",
      "step: 1711,\t,class_loss:0.0327,\t\n",
      "step: 1712,\t,class_loss:0.0428,\t\n",
      "step: 1713,\t,class_loss:0.0217,\t\n",
      "step: 1714,\t,class_loss:0.0449,\t\n",
      "step: 1715,\t,class_loss:0.0353,\t\n",
      "step: 1716,\t,class_loss:0.0333,\t\n",
      "step: 1717,\t,class_loss:0.1031,\t\n",
      "step: 1718,\t,class_loss:0.1772,\t\n",
      "step: 1719,\t,class_loss:0.0300,\t\n",
      "step: 1720,\t,class_loss:0.0751,\t\n",
      "step: 1721,\t,class_loss:0.0354,\t\n",
      "step: 1722,\t,class_loss:0.2075,\t\n",
      "step: 1723,\t,class_loss:0.0394,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1724,\t,class_loss:0.1179,\t\n",
      "step: 1725,\t,class_loss:0.0518,\t\n",
      "step: 1726,\t,class_loss:0.0584,\t\n",
      "step: 1727,\t,class_loss:0.0207,\t\n",
      "step: 1728,\t,class_loss:0.0393,\t\n",
      "step: 1729,\t,class_loss:0.0628,\t\n",
      "step: 1730,\t,class_loss:0.0435,\t\n",
      "step: 1731,\t,class_loss:0.0324,\t\n",
      "step: 1732,\t,class_loss:0.0334,\t\n",
      "step: 1733,\t,class_loss:0.0839,\t\n",
      "step: 1734,\t,class_loss:0.0789,\t\n",
      "step: 1735,\t,class_loss:0.0350,\t\n",
      "step: 1736,\t,class_loss:0.0435,\t\n",
      "step: 1737,\t,class_loss:0.0286,\t\n",
      "step: 1738,\t,class_loss:0.0099,\t\n",
      "step: 1739,\t,class_loss:0.1105,\t\n",
      "step: 1740,\t,class_loss:0.1710,\t\n",
      "step: 1741,\t,class_loss:0.0611,\t\n",
      "step: 1742,\t,class_loss:0.1094,\t\n",
      "step: 1743,\t,class_loss:0.0491,\t\n",
      "step: 1744,\t,class_loss:0.1142,\t\n",
      "step: 1745,\t,class_loss:0.1175,\t\n",
      "step: 1746,\t,class_loss:0.3527,\t\n",
      "step: 1747,\t,class_loss:0.0728,\t\n",
      "step: 1748,\t,class_loss:0.2496,\t\n",
      "iter: 01749, \t precision: 0.7831,\t best_acc:0.7871\n",
      "step: 1749,\t,class_loss:0.1530,\t\n",
      "step: 1750,\t,class_loss:0.0524,\t\n",
      "step: 1751,\t,class_loss:0.1089,\t\n",
      "step: 1752,\t,class_loss:0.1907,\t\n",
      "step: 1753,\t,class_loss:0.0193,\t\n",
      "step: 1754,\t,class_loss:0.0845,\t\n",
      "step: 1755,\t,class_loss:0.0213,\t\n",
      "step: 1756,\t,class_loss:0.0347,\t\n",
      "step: 1757,\t,class_loss:0.0412,\t\n",
      "step: 1758,\t,class_loss:0.0562,\t\n",
      "step: 1759,\t,class_loss:0.1309,\t\n",
      "step: 1760,\t,class_loss:0.0744,\t\n",
      "step: 1761,\t,class_loss:0.0159,\t\n",
      "step: 1762,\t,class_loss:0.0333,\t\n",
      "step: 1763,\t,class_loss:0.0253,\t\n",
      "step: 1764,\t,class_loss:0.0640,\t\n",
      "step: 1765,\t,class_loss:0.0545,\t\n",
      "step: 1766,\t,class_loss:0.0649,\t\n",
      "step: 1767,\t,class_loss:0.0326,\t\n",
      "step: 1768,\t,class_loss:0.0203,\t\n",
      "step: 1769,\t,class_loss:0.1498,\t\n",
      "step: 1770,\t,class_loss:0.0398,\t\n",
      "step: 1771,\t,class_loss:0.1606,\t\n",
      "step: 1772,\t,class_loss:0.0674,\t\n",
      "step: 1773,\t,class_loss:0.0312,\t\n",
      "step: 1774,\t,class_loss:0.0368,\t\n",
      "step: 1775,\t,class_loss:0.1404,\t\n",
      "step: 1776,\t,class_loss:0.1748,\t\n",
      "step: 1777,\t,class_loss:0.0701,\t\n",
      "step: 1778,\t,class_loss:0.0500,\t\n",
      "step: 1779,\t,class_loss:0.1710,\t\n",
      "step: 1780,\t,class_loss:0.0732,\t\n",
      "step: 1781,\t,class_loss:0.0615,\t\n",
      "step: 1782,\t,class_loss:0.0301,\t\n",
      "step: 1783,\t,class_loss:0.0162,\t\n",
      "step: 1784,\t,class_loss:0.0394,\t\n",
      "step: 1785,\t,class_loss:0.0300,\t\n",
      "step: 1786,\t,class_loss:0.0852,\t\n",
      "step: 1787,\t,class_loss:0.0204,\t\n",
      "step: 1788,\t,class_loss:0.0957,\t\n",
      "step: 1789,\t,class_loss:0.1906,\t\n",
      "step: 1790,\t,class_loss:0.2236,\t\n",
      "step: 1791,\t,class_loss:0.1911,\t\n",
      "step: 1792,\t,class_loss:0.0069,\t\n",
      "step: 1793,\t,class_loss:0.0356,\t\n",
      "step: 1794,\t,class_loss:0.0409,\t\n",
      "step: 1795,\t,class_loss:0.0448,\t\n",
      "step: 1796,\t,class_loss:0.1546,\t\n",
      "step: 1797,\t,class_loss:0.0281,\t\n",
      "step: 1798,\t,class_loss:0.0558,\t\n",
      "iter: 01799, \t precision: 0.7751,\t best_acc:0.7871\n",
      "step: 1799,\t,class_loss:0.0510,\t\n",
      "step: 1800,\t,class_loss:0.0973,\t\n",
      "step: 1801,\t,class_loss:0.0328,\t\n",
      "step: 1802,\t,class_loss:0.0668,\t\n",
      "step: 1803,\t,class_loss:0.0550,\t\n",
      "step: 1804,\t,class_loss:0.0755,\t\n",
      "step: 1805,\t,class_loss:0.0538,\t\n",
      "step: 1806,\t,class_loss:0.0842,\t\n",
      "step: 1807,\t,class_loss:0.0374,\t\n",
      "step: 1808,\t,class_loss:0.1558,\t\n",
      "step: 1809,\t,class_loss:0.1096,\t\n",
      "step: 1810,\t,class_loss:0.0975,\t\n",
      "step: 1811,\t,class_loss:0.0560,\t\n",
      "step: 1812,\t,class_loss:0.0485,\t\n",
      "step: 1813,\t,class_loss:0.0757,\t\n",
      "step: 1814,\t,class_loss:0.0262,\t\n",
      "step: 1815,\t,class_loss:0.0681,\t\n",
      "step: 1816,\t,class_loss:0.1202,\t\n",
      "step: 1817,\t,class_loss:0.0789,\t\n",
      "step: 1818,\t,class_loss:0.0907,\t\n",
      "step: 1819,\t,class_loss:0.0408,\t\n",
      "step: 1820,\t,class_loss:0.0118,\t\n",
      "step: 1821,\t,class_loss:0.0372,\t\n",
      "step: 1822,\t,class_loss:0.1412,\t\n",
      "step: 1823,\t,class_loss:0.0190,\t\n",
      "step: 1824,\t,class_loss:0.0413,\t\n",
      "step: 1825,\t,class_loss:0.1261,\t\n",
      "step: 1826,\t,class_loss:0.0735,\t\n",
      "step: 1827,\t,class_loss:0.0165,\t\n",
      "step: 1828,\t,class_loss:0.0830,\t\n",
      "step: 1829,\t,class_loss:0.0235,\t\n",
      "step: 1830,\t,class_loss:0.0388,\t\n",
      "step: 1831,\t,class_loss:0.0258,\t\n",
      "step: 1832,\t,class_loss:0.0120,\t\n",
      "step: 1833,\t,class_loss:0.0345,\t\n",
      "step: 1834,\t,class_loss:0.0662,\t\n",
      "step: 1835,\t,class_loss:0.0974,\t\n",
      "step: 1836,\t,class_loss:0.0740,\t\n",
      "step: 1837,\t,class_loss:0.0446,\t\n",
      "step: 1838,\t,class_loss:0.0514,\t\n",
      "step: 1839,\t,class_loss:0.0471,\t\n",
      "step: 1840,\t,class_loss:0.0500,\t\n",
      "step: 1841,\t,class_loss:0.0676,\t\n",
      "step: 1842,\t,class_loss:0.0181,\t\n",
      "step: 1843,\t,class_loss:0.0899,\t\n",
      "step: 1844,\t,class_loss:0.0501,\t\n",
      "step: 1845,\t,class_loss:0.0813,\t\n",
      "step: 1846,\t,class_loss:0.0532,\t\n",
      "step: 1847,\t,class_loss:0.0651,\t\n",
      "step: 1848,\t,class_loss:0.0596,\t\n",
      "iter: 01849, \t precision: 0.7831,\t best_acc:0.7871\n",
      "step: 1849,\t,class_loss:0.1248,\t\n",
      "step: 1850,\t,class_loss:0.1262,\t\n",
      "step: 1851,\t,class_loss:0.0191,\t\n",
      "step: 1852,\t,class_loss:0.1160,\t\n",
      "step: 1853,\t,class_loss:0.0414,\t\n",
      "step: 1854,\t,class_loss:0.0616,\t\n",
      "step: 1855,\t,class_loss:0.0317,\t\n",
      "step: 1856,\t,class_loss:0.0546,\t\n",
      "step: 1857,\t,class_loss:0.0431,\t\n",
      "step: 1858,\t,class_loss:0.0774,\t\n",
      "step: 1859,\t,class_loss:0.0599,\t\n",
      "step: 1860,\t,class_loss:0.2753,\t\n",
      "step: 1861,\t,class_loss:0.0322,\t\n",
      "step: 1862,\t,class_loss:0.0184,\t\n",
      "step: 1863,\t,class_loss:0.0200,\t\n",
      "step: 1864,\t,class_loss:0.0791,\t\n",
      "step: 1865,\t,class_loss:0.0356,\t\n",
      "step: 1866,\t,class_loss:0.1017,\t\n",
      "step: 1867,\t,class_loss:0.0978,\t\n",
      "step: 1868,\t,class_loss:0.0672,\t\n",
      "step: 1869,\t,class_loss:0.0272,\t\n",
      "step: 1870,\t,class_loss:0.0967,\t\n",
      "step: 1871,\t,class_loss:0.0500,\t\n",
      "step: 1872,\t,class_loss:0.0443,\t\n",
      "step: 1873,\t,class_loss:0.0342,\t\n",
      "step: 1874,\t,class_loss:0.0177,\t\n",
      "step: 1875,\t,class_loss:0.0302,\t\n",
      "step: 1876,\t,class_loss:0.0225,\t\n",
      "step: 1877,\t,class_loss:0.0769,\t\n",
      "step: 1878,\t,class_loss:0.0462,\t\n",
      "step: 1879,\t,class_loss:0.0502,\t\n",
      "step: 1880,\t,class_loss:0.0628,\t\n",
      "step: 1881,\t,class_loss:0.0161,\t\n",
      "step: 1882,\t,class_loss:0.0263,\t\n",
      "step: 1883,\t,class_loss:0.1025,\t\n",
      "step: 1884,\t,class_loss:0.1275,\t\n",
      "step: 1885,\t,class_loss:0.0186,\t\n",
      "step: 1886,\t,class_loss:0.0736,\t\n",
      "step: 1887,\t,class_loss:0.0843,\t\n",
      "step: 1888,\t,class_loss:0.0339,\t\n",
      "step: 1889,\t,class_loss:0.0444,\t\n",
      "step: 1890,\t,class_loss:0.0719,\t\n",
      "step: 1891,\t,class_loss:0.0428,\t\n",
      "step: 1892,\t,class_loss:0.0921,\t\n",
      "step: 1893,\t,class_loss:0.0364,\t\n",
      "step: 1894,\t,class_loss:0.0842,\t\n",
      "step: 1895,\t,class_loss:0.0361,\t\n",
      "step: 1896,\t,class_loss:0.0171,\t\n",
      "step: 1897,\t,class_loss:0.1274,\t\n",
      "step: 1898,\t,class_loss:0.0556,\t\n",
      "iter: 01899, \t precision: 0.7831,\t best_acc:0.7871\n",
      "step: 1899,\t,class_loss:0.0291,\t\n",
      "step: 1900,\t,class_loss:0.0330,\t\n",
      "step: 1901,\t,class_loss:0.0147,\t\n",
      "step: 1902,\t,class_loss:0.1248,\t\n",
      "step: 1903,\t,class_loss:0.0157,\t\n",
      "step: 1904,\t,class_loss:0.1411,\t\n",
      "step: 1905,\t,class_loss:0.0767,\t\n",
      "step: 1906,\t,class_loss:0.1271,\t\n",
      "step: 1907,\t,class_loss:0.0935,\t\n",
      "step: 1908,\t,class_loss:0.1290,\t\n",
      "step: 1909,\t,class_loss:0.0619,\t\n",
      "step: 1910,\t,class_loss:0.0329,\t\n",
      "step: 1911,\t,class_loss:0.0219,\t\n",
      "step: 1912,\t,class_loss:0.0351,\t\n",
      "step: 1913,\t,class_loss:0.0405,\t\n",
      "step: 1914,\t,class_loss:0.0564,\t\n",
      "step: 1915,\t,class_loss:0.0441,\t\n",
      "step: 1916,\t,class_loss:0.1403,\t\n",
      "step: 1917,\t,class_loss:0.0773,\t\n",
      "step: 1918,\t,class_loss:0.2138,\t\n",
      "step: 1919,\t,class_loss:0.0325,\t\n",
      "step: 1920,\t,class_loss:0.0301,\t\n",
      "step: 1921,\t,class_loss:0.0290,\t\n",
      "step: 1922,\t,class_loss:0.0235,\t\n",
      "step: 1923,\t,class_loss:0.0592,\t\n",
      "step: 1924,\t,class_loss:0.0817,\t\n",
      "step: 1925,\t,class_loss:0.0591,\t\n",
      "step: 1926,\t,class_loss:0.1038,\t\n",
      "step: 1927,\t,class_loss:0.0571,\t\n",
      "step: 1928,\t,class_loss:0.0518,\t\n",
      "step: 1929,\t,class_loss:0.1053,\t\n",
      "step: 1930,\t,class_loss:0.0529,\t\n",
      "step: 1931,\t,class_loss:0.1199,\t\n",
      "step: 1932,\t,class_loss:0.0170,\t\n",
      "step: 1933,\t,class_loss:0.1000,\t\n",
      "step: 1934,\t,class_loss:0.0355,\t\n",
      "step: 1935,\t,class_loss:0.0880,\t\n",
      "step: 1936,\t,class_loss:0.0332,\t\n",
      "step: 1937,\t,class_loss:0.0474,\t\n",
      "step: 1938,\t,class_loss:0.1067,\t\n",
      "step: 1939,\t,class_loss:0.1222,\t\n",
      "step: 1940,\t,class_loss:0.0278,\t\n",
      "step: 1941,\t,class_loss:0.0294,\t\n",
      "step: 1942,\t,class_loss:0.0850,\t\n",
      "step: 1943,\t,class_loss:0.0178,\t\n",
      "step: 1944,\t,class_loss:0.0618,\t\n",
      "step: 1945,\t,class_loss:0.1855,\t\n",
      "step: 1946,\t,class_loss:0.0754,\t\n",
      "step: 1947,\t,class_loss:0.0149,\t\n",
      "step: 1948,\t,class_loss:0.0319,\t\n",
      "iter: 01949, \t precision: 0.7811,\t best_acc:0.7871\n",
      "step: 1949,\t,class_loss:0.0561,\t\n",
      "step: 1950,\t,class_loss:0.1266,\t\n",
      "step: 1951,\t,class_loss:0.0369,\t\n",
      "step: 1952,\t,class_loss:0.0444,\t\n",
      "step: 1953,\t,class_loss:0.0227,\t\n",
      "step: 1954,\t,class_loss:0.0565,\t\n",
      "step: 1955,\t,class_loss:0.0844,\t\n",
      "step: 1956,\t,class_loss:0.0375,\t\n",
      "step: 1957,\t,class_loss:0.0326,\t\n",
      "step: 1958,\t,class_loss:0.0427,\t\n",
      "step: 1959,\t,class_loss:0.0395,\t\n",
      "step: 1960,\t,class_loss:0.0508,\t\n",
      "step: 1961,\t,class_loss:0.0272,\t\n",
      "step: 1962,\t,class_loss:0.0332,\t\n",
      "step: 1963,\t,class_loss:0.0497,\t\n",
      "step: 1964,\t,class_loss:0.0279,\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1965,\t,class_loss:0.0494,\t\n",
      "step: 1966,\t,class_loss:0.0725,\t\n",
      "step: 1967,\t,class_loss:0.0240,\t\n",
      "step: 1968,\t,class_loss:0.0105,\t\n",
      "step: 1969,\t,class_loss:0.0234,\t\n",
      "step: 1970,\t,class_loss:0.0235,\t\n",
      "step: 1971,\t,class_loss:0.0490,\t\n",
      "step: 1972,\t,class_loss:0.1053,\t\n",
      "step: 1973,\t,class_loss:0.1139,\t\n",
      "step: 1974,\t,class_loss:0.0138,\t\n",
      "step: 1975,\t,class_loss:0.1671,\t\n",
      "step: 1976,\t,class_loss:0.1253,\t\n",
      "step: 1977,\t,class_loss:0.0225,\t\n",
      "step: 1978,\t,class_loss:0.0576,\t\n",
      "step: 1979,\t,class_loss:0.0452,\t\n",
      "step: 1980,\t,class_loss:0.0389,\t\n",
      "step: 1981,\t,class_loss:0.0570,\t\n",
      "step: 1982,\t,class_loss:0.1941,\t\n",
      "step: 1983,\t,class_loss:0.0798,\t\n",
      "step: 1984,\t,class_loss:0.1091,\t\n",
      "step: 1985,\t,class_loss:0.0201,\t\n",
      "step: 1986,\t,class_loss:0.0490,\t\n",
      "step: 1987,\t,class_loss:0.0447,\t\n",
      "step: 1988,\t,class_loss:0.1476,\t\n",
      "step: 1989,\t,class_loss:0.0374,\t\n",
      "step: 1990,\t,class_loss:0.1416,\t\n",
      "step: 1991,\t,class_loss:0.0480,\t\n",
      "step: 1992,\t,class_loss:0.0353,\t\n",
      "step: 1993,\t,class_loss:0.0521,\t\n",
      "step: 1994,\t,class_loss:0.0361,\t\n",
      "step: 1995,\t,class_loss:0.1727,\t\n",
      "step: 1996,\t,class_loss:0.1002,\t\n",
      "step: 1997,\t,class_loss:0.0167,\t\n",
      "step: 1998,\t,class_loss:0.0380,\t\n",
      "iter: 01999, \t precision: 0.7871,\t best_acc:0.7871\n",
      "step: 1999,\t,class_loss:0.0248,\t\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import network\n",
    "import loss\n",
    "import pre_process as prep\n",
    "import lr_schedule\n",
    "from pre_process import ImageList, image_classification_test\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Code for RSDA-MSTN')\n",
    "    parser.add_argument('--gpu_id', type=str, nargs='?', default='1', help=\"device id to run\")\n",
    "    parser.add_argument('--source', type=str, default='amazon',choices=[\"amazon\", \"dslr\",\"webcam\"])\n",
    "    parser.add_argument('--target', type=str, default='dslr', choices=[\"amazon\", \"dslr\", \"webcam\"])\n",
    "    parser.add_argument('--test_interval', type=int, default=50, help=\"interval of two continuous test phase\")\n",
    "    parser.add_argument('--snapshot_interval', type=int, default=1000, help=\"interval of two continuous output model\")\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help=\"learning rate\")\n",
    "    parser.add_argument('--stages', type=int, default=6, help=\"training stages\")\n",
    "    args = parser.parse_args([])\n",
    "    s_dset_path = 'data/office/' + args.source + '_list.txt' #'../../data/office/' + args.source + '_list.txt'\n",
    "    t_dset_path = 'data/office/' + args.target + '_list.txt' #'../../data/office/' + args.target + '_list.txt'\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "    config = {}\n",
    "    config[\"source\"] = args.source\n",
    "    config[\"target\"] = args.target\n",
    "    config[\"gpu\"] = args.gpu_id\n",
    "    config[\"test_interval\"] = args.test_interval\n",
    "    config[\"snapshot_interval\"] = args.snapshot_interval\n",
    "    config[\"output_for_test\"] = True\n",
    "    config[\"output_path\"] = \"snapshot/init\"\n",
    "    if not osp.exists(config[\"output_path\"]):\n",
    "        os.makedirs(config[\"output_path\"])\n",
    "    config[\"out_file\"] = open(osp.join(config[\"output_path\"],args.source+\"_\"+args.target+ \"class_log.txt\"), \"w\")\n",
    "\n",
    "    config[\"prep\"] = {'params':{\"resize_size\":256, \"crop_size\":224}}\n",
    "    config[\"network\"] = {\"name\":network.ResNet50, \\\n",
    "            \"params\":{\"new_cls\":True,\"feature_dim\":256,\"class_num\":31} }\n",
    "    config[\"optimizer\"] = {\"type\":optim.SGD, \"optim_params\":{'lr':args.lr, \"momentum\":0.9, \\\n",
    "                           \"weight_decay\":0.0005, \"nesterov\":True}, \"lr_type\":\"inv\", \\\n",
    "                           \"lr_param\":{\"lr\":args.lr, \"gamma\":0.001, \"power\":0.75} }\n",
    "    config[\"data\"] = {\"source\":{\"list_path\":s_dset_path, \"batch_size\":18}, \\\n",
    "                      \"target\":{\"list_path\":t_dset_path, \"batch_size\":18}, \\\n",
    "                      \"test\":{\"list_path\":t_dset_path, \"batch_size\":36}}\n",
    "    config[\"out_file\"].flush()\n",
    "    if config[\"source\"] == \"amazon\" and config[\"target\"] == \"dslr\":\n",
    "        config[\"iterations\"] = 2000\n",
    "        seed = 0\n",
    "    elif config[\"source\"] == \"amazon\" and config[\"target\"] == \"webcam\":\n",
    "        config[\"iterations\"] = 2000\n",
    "        seed = 0\n",
    "    else:\n",
    "        config[\"iterations\"] = 4000\n",
    "        seed = 1\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    config[\"out_file\"].write('\\n--- initialization ---\\n')\n",
    "    source = config[\"source\"]\n",
    "    target = config[\"target\"]\n",
    "    prep_dict = {}\n",
    "    prep_dict[\"source\"] = prep.image_train(**config[\"prep\"]['params'])\n",
    "    prep_dict[\"target\"] = prep.image_train(**config[\"prep\"]['params'])\n",
    "\n",
    "    prep_dict[\"test\"] = prep.image_test(**config[\"prep\"]['params'])\n",
    "\n",
    "    ## prepare data\n",
    "    dsets = {}\n",
    "    dset_loaders = {}\n",
    "    data_config = config[\"data\"]\n",
    "    train_bs = data_config[\"source\"][\"batch_size\"]\n",
    "    test_bs = data_config[\"test\"][\"batch_size\"]\n",
    "    dsets[\"source\"] = ImageList(open(data_config[\"source\"][\"list_path\"]).readlines(), \\\n",
    "                                transform=prep_dict[\"source\"])\n",
    "    dset_loaders[\"source\"] = DataLoader(dsets[\"source\"], batch_size=train_bs, \\\n",
    "                                        shuffle=True, num_workers=0, drop_last=True)\n",
    "    dsets[\"target\"] = ImageList(open(data_config[\"target\"][\"list_path\"]).readlines(), \\\n",
    "                                transform=prep_dict[\"target\"])\n",
    "    dset_loaders[\"target\"] = DataLoader(dsets[\"target\"], batch_size=train_bs, \\\n",
    "                                        shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "    dsets[\"test\"] = ImageList(open(data_config[\"test\"][\"list_path\"]).readlines(), \\\n",
    "                              transform=prep_dict[\"test\"])\n",
    "    dset_loaders[\"test\"] = DataLoader(dsets[\"test\"], batch_size=test_bs, \\\n",
    "                                      shuffle=False, num_workers=0)\n",
    "\n",
    "    class_num = config[\"network\"][\"params\"][\"class_num\"]\n",
    "\n",
    "    ## set base network\n",
    "    net_config = config[\"network\"]\n",
    "    base_network = net_config[\"name\"](**net_config[\"params\"])\n",
    "    base_network = base_network.cuda()\n",
    "\n",
    "    ## add additional network for some methods\n",
    "    ad_net = network.AdversarialNetwork(base_network.output_num(), 1024)\n",
    "    ad_net = ad_net.cuda()\n",
    "\n",
    "    gpus = config['gpu'].split(',')\n",
    "    if len(gpus) > 1:\n",
    "        ad_net = nn.DataParallel(ad_net)\n",
    "        base_network = nn.DataParallel(base_network)\n",
    "\n",
    "    parameter_classifier = [base_network.get_parameters()[1]]\n",
    "    parameter_feature = base_network.get_parameters()[0:1] + ad_net.get_parameters()\n",
    "\n",
    "    ## set optimizer\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer_classfier = optimizer_config[\"type\"](parameter_classifier, \\\n",
    "                                                   **(optimizer_config[\"optim_params\"]))\n",
    "    optimizer_feature = optimizer_config[\"type\"](parameter_feature, \\\n",
    "                                                 **(optimizer_config[\"optim_params\"]))\n",
    "    param_lr = []\n",
    "    for param_group in optimizer_feature.param_groups:\n",
    "        param_lr.append(param_group[\"lr\"])\n",
    "    param_lr.append(optimizer_classfier.param_groups[0][\"lr\"])\n",
    "    schedule_param = optimizer_config[\"lr_param\"]\n",
    "    lr_scheduler = lr_schedule.schedule_dict[optimizer_config[\"lr_type\"]]\n",
    "\n",
    "    ## train\n",
    "    len_train_source = len(dset_loaders[\"source\"])\n",
    "    len_train_target = len(dset_loaders[\"target\"])\n",
    "    best_acc = 0.0\n",
    "    best_model = copy.deepcopy(base_network)\n",
    "\n",
    "    Cs_memory=torch.zeros(class_num,256).cuda()\n",
    "    Ct_memory=torch.zeros(class_num,256).cuda()\n",
    "\n",
    "\n",
    "    for i in range(config[\"iterations\"]):\n",
    "        if i % config[\"test_interval\"] == config[\"test_interval\"] - 1:\n",
    "            base_network.train(False)\n",
    "            temp_acc = image_classification_test(dset_loaders,base_network)\n",
    "            temp_model = base_network\n",
    "            if temp_acc > best_acc:\n",
    "                best_acc = temp_acc\n",
    "                best_model = copy.deepcopy(temp_model)\n",
    "            log_str = \"iter: {:05d}, \\t precision: {:.4f},\\t best_acc:{:.4f}\".format(i, temp_acc, best_acc)\n",
    "            config[\"out_file\"].write(log_str + \"\\n\")\n",
    "            config[\"out_file\"].flush()\n",
    "            print(log_str)\n",
    "        if (i + 1) % config[\"snapshot_interval\"] == 0:\n",
    "            if not os.path.exists(\"save/init_model\"):\n",
    "                os.makedirs(\"save/init_model\")\n",
    "            torch.save(best_model, 'save/init_model/' + source + '_' + target + 'class.pkl')\n",
    "\n",
    "        ## train one iter\n",
    "        base_network.train(True)\n",
    "        ad_net.train(True)\n",
    "        optimizer_classfier = lr_scheduler(optimizer_classfier, i, **schedule_param)\n",
    "        optimizer_feature = lr_scheduler(optimizer_feature, i, **schedule_param)\n",
    "\n",
    "        if i % len_train_source == 0:\n",
    "            iter_source = iter(dset_loaders[\"source\"])\n",
    "        if i % len_train_target == 0:\n",
    "            iter_target = iter(dset_loaders[\"target\"])\n",
    "        inputs_source, labels_source = iter_source.next()\n",
    "        inputs_target, labels_target = iter_target.next()\n",
    "        inputs_source, inputs_target, labels_source = inputs_source.cuda(), inputs_target.cuda(), labels_source.cuda()\n",
    "        features_source, outputs_source = base_network(inputs_source)\n",
    "\n",
    "        classifier_loss = nn.CrossEntropyLoss()(outputs_source, labels_source)\n",
    "\n",
    "        loss_total = classifier_loss\n",
    "\n",
    "        optimizer_classfier.zero_grad()\n",
    "        optimizer_feature.zero_grad()\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer_feature.step()\n",
    "        optimizer_classfier.step()\n",
    "\n",
    "        print('step:{: d},\\t,class_loss:{:.4f},\\t'\n",
    "              ''.format(i, classifier_loss.item()))\n",
    "        Cs_memory.detach_()\n",
    "        Ct_memory.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
