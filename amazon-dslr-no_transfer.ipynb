{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "step: 0,\t,class_loss:3.6366,\t,sm:28.31\n",
      "step: 1,\t,class_loss:3.4498,\t,sm:24.96\n",
      "step: 2,\t,class_loss:3.4750,\t,sm:24.05\n",
      "step: 3,\t,class_loss:3.5438,\t,sm:22.64\n",
      "step: 4,\t,class_loss:3.4941,\t,sm:22.27\n",
      "step: 5,\t,class_loss:3.5317,\t,sm:21.71\n",
      "step: 6,\t,class_loss:3.4979,\t,sm:21.59\n",
      "step: 7,\t,class_loss:3.5102,\t,sm:20.13\n",
      "step: 8,\t,class_loss:3.3166,\t,sm:20.00\n",
      "step: 9,\t,class_loss:3.3018,\t,sm:19.27\n",
      "step: 10,\t,class_loss:3.3440,\t,sm:18.31\n",
      "step: 11,\t,class_loss:3.3863,\t,sm:18.40\n",
      "step: 12,\t,class_loss:3.3420,\t,sm:16.18\n",
      "step: 13,\t,class_loss:3.3388,\t,sm:16.06\n",
      "step: 14,\t,class_loss:3.2664,\t,sm:15.75\n",
      "step: 15,\t,class_loss:3.2584,\t,sm:15.61\n",
      "step: 16,\t,class_loss:3.2211,\t,sm:15.55\n",
      "step: 17,\t,class_loss:3.2312,\t,sm:14.15\n",
      "step: 18,\t,class_loss:3.2178,\t,sm:12.02\n",
      "step: 19,\t,class_loss:3.0690,\t,sm:11.23\n",
      "step: 20,\t,class_loss:3.2723,\t,sm:11.04\n",
      "step: 21,\t,class_loss:3.1677,\t,sm:10.93\n",
      "step: 22,\t,class_loss:3.0478,\t,sm:9.88\n",
      "step: 23,\t,class_loss:3.0264,\t,sm:9.54\n",
      "step: 24,\t,class_loss:2.9835,\t,sm:7.34\n",
      "step: 25,\t,class_loss:3.0292,\t,sm:7.07\n",
      "step: 26,\t,class_loss:3.1731,\t,sm:7.05\n",
      "step: 27,\t,class_loss:2.9113,\t,sm:6.93\n",
      "step: 28,\t,class_loss:2.9054,\t,sm:6.76\n",
      "step: 29,\t,class_loss:3.0471,\t,sm:6.61\n",
      "step: 30,\t,class_loss:2.9785,\t,sm:6.56\n",
      "step: 31,\t,class_loss:3.0125,\t,sm:6.63\n",
      "step: 32,\t,class_loss:2.7512,\t,sm:6.55\n",
      "step: 33,\t,class_loss:2.8289,\t,sm:6.43\n",
      "step: 34,\t,class_loss:2.9258,\t,sm:6.51\n",
      "step: 35,\t,class_loss:2.8035,\t,sm:6.43\n",
      "step: 36,\t,class_loss:2.6884,\t,sm:6.41\n",
      "step: 37,\t,class_loss:2.8265,\t,sm:6.36\n",
      "step: 38,\t,class_loss:2.7018,\t,sm:6.32\n",
      "step: 39,\t,class_loss:2.7029,\t,sm:6.31\n",
      "step: 40,\t,class_loss:2.7231,\t,sm:6.15\n",
      "step: 41,\t,class_loss:2.7304,\t,sm:5.14\n",
      "step: 42,\t,class_loss:2.4318,\t,sm:5.00\n",
      "step: 43,\t,class_loss:2.7920,\t,sm:4.98\n",
      "step: 44,\t,class_loss:2.3449,\t,sm:5.04\n",
      "step: 45,\t,class_loss:2.4766,\t,sm:5.25\n",
      "step: 46,\t,class_loss:2.4226,\t,sm:5.27\n",
      "step: 47,\t,class_loss:2.5221,\t,sm:5.09\n",
      "step: 48,\t,class_loss:2.3293,\t,sm:5.32\n",
      "iter: 00049, \t precision: 0.4197,\t best_acc:0.4197\n",
      "step: 49,\t,class_loss:2.3059,\t,sm:5.24\n",
      "step: 50,\t,class_loss:2.5588,\t,sm:4.03\n",
      "step: 51,\t,class_loss:2.3868,\t,sm:4.24\n",
      "step: 52,\t,class_loss:2.0926,\t,sm:4.28\n",
      "step: 53,\t,class_loss:2.3474,\t,sm:4.19\n",
      "step: 54,\t,class_loss:2.2524,\t,sm:4.33\n",
      "step: 55,\t,class_loss:2.3084,\t,sm:4.00\n",
      "step: 56,\t,class_loss:2.4747,\t,sm:3.90\n",
      "step: 57,\t,class_loss:2.3179,\t,sm:3.87\n",
      "step: 58,\t,class_loss:2.1802,\t,sm:3.84\n",
      "step: 59,\t,class_loss:2.4741,\t,sm:3.93\n",
      "step: 60,\t,class_loss:2.3120,\t,sm:4.00\n",
      "step: 61,\t,class_loss:2.2236,\t,sm:4.17\n",
      "step: 62,\t,class_loss:2.4708,\t,sm:4.30\n",
      "step: 63,\t,class_loss:2.1834,\t,sm:4.28\n",
      "step: 64,\t,class_loss:2.3452,\t,sm:4.26\n",
      "step: 65,\t,class_loss:1.8851,\t,sm:4.31\n",
      "step: 66,\t,class_loss:2.1324,\t,sm:4.38\n",
      "step: 67,\t,class_loss:1.9627,\t,sm:4.43\n",
      "step: 68,\t,class_loss:2.0594,\t,sm:4.24\n",
      "step: 69,\t,class_loss:2.1271,\t,sm:4.12\n",
      "step: 70,\t,class_loss:1.8786,\t,sm:3.95\n",
      "step: 71,\t,class_loss:1.7956,\t,sm:4.14\n",
      "step: 72,\t,class_loss:1.9280,\t,sm:4.06\n",
      "step: 73,\t,class_loss:2.1990,\t,sm:3.93\n",
      "step: 74,\t,class_loss:1.9345,\t,sm:3.94\n",
      "step: 75,\t,class_loss:1.3745,\t,sm:3.90\n",
      "step: 76,\t,class_loss:2.0434,\t,sm:4.08\n",
      "step: 77,\t,class_loss:1.4723,\t,sm:4.00\n",
      "step: 78,\t,class_loss:1.7929,\t,sm:3.88\n",
      "step: 79,\t,class_loss:1.9306,\t,sm:4.03\n",
      "step: 80,\t,class_loss:1.4990,\t,sm:3.92\n",
      "step: 81,\t,class_loss:1.6279,\t,sm:3.73\n",
      "step: 82,\t,class_loss:1.6624,\t,sm:3.72\n",
      "step: 83,\t,class_loss:1.7178,\t,sm:3.82\n",
      "step: 84,\t,class_loss:1.7911,\t,sm:3.71\n",
      "step: 85,\t,class_loss:1.7146,\t,sm:3.70\n",
      "step: 86,\t,class_loss:1.6888,\t,sm:3.46\n",
      "step: 87,\t,class_loss:1.7825,\t,sm:3.51\n",
      "step: 88,\t,class_loss:1.7003,\t,sm:3.48\n",
      "step: 89,\t,class_loss:1.4283,\t,sm:3.35\n",
      "step: 90,\t,class_loss:1.7751,\t,sm:3.17\n",
      "step: 91,\t,class_loss:1.3668,\t,sm:3.53\n",
      "step: 92,\t,class_loss:1.6299,\t,sm:3.72\n",
      "step: 93,\t,class_loss:1.4010,\t,sm:3.82\n",
      "step: 94,\t,class_loss:1.4154,\t,sm:3.83\n",
      "step: 95,\t,class_loss:1.4290,\t,sm:3.61\n",
      "step: 96,\t,class_loss:1.1661,\t,sm:4.01\n",
      "step: 97,\t,class_loss:1.4185,\t,sm:3.93\n",
      "step: 98,\t,class_loss:1.1497,\t,sm:4.09\n",
      "iter: 00099, \t precision: 0.6205,\t best_acc:0.6205\n",
      "step: 99,\t,class_loss:1.3520,\t,sm:4.04\n",
      "step: 100,\t,class_loss:1.6041,\t,sm:3.82\n",
      "step: 101,\t,class_loss:1.3363,\t,sm:3.77\n",
      "step: 102,\t,class_loss:1.1500,\t,sm:3.80\n",
      "step: 103,\t,class_loss:1.0717,\t,sm:3.91\n",
      "step: 104,\t,class_loss:1.4113,\t,sm:3.92\n",
      "step: 105,\t,class_loss:1.3095,\t,sm:3.92\n",
      "step: 106,\t,class_loss:1.1856,\t,sm:4.07\n",
      "step: 107,\t,class_loss:1.2995,\t,sm:3.67\n",
      "step: 108,\t,class_loss:1.0735,\t,sm:3.79\n",
      "step: 109,\t,class_loss:1.5495,\t,sm:3.87\n",
      "step: 110,\t,class_loss:1.5008,\t,sm:4.11\n",
      "step: 111,\t,class_loss:1.4625,\t,sm:4.36\n",
      "step: 112,\t,class_loss:1.2689,\t,sm:4.10\n",
      "step: 113,\t,class_loss:1.4339,\t,sm:4.03\n",
      "step: 114,\t,class_loss:1.0186,\t,sm:3.86\n",
      "step: 115,\t,class_loss:1.5435,\t,sm:3.89\n",
      "step: 116,\t,class_loss:1.1446,\t,sm:3.83\n",
      "step: 117,\t,class_loss:1.0846,\t,sm:4.01\n",
      "step: 118,\t,class_loss:1.0223,\t,sm:4.13\n",
      "step: 119,\t,class_loss:1.2316,\t,sm:4.08\n",
      "step: 120,\t,class_loss:1.0221,\t,sm:3.86\n",
      "step: 121,\t,class_loss:1.3630,\t,sm:4.19\n",
      "step: 122,\t,class_loss:0.9760,\t,sm:4.02\n",
      "step: 123,\t,class_loss:0.8755,\t,sm:3.87\n",
      "step: 124,\t,class_loss:0.9283,\t,sm:4.06\n",
      "step: 125,\t,class_loss:0.9302,\t,sm:3.99\n",
      "step: 126,\t,class_loss:0.8973,\t,sm:4.08\n",
      "step: 127,\t,class_loss:1.0207,\t,sm:4.31\n",
      "step: 128,\t,class_loss:1.2388,\t,sm:4.21\n",
      "step: 129,\t,class_loss:0.9909,\t,sm:3.74\n",
      "step: 130,\t,class_loss:1.0454,\t,sm:3.80\n",
      "step: 131,\t,class_loss:1.4092,\t,sm:3.76\n",
      "step: 132,\t,class_loss:1.0494,\t,sm:3.83\n",
      "step: 133,\t,class_loss:1.1296,\t,sm:3.97\n",
      "step: 134,\t,class_loss:0.7726,\t,sm:4.03\n",
      "step: 135,\t,class_loss:1.1075,\t,sm:3.67\n",
      "step: 136,\t,class_loss:1.3199,\t,sm:3.81\n",
      "step: 137,\t,class_loss:1.1368,\t,sm:3.91\n",
      "step: 138,\t,class_loss:1.3299,\t,sm:3.77\n",
      "step: 139,\t,class_loss:0.8532,\t,sm:3.48\n",
      "step: 140,\t,class_loss:1.3869,\t,sm:3.40\n",
      "step: 141,\t,class_loss:1.0456,\t,sm:3.28\n",
      "step: 142,\t,class_loss:1.0547,\t,sm:3.24\n",
      "step: 143,\t,class_loss:1.1874,\t,sm:3.12\n",
      "step: 144,\t,class_loss:1.0132,\t,sm:3.41\n",
      "step: 145,\t,class_loss:1.0814,\t,sm:3.63\n",
      "step: 146,\t,class_loss:0.7453,\t,sm:3.82\n",
      "step: 147,\t,class_loss:0.9964,\t,sm:3.81\n",
      "step: 148,\t,class_loss:0.9683,\t,sm:3.63\n",
      "iter: 00149, \t precision: 0.6867,\t best_acc:0.6867\n",
      "step: 149,\t,class_loss:1.0140,\t,sm:3.73\n",
      "step: 150,\t,class_loss:0.9964,\t,sm:3.71\n",
      "step: 151,\t,class_loss:0.9867,\t,sm:3.61\n",
      "step: 152,\t,class_loss:1.1081,\t,sm:3.72\n",
      "step: 153,\t,class_loss:1.1325,\t,sm:3.82\n",
      "step: 154,\t,class_loss:1.2684,\t,sm:3.90\n",
      "step: 155,\t,class_loss:0.7992,\t,sm:3.74\n",
      "step: 156,\t,class_loss:0.8262,\t,sm:3.76\n",
      "step: 157,\t,class_loss:0.7704,\t,sm:3.61\n",
      "step: 158,\t,class_loss:0.7778,\t,sm:3.75\n",
      "step: 159,\t,class_loss:0.7587,\t,sm:3.85\n",
      "step: 160,\t,class_loss:1.0303,\t,sm:3.52\n",
      "step: 161,\t,class_loss:0.9794,\t,sm:3.55\n",
      "step: 162,\t,class_loss:1.0302,\t,sm:3.71\n",
      "step: 163,\t,class_loss:0.5924,\t,sm:3.48\n",
      "step: 164,\t,class_loss:0.7265,\t,sm:3.64\n",
      "step: 165,\t,class_loss:0.6005,\t,sm:3.43\n",
      "step: 166,\t,class_loss:0.8340,\t,sm:3.50\n",
      "step: 167,\t,class_loss:1.0504,\t,sm:3.74\n",
      "step: 168,\t,class_loss:0.8840,\t,sm:3.91\n",
      "step: 169,\t,class_loss:0.8802,\t,sm:4.01\n",
      "step: 170,\t,class_loss:1.3686,\t,sm:3.62\n",
      "step: 171,\t,class_loss:1.0303,\t,sm:3.59\n",
      "step: 172,\t,class_loss:0.6810,\t,sm:3.65\n",
      "step: 173,\t,class_loss:0.6644,\t,sm:3.73\n",
      "step: 174,\t,class_loss:0.8497,\t,sm:3.60\n",
      "step: 175,\t,class_loss:0.6276,\t,sm:3.62\n",
      "step: 176,\t,class_loss:0.9486,\t,sm:3.40\n",
      "step: 177,\t,class_loss:0.8501,\t,sm:3.42\n",
      "step: 178,\t,class_loss:0.8452,\t,sm:3.43\n",
      "step: 179,\t,class_loss:0.7791,\t,sm:3.34\n",
      "step: 180,\t,class_loss:0.6253,\t,sm:3.19\n",
      "step: 181,\t,class_loss:0.8300,\t,sm:3.53\n",
      "step: 182,\t,class_loss:0.9614,\t,sm:3.40\n",
      "step: 183,\t,class_loss:0.5791,\t,sm:3.51\n",
      "step: 184,\t,class_loss:0.5930,\t,sm:3.55\n",
      "step: 185,\t,class_loss:0.7671,\t,sm:3.59\n",
      "step: 186,\t,class_loss:0.9109,\t,sm:3.65\n",
      "step: 187,\t,class_loss:0.5808,\t,sm:3.46\n",
      "step: 188,\t,class_loss:0.7791,\t,sm:3.62\n",
      "step: 189,\t,class_loss:0.7190,\t,sm:3.58\n",
      "step: 190,\t,class_loss:0.7746,\t,sm:3.64\n",
      "step: 191,\t,class_loss:0.8902,\t,sm:3.60\n",
      "step: 192,\t,class_loss:0.6356,\t,sm:3.57\n",
      "step: 193,\t,class_loss:0.7497,\t,sm:3.43\n",
      "step: 194,\t,class_loss:0.6666,\t,sm:3.27\n",
      "step: 195,\t,class_loss:0.9603,\t,sm:3.47\n",
      "step: 196,\t,class_loss:0.7113,\t,sm:3.55\n",
      "step: 197,\t,class_loss:0.5279,\t,sm:3.46\n",
      "step: 198,\t,class_loss:0.7165,\t,sm:3.40\n",
      "iter: 00199, \t precision: 0.7149,\t best_acc:0.7149\n",
      "step: 199,\t,class_loss:0.7232,\t,sm:3.39\n",
      "step: 200,\t,class_loss:0.7160,\t,sm:3.51\n",
      "step: 201,\t,class_loss:0.8582,\t,sm:3.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 202,\t,class_loss:0.6635,\t,sm:3.37\n",
      "step: 203,\t,class_loss:0.7089,\t,sm:3.33\n",
      "step: 204,\t,class_loss:0.6845,\t,sm:3.41\n",
      "step: 205,\t,class_loss:0.8719,\t,sm:3.47\n",
      "step: 206,\t,class_loss:0.6840,\t,sm:3.31\n",
      "step: 207,\t,class_loss:0.6982,\t,sm:3.12\n",
      "step: 208,\t,class_loss:0.7324,\t,sm:3.26\n",
      "step: 209,\t,class_loss:0.7331,\t,sm:3.34\n",
      "step: 210,\t,class_loss:0.8649,\t,sm:3.43\n",
      "step: 211,\t,class_loss:0.6322,\t,sm:3.56\n",
      "step: 212,\t,class_loss:0.6750,\t,sm:3.50\n",
      "step: 213,\t,class_loss:0.7921,\t,sm:3.42\n",
      "step: 214,\t,class_loss:0.5645,\t,sm:3.25\n",
      "step: 215,\t,class_loss:0.6604,\t,sm:3.28\n",
      "step: 216,\t,class_loss:0.7153,\t,sm:3.42\n",
      "step: 217,\t,class_loss:0.9975,\t,sm:3.53\n",
      "step: 218,\t,class_loss:0.3986,\t,sm:3.24\n",
      "step: 219,\t,class_loss:0.6506,\t,sm:3.29\n",
      "step: 220,\t,class_loss:0.6948,\t,sm:3.30\n",
      "step: 221,\t,class_loss:0.5547,\t,sm:3.10\n",
      "step: 222,\t,class_loss:0.7448,\t,sm:3.33\n",
      "step: 223,\t,class_loss:0.6639,\t,sm:3.43\n",
      "step: 224,\t,class_loss:0.7611,\t,sm:3.31\n",
      "step: 225,\t,class_loss:0.7236,\t,sm:3.12\n",
      "step: 226,\t,class_loss:0.6510,\t,sm:3.27\n",
      "step: 227,\t,class_loss:1.0228,\t,sm:3.36\n",
      "step: 228,\t,class_loss:0.6567,\t,sm:3.39\n",
      "step: 229,\t,class_loss:0.6681,\t,sm:3.37\n",
      "step: 230,\t,class_loss:0.5690,\t,sm:3.30\n",
      "step: 231,\t,class_loss:0.5759,\t,sm:2.79\n",
      "step: 232,\t,class_loss:0.8326,\t,sm:2.77\n",
      "step: 233,\t,class_loss:0.5998,\t,sm:2.78\n",
      "step: 234,\t,class_loss:0.7311,\t,sm:2.64\n",
      "step: 235,\t,class_loss:0.8537,\t,sm:2.64\n",
      "step: 236,\t,class_loss:0.9345,\t,sm:2.69\n",
      "step: 237,\t,class_loss:0.5497,\t,sm:2.39\n",
      "step: 238,\t,class_loss:0.3157,\t,sm:2.38\n",
      "step: 239,\t,class_loss:0.7778,\t,sm:2.36\n",
      "step: 240,\t,class_loss:0.5452,\t,sm:2.50\n",
      "step: 241,\t,class_loss:0.5244,\t,sm:2.64\n",
      "step: 242,\t,class_loss:0.4472,\t,sm:2.45\n",
      "step: 243,\t,class_loss:0.5691,\t,sm:2.34\n",
      "step: 244,\t,class_loss:0.5454,\t,sm:2.32\n",
      "step: 245,\t,class_loss:0.7458,\t,sm:2.18\n",
      "step: 246,\t,class_loss:0.6733,\t,sm:2.28\n",
      "step: 247,\t,class_loss:0.9273,\t,sm:2.27\n",
      "step: 248,\t,class_loss:0.5830,\t,sm:2.62\n",
      "iter: 00249, \t precision: 0.7550,\t best_acc:0.7550\n",
      "step: 249,\t,class_loss:0.3602,\t,sm:2.36\n",
      "step: 250,\t,class_loss:0.8081,\t,sm:2.28\n",
      "step: 251,\t,class_loss:0.7289,\t,sm:2.39\n",
      "step: 252,\t,class_loss:0.5783,\t,sm:2.41\n",
      "step: 253,\t,class_loss:0.6376,\t,sm:2.37\n",
      "step: 254,\t,class_loss:0.7182,\t,sm:2.50\n",
      "step: 255,\t,class_loss:0.4920,\t,sm:2.52\n",
      "step: 256,\t,class_loss:0.5297,\t,sm:2.30\n",
      "step: 257,\t,class_loss:0.6378,\t,sm:2.24\n",
      "step: 258,\t,class_loss:0.5640,\t,sm:2.13\n",
      "step: 259,\t,class_loss:0.7772,\t,sm:2.18\n",
      "step: 260,\t,class_loss:0.4184,\t,sm:2.30\n",
      "step: 261,\t,class_loss:0.8046,\t,sm:2.17\n",
      "step: 262,\t,class_loss:0.6640,\t,sm:2.35\n",
      "step: 263,\t,class_loss:0.5881,\t,sm:2.25\n",
      "step: 264,\t,class_loss:0.4132,\t,sm:2.28\n",
      "step: 265,\t,class_loss:0.5423,\t,sm:2.25\n",
      "step: 266,\t,class_loss:0.6572,\t,sm:2.31\n",
      "step: 267,\t,class_loss:0.3827,\t,sm:2.34\n",
      "step: 268,\t,class_loss:0.4120,\t,sm:2.41\n",
      "step: 269,\t,class_loss:0.7246,\t,sm:2.33\n",
      "step: 270,\t,class_loss:0.7795,\t,sm:2.50\n",
      "step: 271,\t,class_loss:0.4187,\t,sm:2.50\n",
      "step: 272,\t,class_loss:1.0094,\t,sm:2.64\n",
      "step: 273,\t,class_loss:0.5384,\t,sm:2.81\n",
      "step: 274,\t,class_loss:0.5168,\t,sm:2.82\n",
      "step: 275,\t,class_loss:0.5871,\t,sm:2.62\n",
      "step: 276,\t,class_loss:0.3760,\t,sm:2.44\n",
      "step: 277,\t,class_loss:0.5686,\t,sm:2.42\n",
      "step: 278,\t,class_loss:0.8065,\t,sm:2.30\n",
      "step: 279,\t,class_loss:0.6279,\t,sm:2.52\n",
      "step: 280,\t,class_loss:0.3221,\t,sm:2.47\n",
      "step: 281,\t,class_loss:0.3058,\t,sm:2.45\n",
      "step: 282,\t,class_loss:0.6621,\t,sm:2.25\n",
      "step: 283,\t,class_loss:0.4113,\t,sm:2.39\n",
      "step: 284,\t,class_loss:0.5469,\t,sm:2.25\n",
      "step: 285,\t,class_loss:0.3895,\t,sm:2.25\n",
      "step: 286,\t,class_loss:0.3549,\t,sm:2.15\n",
      "step: 287,\t,class_loss:0.8052,\t,sm:2.18\n",
      "step: 288,\t,class_loss:0.4301,\t,sm:2.27\n",
      "step: 289,\t,class_loss:0.6670,\t,sm:2.30\n",
      "step: 290,\t,class_loss:0.3318,\t,sm:2.27\n",
      "step: 291,\t,class_loss:0.3467,\t,sm:2.16\n",
      "step: 292,\t,class_loss:0.4885,\t,sm:2.33\n",
      "step: 293,\t,class_loss:0.4063,\t,sm:2.22\n",
      "step: 294,\t,class_loss:0.5733,\t,sm:2.24\n",
      "step: 295,\t,class_loss:0.3996,\t,sm:2.11\n",
      "step: 296,\t,class_loss:0.3772,\t,sm:1.92\n",
      "step: 297,\t,class_loss:0.6905,\t,sm:2.36\n",
      "step: 298,\t,class_loss:0.3343,\t,sm:2.15\n",
      "iter: 00299, \t precision: 0.7610,\t best_acc:0.7610\n",
      "step: 299,\t,class_loss:0.5716,\t,sm:2.31\n",
      "step: 300,\t,class_loss:0.7342,\t,sm:2.58\n",
      "step: 301,\t,class_loss:0.5614,\t,sm:2.48\n",
      "step: 302,\t,class_loss:0.3475,\t,sm:2.39\n",
      "step: 303,\t,class_loss:0.4631,\t,sm:2.37\n",
      "step: 304,\t,class_loss:0.4741,\t,sm:2.11\n",
      "step: 305,\t,class_loss:0.3508,\t,sm:2.22\n",
      "step: 306,\t,class_loss:0.4486,\t,sm:2.13\n",
      "step: 307,\t,class_loss:0.5891,\t,sm:2.23\n",
      "step: 308,\t,class_loss:0.7575,\t,sm:2.15\n",
      "step: 309,\t,class_loss:0.5921,\t,sm:2.24\n",
      "step: 310,\t,class_loss:0.5881,\t,sm:2.32\n",
      "step: 311,\t,class_loss:0.4265,\t,sm:2.25\n",
      "step: 312,\t,class_loss:0.3387,\t,sm:2.17\n",
      "step: 313,\t,class_loss:0.4287,\t,sm:2.02\n",
      "step: 314,\t,class_loss:0.5753,\t,sm:2.07\n",
      "step: 315,\t,class_loss:0.5276,\t,sm:2.20\n",
      "step: 316,\t,class_loss:0.5998,\t,sm:2.20\n",
      "step: 317,\t,class_loss:0.4323,\t,sm:2.23\n",
      "step: 318,\t,class_loss:0.4649,\t,sm:2.10\n",
      "step: 319,\t,class_loss:0.6800,\t,sm:2.21\n",
      "step: 320,\t,class_loss:0.4419,\t,sm:2.46\n",
      "step: 321,\t,class_loss:0.3822,\t,sm:2.38\n",
      "step: 322,\t,class_loss:0.4342,\t,sm:2.23\n",
      "step: 323,\t,class_loss:0.3904,\t,sm:2.03\n",
      "step: 324,\t,class_loss:0.3995,\t,sm:2.17\n",
      "step: 325,\t,class_loss:0.3048,\t,sm:2.13\n",
      "step: 326,\t,class_loss:0.3289,\t,sm:2.26\n",
      "step: 327,\t,class_loss:0.4727,\t,sm:2.23\n",
      "step: 328,\t,class_loss:0.3810,\t,sm:1.99\n",
      "step: 329,\t,class_loss:0.5372,\t,sm:1.95\n",
      "step: 330,\t,class_loss:0.3947,\t,sm:2.04\n",
      "step: 331,\t,class_loss:0.5404,\t,sm:2.05\n",
      "step: 332,\t,class_loss:0.5375,\t,sm:2.22\n",
      "step: 333,\t,class_loss:0.3075,\t,sm:2.17\n",
      "step: 334,\t,class_loss:0.3158,\t,sm:2.43\n",
      "step: 335,\t,class_loss:0.4836,\t,sm:2.51\n",
      "step: 336,\t,class_loss:0.4083,\t,sm:2.22\n",
      "step: 337,\t,class_loss:0.4501,\t,sm:2.05\n",
      "step: 338,\t,class_loss:0.4987,\t,sm:1.96\n",
      "step: 339,\t,class_loss:0.5013,\t,sm:1.79\n",
      "step: 340,\t,class_loss:0.2530,\t,sm:1.97\n",
      "step: 341,\t,class_loss:0.3611,\t,sm:2.01\n",
      "step: 342,\t,class_loss:0.4402,\t,sm:2.16\n",
      "step: 343,\t,class_loss:0.4495,\t,sm:2.21\n",
      "step: 344,\t,class_loss:0.3339,\t,sm:2.17\n",
      "step: 345,\t,class_loss:0.4492,\t,sm:2.05\n",
      "step: 346,\t,class_loss:0.7330,\t,sm:1.93\n",
      "step: 347,\t,class_loss:0.4798,\t,sm:1.95\n",
      "step: 348,\t,class_loss:0.2781,\t,sm:2.18\n",
      "iter: 00349, \t precision: 0.7851,\t best_acc:0.7851\n",
      "step: 349,\t,class_loss:0.4524,\t,sm:2.03\n",
      "step: 350,\t,class_loss:0.3997,\t,sm:2.18\n",
      "step: 351,\t,class_loss:0.3423,\t,sm:2.13\n",
      "step: 352,\t,class_loss:0.2714,\t,sm:2.06\n",
      "step: 353,\t,class_loss:0.6406,\t,sm:2.46\n",
      "step: 354,\t,class_loss:0.4934,\t,sm:2.22\n",
      "step: 355,\t,class_loss:0.5476,\t,sm:2.22\n",
      "step: 356,\t,class_loss:0.3266,\t,sm:2.27\n",
      "step: 357,\t,class_loss:0.5149,\t,sm:2.32\n",
      "step: 358,\t,class_loss:0.3344,\t,sm:2.06\n",
      "step: 359,\t,class_loss:0.4203,\t,sm:2.12\n",
      "step: 360,\t,class_loss:0.5420,\t,sm:2.18\n",
      "step: 361,\t,class_loss:0.2783,\t,sm:2.00\n",
      "step: 362,\t,class_loss:0.4328,\t,sm:2.10\n",
      "step: 363,\t,class_loss:0.4583,\t,sm:2.18\n",
      "step: 364,\t,class_loss:0.8051,\t,sm:2.01\n",
      "step: 365,\t,class_loss:0.4012,\t,sm:2.15\n",
      "step: 366,\t,class_loss:0.5766,\t,sm:1.94\n",
      "step: 367,\t,class_loss:0.5628,\t,sm:1.91\n",
      "step: 368,\t,class_loss:0.3343,\t,sm:1.78\n",
      "step: 369,\t,class_loss:0.5571,\t,sm:1.88\n",
      "step: 370,\t,class_loss:0.4702,\t,sm:2.10\n",
      "step: 371,\t,class_loss:0.5462,\t,sm:2.16\n",
      "step: 372,\t,class_loss:0.4808,\t,sm:2.04\n",
      "step: 373,\t,class_loss:0.4046,\t,sm:2.00\n",
      "step: 374,\t,class_loss:0.6812,\t,sm:2.04\n",
      "step: 375,\t,class_loss:0.5635,\t,sm:2.12\n",
      "step: 376,\t,class_loss:0.3415,\t,sm:2.16\n",
      "step: 377,\t,class_loss:0.5173,\t,sm:2.27\n",
      "step: 378,\t,class_loss:0.4562,\t,sm:2.14\n",
      "step: 379,\t,class_loss:0.4450,\t,sm:2.14\n",
      "step: 380,\t,class_loss:0.3678,\t,sm:2.28\n",
      "step: 381,\t,class_loss:0.2633,\t,sm:2.11\n",
      "step: 382,\t,class_loss:0.4537,\t,sm:2.09\n",
      "step: 383,\t,class_loss:0.2792,\t,sm:2.05\n",
      "step: 384,\t,class_loss:0.3513,\t,sm:1.89\n",
      "step: 385,\t,class_loss:0.3594,\t,sm:2.05\n",
      "step: 386,\t,class_loss:0.4735,\t,sm:1.96\n",
      "step: 387,\t,class_loss:0.3291,\t,sm:2.02\n",
      "step: 388,\t,class_loss:0.3368,\t,sm:1.97\n",
      "step: 389,\t,class_loss:0.3864,\t,sm:1.80\n",
      "step: 390,\t,class_loss:0.4117,\t,sm:1.84\n",
      "step: 391,\t,class_loss:0.4943,\t,sm:1.97\n",
      "step: 392,\t,class_loss:0.3047,\t,sm:2.27\n",
      "step: 393,\t,class_loss:0.3140,\t,sm:2.07\n",
      "step: 394,\t,class_loss:0.2154,\t,sm:2.01\n",
      "step: 395,\t,class_loss:0.5585,\t,sm:2.36\n",
      "step: 396,\t,class_loss:0.4125,\t,sm:2.01\n",
      "step: 397,\t,class_loss:0.3656,\t,sm:1.68\n",
      "step: 398,\t,class_loss:0.3848,\t,sm:1.86\n",
      "iter: 00399, \t precision: 0.7892,\t best_acc:0.7892\n",
      "step: 399,\t,class_loss:0.3834,\t,sm:2.07\n",
      "step: 400,\t,class_loss:0.2026,\t,sm:1.80\n",
      "step: 401,\t,class_loss:0.3895,\t,sm:1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 402,\t,class_loss:0.1779,\t,sm:1.77\n",
      "step: 403,\t,class_loss:0.3629,\t,sm:1.89\n",
      "step: 404,\t,class_loss:0.2214,\t,sm:1.82\n",
      "step: 405,\t,class_loss:0.4453,\t,sm:1.81\n",
      "step: 406,\t,class_loss:0.4101,\t,sm:1.82\n",
      "step: 407,\t,class_loss:0.5123,\t,sm:1.87\n",
      "step: 408,\t,class_loss:0.5550,\t,sm:1.78\n",
      "step: 409,\t,class_loss:0.4149,\t,sm:1.78\n",
      "step: 410,\t,class_loss:0.3155,\t,sm:1.78\n",
      "step: 411,\t,class_loss:0.5182,\t,sm:1.81\n",
      "step: 412,\t,class_loss:0.4257,\t,sm:1.83\n",
      "step: 413,\t,class_loss:0.4114,\t,sm:1.86\n",
      "step: 414,\t,class_loss:0.2855,\t,sm:1.76\n",
      "step: 415,\t,class_loss:0.3698,\t,sm:1.84\n",
      "step: 416,\t,class_loss:0.2568,\t,sm:1.78\n",
      "step: 417,\t,class_loss:0.3931,\t,sm:1.67\n",
      "step: 418,\t,class_loss:0.2977,\t,sm:1.63\n",
      "step: 419,\t,class_loss:0.3843,\t,sm:1.55\n",
      "step: 420,\t,class_loss:0.2572,\t,sm:1.69\n",
      "step: 421,\t,class_loss:0.2722,\t,sm:1.70\n",
      "step: 422,\t,class_loss:0.4971,\t,sm:1.97\n",
      "step: 423,\t,class_loss:0.3471,\t,sm:1.79\n",
      "step: 424,\t,class_loss:0.2954,\t,sm:1.68\n",
      "step: 425,\t,class_loss:0.3116,\t,sm:1.74\n",
      "step: 426,\t,class_loss:0.3256,\t,sm:1.71\n",
      "step: 427,\t,class_loss:0.2841,\t,sm:1.63\n",
      "step: 428,\t,class_loss:0.3680,\t,sm:1.90\n",
      "step: 429,\t,class_loss:0.3871,\t,sm:2.23\n",
      "step: 430,\t,class_loss:0.3002,\t,sm:2.21\n",
      "step: 431,\t,class_loss:0.2953,\t,sm:2.19\n",
      "step: 432,\t,class_loss:0.3473,\t,sm:2.23\n",
      "step: 433,\t,class_loss:0.3592,\t,sm:2.03\n",
      "step: 434,\t,class_loss:0.6404,\t,sm:1.95\n",
      "step: 435,\t,class_loss:0.4127,\t,sm:2.03\n",
      "step: 436,\t,class_loss:0.4289,\t,sm:2.04\n",
      "step: 437,\t,class_loss:0.5002,\t,sm:2.00\n",
      "step: 438,\t,class_loss:0.3136,\t,sm:1.68\n",
      "step: 439,\t,class_loss:0.4787,\t,sm:1.68\n",
      "step: 440,\t,class_loss:0.2762,\t,sm:1.80\n",
      "step: 441,\t,class_loss:0.4104,\t,sm:1.74\n",
      "step: 442,\t,class_loss:0.1710,\t,sm:1.72\n",
      "step: 443,\t,class_loss:0.3443,\t,sm:1.59\n",
      "step: 444,\t,class_loss:0.5061,\t,sm:1.58\n",
      "step: 445,\t,class_loss:0.3439,\t,sm:1.47\n",
      "step: 446,\t,class_loss:0.3720,\t,sm:1.60\n",
      "step: 447,\t,class_loss:0.2007,\t,sm:1.72\n",
      "step: 448,\t,class_loss:0.3888,\t,sm:1.67\n",
      "iter: 00449, \t precision: 0.8012,\t best_acc:0.8012\n",
      "step: 449,\t,class_loss:0.2189,\t,sm:1.86\n",
      "step: 450,\t,class_loss:0.1964,\t,sm:1.79\n",
      "step: 451,\t,class_loss:0.2648,\t,sm:1.83\n",
      "step: 452,\t,class_loss:0.2631,\t,sm:1.80\n",
      "step: 453,\t,class_loss:0.4403,\t,sm:1.89\n",
      "step: 454,\t,class_loss:0.3960,\t,sm:1.86\n",
      "step: 455,\t,class_loss:0.4227,\t,sm:1.87\n",
      "step: 456,\t,class_loss:0.5000,\t,sm:1.88\n",
      "step: 457,\t,class_loss:0.2578,\t,sm:1.67\n",
      "step: 458,\t,class_loss:0.4675,\t,sm:1.69\n",
      "step: 459,\t,class_loss:0.5947,\t,sm:1.73\n",
      "step: 460,\t,class_loss:0.3422,\t,sm:1.85\n",
      "step: 461,\t,class_loss:0.4446,\t,sm:2.06\n",
      "step: 462,\t,class_loss:0.3031,\t,sm:1.99\n",
      "step: 463,\t,class_loss:0.4567,\t,sm:2.00\n",
      "step: 464,\t,class_loss:0.4196,\t,sm:1.76\n",
      "step: 465,\t,class_loss:0.2577,\t,sm:1.70\n",
      "step: 466,\t,class_loss:0.1841,\t,sm:1.80\n",
      "step: 467,\t,class_loss:0.3458,\t,sm:1.94\n",
      "step: 468,\t,class_loss:0.2040,\t,sm:1.78\n",
      "step: 469,\t,class_loss:0.3536,\t,sm:1.78\n",
      "step: 470,\t,class_loss:0.3218,\t,sm:1.88\n",
      "step: 471,\t,class_loss:0.1441,\t,sm:1.72\n",
      "step: 472,\t,class_loss:0.1893,\t,sm:1.84\n",
      "step: 473,\t,class_loss:0.2993,\t,sm:1.75\n",
      "step: 474,\t,class_loss:0.3484,\t,sm:1.83\n",
      "step: 475,\t,class_loss:0.3738,\t,sm:1.91\n",
      "step: 476,\t,class_loss:0.1980,\t,sm:1.99\n",
      "step: 477,\t,class_loss:0.3705,\t,sm:1.85\n",
      "step: 478,\t,class_loss:0.1463,\t,sm:1.82\n",
      "step: 479,\t,class_loss:0.3382,\t,sm:1.84\n",
      "step: 480,\t,class_loss:0.3582,\t,sm:1.92\n",
      "step: 481,\t,class_loss:0.2142,\t,sm:2.09\n",
      "step: 482,\t,class_loss:0.2289,\t,sm:1.91\n",
      "step: 483,\t,class_loss:0.2845,\t,sm:2.00\n",
      "step: 484,\t,class_loss:0.5582,\t,sm:2.23\n",
      "step: 485,\t,class_loss:0.3922,\t,sm:1.87\n",
      "step: 486,\t,class_loss:0.2587,\t,sm:1.85\n",
      "step: 487,\t,class_loss:0.3042,\t,sm:1.63\n",
      "step: 488,\t,class_loss:0.2348,\t,sm:1.61\n",
      "step: 489,\t,class_loss:0.2511,\t,sm:1.70\n",
      "step: 490,\t,class_loss:0.5834,\t,sm:1.80\n",
      "step: 491,\t,class_loss:0.3809,\t,sm:1.74\n",
      "step: 492,\t,class_loss:0.2587,\t,sm:1.70\n",
      "step: 493,\t,class_loss:0.1963,\t,sm:1.70\n",
      "step: 494,\t,class_loss:0.2726,\t,sm:1.79\n",
      "step: 495,\t,class_loss:0.2960,\t,sm:1.92\n",
      "step: 496,\t,class_loss:0.6576,\t,sm:2.00\n",
      "step: 497,\t,class_loss:0.2519,\t,sm:1.87\n",
      "step: 498,\t,class_loss:0.1001,\t,sm:1.77\n",
      "iter: 00499, \t precision: 0.8032,\t best_acc:0.8032\n",
      "step: 499,\t,class_loss:0.2363,\t,sm:1.68\n",
      "step: 500,\t,class_loss:0.2767,\t,sm:1.51\n",
      "step: 501,\t,class_loss:0.2972,\t,sm:1.52\n",
      "step: 502,\t,class_loss:0.2801,\t,sm:1.73\n",
      "step: 503,\t,class_loss:0.2273,\t,sm:1.65\n",
      "step: 504,\t,class_loss:0.2764,\t,sm:1.67\n",
      "step: 505,\t,class_loss:0.2939,\t,sm:1.58\n",
      "step: 506,\t,class_loss:0.3422,\t,sm:1.69\n",
      "step: 507,\t,class_loss:0.3282,\t,sm:1.84\n",
      "step: 508,\t,class_loss:0.2843,\t,sm:1.81\n",
      "step: 509,\t,class_loss:0.2463,\t,sm:1.70\n",
      "step: 510,\t,class_loss:0.4649,\t,sm:1.78\n",
      "step: 511,\t,class_loss:0.4173,\t,sm:1.93\n",
      "step: 512,\t,class_loss:0.1534,\t,sm:1.72\n",
      "step: 513,\t,class_loss:0.3387,\t,sm:1.72\n",
      "step: 514,\t,class_loss:0.2410,\t,sm:1.80\n",
      "step: 515,\t,class_loss:0.1326,\t,sm:1.95\n",
      "step: 516,\t,class_loss:0.3652,\t,sm:1.73\n",
      "step: 517,\t,class_loss:0.3299,\t,sm:1.72\n",
      "step: 518,\t,class_loss:0.4476,\t,sm:1.58\n",
      "step: 519,\t,class_loss:0.2775,\t,sm:1.48\n",
      "step: 520,\t,class_loss:0.1742,\t,sm:1.47\n",
      "step: 521,\t,class_loss:0.2087,\t,sm:1.59\n",
      "step: 522,\t,class_loss:0.3777,\t,sm:1.70\n",
      "step: 523,\t,class_loss:0.2426,\t,sm:1.46\n",
      "step: 524,\t,class_loss:0.3144,\t,sm:1.43\n",
      "step: 525,\t,class_loss:0.1481,\t,sm:1.59\n",
      "step: 526,\t,class_loss:0.2381,\t,sm:1.72\n",
      "step: 527,\t,class_loss:0.5406,\t,sm:1.57\n",
      "step: 528,\t,class_loss:0.3484,\t,sm:1.77\n",
      "step: 529,\t,class_loss:0.3039,\t,sm:1.69\n",
      "step: 530,\t,class_loss:0.2749,\t,sm:1.58\n",
      "step: 531,\t,class_loss:0.3374,\t,sm:1.46\n",
      "step: 532,\t,class_loss:0.3345,\t,sm:1.44\n",
      "step: 533,\t,class_loss:0.0755,\t,sm:1.42\n",
      "step: 534,\t,class_loss:0.1617,\t,sm:1.59\n",
      "step: 535,\t,class_loss:0.3701,\t,sm:1.44\n",
      "step: 536,\t,class_loss:0.3744,\t,sm:1.70\n",
      "step: 537,\t,class_loss:0.3253,\t,sm:1.66\n",
      "step: 538,\t,class_loss:0.3603,\t,sm:1.79\n",
      "step: 539,\t,class_loss:0.3149,\t,sm:1.60\n",
      "step: 540,\t,class_loss:0.2844,\t,sm:1.64\n",
      "step: 541,\t,class_loss:0.5113,\t,sm:1.63\n",
      "step: 542,\t,class_loss:0.2646,\t,sm:1.47\n",
      "step: 543,\t,class_loss:0.2673,\t,sm:1.49\n",
      "step: 544,\t,class_loss:0.3100,\t,sm:1.57\n",
      "step: 545,\t,class_loss:0.3242,\t,sm:1.54\n",
      "step: 546,\t,class_loss:0.3017,\t,sm:1.56\n",
      "step: 547,\t,class_loss:0.0767,\t,sm:1.69\n",
      "step: 548,\t,class_loss:0.1006,\t,sm:1.64\n",
      "iter: 00549, \t precision: 0.8092,\t best_acc:0.8092\n",
      "step: 549,\t,class_loss:0.2880,\t,sm:1.81\n",
      "step: 550,\t,class_loss:0.2676,\t,sm:1.61\n",
      "step: 551,\t,class_loss:0.3436,\t,sm:1.68\n",
      "step: 552,\t,class_loss:0.2211,\t,sm:1.75\n",
      "step: 553,\t,class_loss:0.2372,\t,sm:1.63\n",
      "step: 554,\t,class_loss:0.2200,\t,sm:1.53\n",
      "step: 555,\t,class_loss:0.2769,\t,sm:1.60\n",
      "step: 556,\t,class_loss:0.2484,\t,sm:1.59\n",
      "step: 557,\t,class_loss:0.4302,\t,sm:1.57\n",
      "step: 558,\t,class_loss:0.1882,\t,sm:1.40\n",
      "step: 559,\t,class_loss:0.2122,\t,sm:1.39\n",
      "step: 560,\t,class_loss:0.1805,\t,sm:1.46\n",
      "step: 561,\t,class_loss:0.2126,\t,sm:1.51\n",
      "step: 562,\t,class_loss:0.3057,\t,sm:1.54\n",
      "step: 563,\t,class_loss:0.2514,\t,sm:1.36\n",
      "step: 564,\t,class_loss:0.1726,\t,sm:1.30\n",
      "step: 565,\t,class_loss:0.2547,\t,sm:1.60\n",
      "step: 566,\t,class_loss:0.4112,\t,sm:1.57\n",
      "step: 567,\t,class_loss:0.2369,\t,sm:1.46\n",
      "step: 568,\t,class_loss:0.4411,\t,sm:1.51\n",
      "step: 569,\t,class_loss:0.2452,\t,sm:1.53\n",
      "step: 570,\t,class_loss:0.3096,\t,sm:1.54\n",
      "step: 571,\t,class_loss:0.1928,\t,sm:1.52\n",
      "step: 572,\t,class_loss:0.2936,\t,sm:1.47\n",
      "step: 573,\t,class_loss:0.2679,\t,sm:1.47\n",
      "step: 574,\t,class_loss:0.2261,\t,sm:1.43\n",
      "step: 575,\t,class_loss:0.2873,\t,sm:1.33\n",
      "step: 576,\t,class_loss:0.3060,\t,sm:1.33\n",
      "step: 577,\t,class_loss:0.1711,\t,sm:1.33\n",
      "step: 578,\t,class_loss:0.2290,\t,sm:1.44\n",
      "step: 579,\t,class_loss:0.2135,\t,sm:1.41\n",
      "step: 580,\t,class_loss:0.3562,\t,sm:1.60\n",
      "step: 581,\t,class_loss:0.1922,\t,sm:1.37\n",
      "step: 582,\t,class_loss:0.1430,\t,sm:1.45\n",
      "step: 583,\t,class_loss:0.3006,\t,sm:1.56\n",
      "step: 584,\t,class_loss:0.3340,\t,sm:1.79\n",
      "step: 585,\t,class_loss:0.4280,\t,sm:1.70\n",
      "step: 586,\t,class_loss:0.1796,\t,sm:1.66\n",
      "step: 587,\t,class_loss:0.1633,\t,sm:1.64\n",
      "step: 588,\t,class_loss:0.4166,\t,sm:1.52\n",
      "step: 589,\t,class_loss:0.1576,\t,sm:1.53\n",
      "step: 590,\t,class_loss:0.1149,\t,sm:1.44\n",
      "step: 591,\t,class_loss:0.2872,\t,sm:1.38\n",
      "step: 592,\t,class_loss:0.3125,\t,sm:1.37\n",
      "step: 593,\t,class_loss:0.1702,\t,sm:1.63\n",
      "step: 594,\t,class_loss:0.2895,\t,sm:1.73\n",
      "step: 595,\t,class_loss:0.3146,\t,sm:1.54\n",
      "step: 596,\t,class_loss:0.1503,\t,sm:1.35\n",
      "step: 597,\t,class_loss:0.3105,\t,sm:1.37\n",
      "step: 598,\t,class_loss:0.1927,\t,sm:1.41\n",
      "iter: 00599, \t precision: 0.8233,\t best_acc:0.8233\n",
      "step: 599,\t,class_loss:0.2944,\t,sm:1.44\n",
      "step: 600,\t,class_loss:0.3639,\t,sm:1.35\n",
      "step: 601,\t,class_loss:0.2458,\t,sm:1.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 602,\t,class_loss:0.1842,\t,sm:1.55\n",
      "step: 603,\t,class_loss:0.1623,\t,sm:1.52\n",
      "step: 604,\t,class_loss:0.3118,\t,sm:1.51\n",
      "step: 605,\t,class_loss:0.3225,\t,sm:1.49\n",
      "step: 606,\t,class_loss:0.1655,\t,sm:1.45\n",
      "step: 607,\t,class_loss:0.0914,\t,sm:1.34\n",
      "step: 608,\t,class_loss:0.2866,\t,sm:1.27\n",
      "step: 609,\t,class_loss:0.2098,\t,sm:1.27\n",
      "step: 610,\t,class_loss:0.3468,\t,sm:1.28\n",
      "step: 611,\t,class_loss:0.4014,\t,sm:1.31\n",
      "step: 612,\t,class_loss:0.2711,\t,sm:1.45\n",
      "step: 613,\t,class_loss:0.2027,\t,sm:1.33\n",
      "step: 614,\t,class_loss:0.2249,\t,sm:1.46\n",
      "step: 615,\t,class_loss:0.2484,\t,sm:1.26\n",
      "step: 616,\t,class_loss:0.1270,\t,sm:1.27\n",
      "step: 617,\t,class_loss:0.1303,\t,sm:1.27\n",
      "step: 618,\t,class_loss:0.1691,\t,sm:1.27\n",
      "step: 619,\t,class_loss:0.1632,\t,sm:1.41\n",
      "step: 620,\t,class_loss:0.2426,\t,sm:1.35\n",
      "step: 621,\t,class_loss:0.1231,\t,sm:1.40\n",
      "step: 622,\t,class_loss:0.4098,\t,sm:1.62\n",
      "step: 623,\t,class_loss:0.2161,\t,sm:1.33\n",
      "step: 624,\t,class_loss:0.3165,\t,sm:1.38\n",
      "step: 625,\t,class_loss:0.3000,\t,sm:1.40\n",
      "step: 626,\t,class_loss:0.1693,\t,sm:1.31\n",
      "step: 627,\t,class_loss:0.2472,\t,sm:1.28\n",
      "step: 628,\t,class_loss:0.2154,\t,sm:1.34\n",
      "step: 629,\t,class_loss:0.2100,\t,sm:1.28\n",
      "step: 630,\t,class_loss:0.1587,\t,sm:1.30\n",
      "step: 631,\t,class_loss:0.2348,\t,sm:1.38\n",
      "step: 632,\t,class_loss:0.3367,\t,sm:1.38\n",
      "step: 633,\t,class_loss:0.1403,\t,sm:1.45\n",
      "step: 634,\t,class_loss:0.1121,\t,sm:1.26\n",
      "step: 635,\t,class_loss:0.1275,\t,sm:1.36\n",
      "step: 636,\t,class_loss:0.0726,\t,sm:1.37\n",
      "step: 637,\t,class_loss:0.2300,\t,sm:1.39\n",
      "step: 638,\t,class_loss:0.1104,\t,sm:1.28\n",
      "step: 639,\t,class_loss:0.2344,\t,sm:1.33\n",
      "step: 640,\t,class_loss:0.1902,\t,sm:1.24\n",
      "step: 641,\t,class_loss:0.2315,\t,sm:1.21\n",
      "step: 642,\t,class_loss:0.1981,\t,sm:1.35\n",
      "step: 643,\t,class_loss:0.1899,\t,sm:1.38\n",
      "step: 644,\t,class_loss:0.2714,\t,sm:1.36\n",
      "step: 645,\t,class_loss:0.2675,\t,sm:1.40\n",
      "step: 646,\t,class_loss:0.1200,\t,sm:1.40\n",
      "step: 647,\t,class_loss:0.2262,\t,sm:1.36\n",
      "step: 648,\t,class_loss:0.1544,\t,sm:1.34\n",
      "iter: 00649, \t precision: 0.8213,\t best_acc:0.8233\n",
      "step: 649,\t,class_loss:0.2230,\t,sm:1.41\n",
      "step: 650,\t,class_loss:0.0788,\t,sm:1.31\n",
      "step: 651,\t,class_loss:0.0769,\t,sm:1.36\n",
      "step: 652,\t,class_loss:0.1618,\t,sm:1.58\n",
      "step: 653,\t,class_loss:0.1222,\t,sm:1.37\n",
      "step: 654,\t,class_loss:0.1266,\t,sm:1.23\n",
      "step: 655,\t,class_loss:0.1130,\t,sm:1.37\n",
      "step: 656,\t,class_loss:0.2598,\t,sm:1.34\n",
      "step: 657,\t,class_loss:0.2257,\t,sm:1.30\n",
      "step: 658,\t,class_loss:0.1782,\t,sm:1.40\n",
      "step: 659,\t,class_loss:0.2738,\t,sm:1.48\n",
      "step: 660,\t,class_loss:0.2465,\t,sm:1.44\n",
      "step: 661,\t,class_loss:0.2863,\t,sm:1.32\n",
      "step: 662,\t,class_loss:0.2138,\t,sm:1.43\n",
      "step: 663,\t,class_loss:0.1687,\t,sm:1.42\n",
      "step: 664,\t,class_loss:0.2922,\t,sm:1.49\n",
      "step: 665,\t,class_loss:0.1579,\t,sm:1.49\n",
      "step: 666,\t,class_loss:0.3439,\t,sm:1.50\n",
      "step: 667,\t,class_loss:0.2961,\t,sm:1.59\n",
      "step: 668,\t,class_loss:0.3032,\t,sm:1.49\n",
      "step: 669,\t,class_loss:0.1427,\t,sm:1.48\n",
      "step: 670,\t,class_loss:0.2568,\t,sm:1.44\n",
      "step: 671,\t,class_loss:0.4136,\t,sm:1.22\n",
      "step: 672,\t,class_loss:0.2474,\t,sm:1.35\n",
      "step: 673,\t,class_loss:0.1265,\t,sm:1.34\n",
      "step: 674,\t,class_loss:0.1607,\t,sm:1.23\n",
      "step: 675,\t,class_loss:0.2334,\t,sm:1.36\n",
      "step: 676,\t,class_loss:0.1944,\t,sm:1.39\n",
      "step: 677,\t,class_loss:0.1814,\t,sm:1.27\n",
      "step: 678,\t,class_loss:0.3547,\t,sm:1.27\n",
      "step: 679,\t,class_loss:0.2649,\t,sm:1.31\n",
      "step: 680,\t,class_loss:0.2294,\t,sm:1.30\n",
      "step: 681,\t,class_loss:0.1433,\t,sm:1.26\n",
      "step: 682,\t,class_loss:0.2058,\t,sm:1.18\n",
      "step: 683,\t,class_loss:0.3134,\t,sm:1.28\n",
      "step: 684,\t,class_loss:0.2875,\t,sm:1.40\n",
      "step: 685,\t,class_loss:0.1933,\t,sm:1.43\n",
      "step: 686,\t,class_loss:0.2829,\t,sm:1.32\n",
      "step: 687,\t,class_loss:0.1363,\t,sm:1.37\n",
      "step: 688,\t,class_loss:0.1561,\t,sm:1.30\n",
      "step: 689,\t,class_loss:0.1490,\t,sm:1.36\n",
      "step: 690,\t,class_loss:0.1627,\t,sm:1.39\n",
      "step: 691,\t,class_loss:0.3412,\t,sm:1.22\n",
      "step: 692,\t,class_loss:0.0527,\t,sm:1.21\n",
      "step: 693,\t,class_loss:0.1437,\t,sm:1.17\n",
      "step: 694,\t,class_loss:0.2380,\t,sm:1.25\n",
      "step: 695,\t,class_loss:0.1941,\t,sm:1.25\n",
      "step: 696,\t,class_loss:0.1315,\t,sm:1.18\n",
      "step: 697,\t,class_loss:0.5387,\t,sm:1.22\n",
      "step: 698,\t,class_loss:0.1949,\t,sm:1.40\n",
      "iter: 00699, \t precision: 0.8414,\t best_acc:0.8414\n",
      "step: 699,\t,class_loss:0.1962,\t,sm:1.21\n",
      "step: 700,\t,class_loss:0.1067,\t,sm:1.29\n",
      "step: 701,\t,class_loss:0.1362,\t,sm:1.27\n",
      "step: 702,\t,class_loss:0.1184,\t,sm:1.36\n",
      "step: 703,\t,class_loss:0.1948,\t,sm:1.36\n",
      "step: 704,\t,class_loss:0.1109,\t,sm:1.24\n",
      "step: 705,\t,class_loss:0.1572,\t,sm:1.19\n",
      "step: 706,\t,class_loss:0.1267,\t,sm:1.18\n",
      "step: 707,\t,class_loss:0.1256,\t,sm:1.26\n",
      "step: 708,\t,class_loss:0.1774,\t,sm:1.24\n",
      "step: 709,\t,class_loss:0.1246,\t,sm:1.25\n",
      "step: 710,\t,class_loss:0.2386,\t,sm:1.38\n",
      "step: 711,\t,class_loss:0.1716,\t,sm:1.49\n",
      "step: 712,\t,class_loss:0.1290,\t,sm:1.26\n",
      "step: 713,\t,class_loss:0.1327,\t,sm:1.14\n",
      "step: 714,\t,class_loss:0.0901,\t,sm:1.36\n",
      "step: 715,\t,class_loss:0.1876,\t,sm:1.36\n",
      "step: 716,\t,class_loss:0.1472,\t,sm:1.35\n",
      "step: 717,\t,class_loss:0.2201,\t,sm:1.42\n",
      "step: 718,\t,class_loss:0.1463,\t,sm:1.35\n",
      "step: 719,\t,class_loss:0.3295,\t,sm:1.39\n",
      "step: 720,\t,class_loss:0.1761,\t,sm:1.37\n",
      "step: 721,\t,class_loss:0.1883,\t,sm:1.29\n",
      "step: 722,\t,class_loss:0.0999,\t,sm:1.12\n",
      "step: 723,\t,class_loss:0.0920,\t,sm:1.07\n",
      "step: 724,\t,class_loss:0.1783,\t,sm:1.10\n",
      "step: 725,\t,class_loss:0.1249,\t,sm:1.16\n",
      "step: 726,\t,class_loss:0.2073,\t,sm:1.20\n",
      "step: 727,\t,class_loss:0.1943,\t,sm:1.24\n",
      "step: 728,\t,class_loss:0.3034,\t,sm:1.17\n",
      "step: 729,\t,class_loss:0.1451,\t,sm:1.18\n",
      "step: 730,\t,class_loss:0.1428,\t,sm:1.14\n",
      "step: 731,\t,class_loss:0.3028,\t,sm:1.32\n",
      "step: 732,\t,class_loss:0.1490,\t,sm:1.22\n",
      "step: 733,\t,class_loss:0.2077,\t,sm:1.36\n",
      "step: 734,\t,class_loss:0.1029,\t,sm:1.28\n",
      "step: 735,\t,class_loss:0.2454,\t,sm:1.25\n",
      "step: 736,\t,class_loss:0.1979,\t,sm:1.18\n",
      "step: 737,\t,class_loss:0.1954,\t,sm:1.06\n",
      "step: 738,\t,class_loss:0.1589,\t,sm:1.12\n",
      "step: 739,\t,class_loss:0.1336,\t,sm:1.09\n",
      "step: 740,\t,class_loss:0.2376,\t,sm:1.21\n",
      "step: 741,\t,class_loss:0.1247,\t,sm:1.12\n",
      "step: 742,\t,class_loss:0.1482,\t,sm:1.30\n",
      "step: 743,\t,class_loss:0.1471,\t,sm:1.23\n",
      "step: 744,\t,class_loss:0.1166,\t,sm:1.27\n",
      "step: 745,\t,class_loss:0.0971,\t,sm:1.18\n",
      "step: 746,\t,class_loss:0.1169,\t,sm:1.04\n",
      "step: 747,\t,class_loss:0.1680,\t,sm:1.11\n",
      "step: 748,\t,class_loss:0.0937,\t,sm:1.21\n",
      "iter: 00749, \t precision: 0.8434,\t best_acc:0.8434\n",
      "step: 749,\t,class_loss:0.1560,\t,sm:1.12\n",
      "step: 750,\t,class_loss:0.0869,\t,sm:1.21\n",
      "step: 751,\t,class_loss:0.1803,\t,sm:1.11\n",
      "step: 752,\t,class_loss:0.2790,\t,sm:1.04\n",
      "step: 753,\t,class_loss:0.1655,\t,sm:1.10\n",
      "step: 754,\t,class_loss:0.1760,\t,sm:1.08\n",
      "step: 755,\t,class_loss:0.1654,\t,sm:1.08\n",
      "step: 756,\t,class_loss:0.4218,\t,sm:1.13\n",
      "step: 757,\t,class_loss:0.2953,\t,sm:1.09\n",
      "step: 758,\t,class_loss:0.2946,\t,sm:1.04\n",
      "step: 759,\t,class_loss:0.1855,\t,sm:1.10\n",
      "step: 760,\t,class_loss:0.0891,\t,sm:1.19\n",
      "step: 761,\t,class_loss:0.1761,\t,sm:1.27\n",
      "step: 762,\t,class_loss:0.1470,\t,sm:1.27\n",
      "step: 763,\t,class_loss:0.1106,\t,sm:1.21\n",
      "step: 764,\t,class_loss:0.1821,\t,sm:1.15\n",
      "step: 765,\t,class_loss:0.2160,\t,sm:1.13\n",
      "step: 766,\t,class_loss:0.1198,\t,sm:1.16\n",
      "step: 767,\t,class_loss:0.1160,\t,sm:1.16\n",
      "step: 768,\t,class_loss:0.3234,\t,sm:1.20\n",
      "step: 769,\t,class_loss:0.3188,\t,sm:1.22\n",
      "step: 770,\t,class_loss:0.2024,\t,sm:1.41\n",
      "step: 771,\t,class_loss:0.1992,\t,sm:1.32\n",
      "step: 772,\t,class_loss:0.1207,\t,sm:1.32\n",
      "step: 773,\t,class_loss:0.2670,\t,sm:1.21\n",
      "step: 774,\t,class_loss:0.1568,\t,sm:1.18\n",
      "step: 775,\t,class_loss:0.1007,\t,sm:1.16\n",
      "step: 776,\t,class_loss:0.2461,\t,sm:1.14\n",
      "step: 777,\t,class_loss:0.0920,\t,sm:1.09\n",
      "step: 778,\t,class_loss:0.2564,\t,sm:1.05\n",
      "step: 779,\t,class_loss:0.2370,\t,sm:1.10\n",
      "step: 780,\t,class_loss:0.1024,\t,sm:1.10\n",
      "step: 781,\t,class_loss:0.0560,\t,sm:1.10\n",
      "step: 782,\t,class_loss:0.0839,\t,sm:1.11\n",
      "step: 783,\t,class_loss:0.2379,\t,sm:1.09\n",
      "step: 784,\t,class_loss:0.0476,\t,sm:1.08\n",
      "step: 785,\t,class_loss:0.1595,\t,sm:1.01\n",
      "step: 786,\t,class_loss:0.1097,\t,sm:1.03\n",
      "step: 787,\t,class_loss:0.1510,\t,sm:1.10\n",
      "step: 788,\t,class_loss:0.1354,\t,sm:1.20\n",
      "step: 789,\t,class_loss:0.1851,\t,sm:1.31\n",
      "step: 790,\t,class_loss:0.0522,\t,sm:1.24\n",
      "step: 791,\t,class_loss:0.0955,\t,sm:1.18\n",
      "step: 792,\t,class_loss:0.1048,\t,sm:1.10\n",
      "step: 793,\t,class_loss:0.1642,\t,sm:1.16\n",
      "step: 794,\t,class_loss:0.1330,\t,sm:1.16\n",
      "step: 795,\t,class_loss:0.1348,\t,sm:1.16\n",
      "step: 796,\t,class_loss:0.2135,\t,sm:1.16\n",
      "step: 797,\t,class_loss:0.0863,\t,sm:1.21\n",
      "step: 798,\t,class_loss:0.1662,\t,sm:1.23\n",
      "iter: 00799, \t precision: 0.8373,\t best_acc:0.8434\n",
      "step: 799,\t,class_loss:0.0627,\t,sm:1.17\n",
      "step: 800,\t,class_loss:0.1313,\t,sm:1.05\n",
      "step: 801,\t,class_loss:0.1649,\t,sm:1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 802,\t,class_loss:0.1978,\t,sm:1.21\n",
      "step: 803,\t,class_loss:0.3068,\t,sm:1.32\n",
      "step: 804,\t,class_loss:0.1967,\t,sm:1.19\n",
      "step: 805,\t,class_loss:0.0746,\t,sm:1.17\n",
      "step: 806,\t,class_loss:0.0689,\t,sm:1.19\n",
      "step: 807,\t,class_loss:0.2379,\t,sm:1.15\n",
      "step: 808,\t,class_loss:0.1959,\t,sm:1.21\n",
      "step: 809,\t,class_loss:0.1841,\t,sm:1.25\n",
      "step: 810,\t,class_loss:0.1624,\t,sm:1.08\n",
      "step: 811,\t,class_loss:0.2090,\t,sm:1.15\n",
      "step: 812,\t,class_loss:0.2796,\t,sm:1.22\n",
      "step: 813,\t,class_loss:0.3676,\t,sm:1.26\n",
      "step: 814,\t,class_loss:0.2323,\t,sm:1.24\n",
      "step: 815,\t,class_loss:0.1964,\t,sm:1.09\n",
      "step: 816,\t,class_loss:0.1517,\t,sm:1.12\n",
      "step: 817,\t,class_loss:0.1466,\t,sm:1.10\n",
      "step: 818,\t,class_loss:0.2412,\t,sm:0.98\n",
      "step: 819,\t,class_loss:0.1241,\t,sm:1.17\n",
      "step: 820,\t,class_loss:0.2572,\t,sm:1.20\n",
      "step: 821,\t,class_loss:0.1218,\t,sm:1.11\n",
      "step: 822,\t,class_loss:0.1913,\t,sm:1.14\n",
      "step: 823,\t,class_loss:0.2007,\t,sm:1.00\n",
      "step: 824,\t,class_loss:0.0995,\t,sm:1.06\n",
      "step: 825,\t,class_loss:0.0543,\t,sm:1.05\n",
      "step: 826,\t,class_loss:0.1849,\t,sm:1.06\n",
      "step: 827,\t,class_loss:0.1107,\t,sm:0.98\n",
      "step: 828,\t,class_loss:0.2021,\t,sm:1.18\n",
      "step: 829,\t,class_loss:0.2537,\t,sm:1.12\n",
      "step: 830,\t,class_loss:0.0953,\t,sm:1.16\n",
      "step: 831,\t,class_loss:0.1747,\t,sm:1.25\n",
      "step: 832,\t,class_loss:0.1104,\t,sm:1.13\n",
      "step: 833,\t,class_loss:0.0589,\t,sm:1.18\n",
      "step: 834,\t,class_loss:0.1336,\t,sm:1.21\n",
      "step: 835,\t,class_loss:0.1461,\t,sm:1.01\n",
      "step: 836,\t,class_loss:0.1435,\t,sm:1.08\n",
      "step: 837,\t,class_loss:0.1886,\t,sm:1.11\n",
      "step: 838,\t,class_loss:0.0994,\t,sm:1.17\n",
      "step: 839,\t,class_loss:0.1688,\t,sm:1.21\n",
      "step: 840,\t,class_loss:0.3069,\t,sm:1.23\n",
      "step: 841,\t,class_loss:0.0921,\t,sm:1.22\n",
      "step: 842,\t,class_loss:0.0468,\t,sm:1.13\n",
      "step: 843,\t,class_loss:0.1623,\t,sm:1.13\n",
      "step: 844,\t,class_loss:0.1643,\t,sm:1.05\n",
      "step: 845,\t,class_loss:0.1294,\t,sm:1.06\n",
      "step: 846,\t,class_loss:0.1597,\t,sm:1.04\n",
      "step: 847,\t,class_loss:0.2092,\t,sm:0.91\n",
      "step: 848,\t,class_loss:0.1023,\t,sm:1.09\n",
      "iter: 00849, \t precision: 0.8474,\t best_acc:0.8474\n",
      "step: 849,\t,class_loss:0.2124,\t,sm:1.17\n",
      "step: 850,\t,class_loss:0.0705,\t,sm:1.20\n",
      "step: 851,\t,class_loss:0.0501,\t,sm:1.15\n",
      "step: 852,\t,class_loss:0.0708,\t,sm:1.05\n",
      "step: 853,\t,class_loss:0.1708,\t,sm:0.97\n",
      "step: 854,\t,class_loss:0.1104,\t,sm:1.01\n",
      "step: 855,\t,class_loss:0.1494,\t,sm:0.92\n",
      "step: 856,\t,class_loss:0.1112,\t,sm:0.91\n",
      "step: 857,\t,class_loss:0.1243,\t,sm:0.95\n",
      "step: 858,\t,class_loss:0.0900,\t,sm:1.09\n",
      "step: 859,\t,class_loss:0.0981,\t,sm:1.09\n",
      "step: 860,\t,class_loss:0.2536,\t,sm:1.22\n",
      "step: 861,\t,class_loss:0.1542,\t,sm:1.11\n",
      "step: 862,\t,class_loss:0.0752,\t,sm:1.11\n",
      "step: 863,\t,class_loss:0.0672,\t,sm:1.03\n",
      "step: 864,\t,class_loss:0.1816,\t,sm:1.07\n",
      "step: 865,\t,class_loss:0.1041,\t,sm:1.10\n",
      "step: 866,\t,class_loss:0.1204,\t,sm:0.98\n",
      "step: 867,\t,class_loss:0.0708,\t,sm:0.99\n",
      "step: 868,\t,class_loss:0.1726,\t,sm:0.93\n",
      "step: 869,\t,class_loss:0.1168,\t,sm:0.99\n",
      "step: 870,\t,class_loss:0.1080,\t,sm:0.98\n",
      "step: 871,\t,class_loss:0.2696,\t,sm:1.07\n",
      "step: 872,\t,class_loss:0.1826,\t,sm:1.09\n",
      "step: 873,\t,class_loss:0.1065,\t,sm:1.02\n",
      "step: 874,\t,class_loss:0.2162,\t,sm:1.11\n",
      "step: 875,\t,class_loss:0.1594,\t,sm:1.16\n",
      "step: 876,\t,class_loss:0.0632,\t,sm:1.15\n",
      "step: 877,\t,class_loss:0.1248,\t,sm:1.10\n",
      "step: 878,\t,class_loss:0.1886,\t,sm:1.05\n",
      "step: 879,\t,class_loss:0.1091,\t,sm:1.08\n",
      "step: 880,\t,class_loss:0.1640,\t,sm:0.97\n",
      "step: 881,\t,class_loss:0.2067,\t,sm:0.97\n",
      "step: 882,\t,class_loss:0.0984,\t,sm:1.00\n",
      "step: 883,\t,class_loss:0.0862,\t,sm:1.00\n",
      "step: 884,\t,class_loss:0.0938,\t,sm:1.08\n",
      "step: 885,\t,class_loss:0.0825,\t,sm:0.92\n",
      "step: 886,\t,class_loss:0.1125,\t,sm:0.93\n",
      "step: 887,\t,class_loss:0.0737,\t,sm:1.01\n",
      "step: 888,\t,class_loss:0.0944,\t,sm:1.08\n",
      "step: 889,\t,class_loss:0.1657,\t,sm:1.27\n",
      "step: 890,\t,class_loss:0.2146,\t,sm:1.21\n",
      "step: 891,\t,class_loss:0.2282,\t,sm:1.15\n",
      "step: 892,\t,class_loss:0.1088,\t,sm:1.20\n",
      "step: 893,\t,class_loss:0.0619,\t,sm:1.12\n",
      "step: 894,\t,class_loss:0.3350,\t,sm:1.07\n",
      "step: 895,\t,class_loss:0.0688,\t,sm:1.09\n",
      "step: 896,\t,class_loss:0.2196,\t,sm:1.18\n",
      "step: 897,\t,class_loss:0.1679,\t,sm:1.34\n",
      "step: 898,\t,class_loss:0.0953,\t,sm:1.26\n",
      "iter: 00899, \t precision: 0.8474,\t best_acc:0.8474\n",
      "step: 899,\t,class_loss:0.0730,\t,sm:1.07\n",
      "step: 900,\t,class_loss:0.1352,\t,sm:0.99\n",
      "step: 901,\t,class_loss:0.1039,\t,sm:0.98\n",
      "step: 902,\t,class_loss:0.0841,\t,sm:1.05\n",
      "step: 903,\t,class_loss:0.0808,\t,sm:1.15\n",
      "step: 904,\t,class_loss:0.1218,\t,sm:1.19\n",
      "step: 905,\t,class_loss:0.1893,\t,sm:1.04\n",
      "step: 906,\t,class_loss:0.1742,\t,sm:0.96\n",
      "step: 907,\t,class_loss:0.1043,\t,sm:0.93\n",
      "step: 908,\t,class_loss:0.0936,\t,sm:0.93\n",
      "step: 909,\t,class_loss:0.1093,\t,sm:1.00\n",
      "step: 910,\t,class_loss:0.1950,\t,sm:1.07\n",
      "step: 911,\t,class_loss:0.0950,\t,sm:1.08\n",
      "step: 912,\t,class_loss:0.1951,\t,sm:1.00\n",
      "step: 913,\t,class_loss:0.1743,\t,sm:1.00\n",
      "step: 914,\t,class_loss:0.2321,\t,sm:1.00\n",
      "step: 915,\t,class_loss:0.1038,\t,sm:1.02\n",
      "step: 916,\t,class_loss:0.0978,\t,sm:0.94\n",
      "step: 917,\t,class_loss:0.0772,\t,sm:1.07\n",
      "step: 918,\t,class_loss:0.0708,\t,sm:0.94\n",
      "step: 919,\t,class_loss:0.0689,\t,sm:0.94\n",
      "step: 920,\t,class_loss:0.0638,\t,sm:0.96\n",
      "step: 921,\t,class_loss:0.1379,\t,sm:0.99\n",
      "step: 922,\t,class_loss:0.2346,\t,sm:1.03\n",
      "step: 923,\t,class_loss:0.1279,\t,sm:1.00\n",
      "step: 924,\t,class_loss:0.0331,\t,sm:1.06\n",
      "step: 925,\t,class_loss:0.1691,\t,sm:1.16\n",
      "step: 926,\t,class_loss:0.1689,\t,sm:1.25\n",
      "step: 927,\t,class_loss:0.1896,\t,sm:1.15\n",
      "step: 928,\t,class_loss:0.1219,\t,sm:1.08\n",
      "step: 929,\t,class_loss:0.1298,\t,sm:0.88\n",
      "step: 930,\t,class_loss:0.0797,\t,sm:0.90\n",
      "step: 931,\t,class_loss:0.1533,\t,sm:0.90\n",
      "step: 932,\t,class_loss:0.0865,\t,sm:0.89\n",
      "step: 933,\t,class_loss:0.1980,\t,sm:1.04\n",
      "step: 934,\t,class_loss:0.2289,\t,sm:0.98\n",
      "step: 935,\t,class_loss:0.2161,\t,sm:0.94\n",
      "step: 936,\t,class_loss:0.0894,\t,sm:0.99\n",
      "step: 937,\t,class_loss:0.0532,\t,sm:0.92\n",
      "step: 938,\t,class_loss:0.0638,\t,sm:0.99\n",
      "step: 939,\t,class_loss:0.1468,\t,sm:1.00\n",
      "step: 940,\t,class_loss:0.1626,\t,sm:0.89\n",
      "step: 941,\t,class_loss:0.2199,\t,sm:0.89\n",
      "step: 942,\t,class_loss:0.1515,\t,sm:1.03\n",
      "step: 943,\t,class_loss:0.1711,\t,sm:0.95\n",
      "step: 944,\t,class_loss:0.0666,\t,sm:0.89\n",
      "step: 945,\t,class_loss:0.0525,\t,sm:0.93\n",
      "step: 946,\t,class_loss:0.2204,\t,sm:0.96\n",
      "step: 947,\t,class_loss:0.1211,\t,sm:0.85\n",
      "step: 948,\t,class_loss:0.1593,\t,sm:1.03\n",
      "iter: 00949, \t precision: 0.8514,\t best_acc:0.8514\n",
      "step: 949,\t,class_loss:0.0792,\t,sm:1.11\n",
      "step: 950,\t,class_loss:0.1645,\t,sm:1.02\n",
      "step: 951,\t,class_loss:0.1301,\t,sm:0.98\n",
      "step: 952,\t,class_loss:0.0563,\t,sm:0.85\n",
      "step: 953,\t,class_loss:0.0846,\t,sm:0.83\n",
      "step: 954,\t,class_loss:0.0595,\t,sm:0.83\n",
      "step: 955,\t,class_loss:0.1030,\t,sm:0.85\n",
      "step: 956,\t,class_loss:0.1165,\t,sm:0.94\n",
      "step: 957,\t,class_loss:0.1074,\t,sm:0.91\n",
      "step: 958,\t,class_loss:0.1222,\t,sm:0.98\n",
      "step: 959,\t,class_loss:0.0885,\t,sm:0.87\n",
      "step: 960,\t,class_loss:0.0941,\t,sm:0.81\n",
      "step: 961,\t,class_loss:0.2514,\t,sm:0.82\n",
      "step: 962,\t,class_loss:0.0653,\t,sm:0.88\n",
      "step: 963,\t,class_loss:0.1089,\t,sm:0.92\n",
      "step: 964,\t,class_loss:0.0823,\t,sm:0.84\n",
      "step: 965,\t,class_loss:0.0512,\t,sm:0.92\n",
      "step: 966,\t,class_loss:0.0704,\t,sm:0.88\n",
      "step: 967,\t,class_loss:0.1354,\t,sm:0.86\n",
      "step: 968,\t,class_loss:0.1066,\t,sm:0.85\n",
      "step: 969,\t,class_loss:0.0583,\t,sm:0.83\n",
      "step: 970,\t,class_loss:0.1948,\t,sm:0.87\n",
      "step: 971,\t,class_loss:0.2014,\t,sm:0.98\n",
      "step: 972,\t,class_loss:0.0557,\t,sm:0.90\n",
      "step: 973,\t,class_loss:0.0845,\t,sm:0.96\n",
      "step: 974,\t,class_loss:0.1681,\t,sm:0.86\n",
      "step: 975,\t,class_loss:0.1715,\t,sm:0.99\n",
      "step: 976,\t,class_loss:0.0424,\t,sm:0.99\n",
      "step: 977,\t,class_loss:0.0541,\t,sm:0.98\n",
      "step: 978,\t,class_loss:0.0979,\t,sm:0.94\n",
      "step: 979,\t,class_loss:0.0658,\t,sm:0.89\n",
      "step: 980,\t,class_loss:0.0571,\t,sm:0.85\n",
      "step: 981,\t,class_loss:0.0648,\t,sm:0.83\n",
      "step: 982,\t,class_loss:0.0915,\t,sm:0.94\n",
      "step: 983,\t,class_loss:0.0871,\t,sm:0.98\n",
      "step: 984,\t,class_loss:0.0929,\t,sm:0.87\n",
      "step: 985,\t,class_loss:0.2134,\t,sm:0.81\n",
      "step: 986,\t,class_loss:0.0505,\t,sm:0.86\n",
      "step: 987,\t,class_loss:0.0952,\t,sm:0.88\n",
      "step: 988,\t,class_loss:0.1147,\t,sm:0.96\n",
      "step: 989,\t,class_loss:0.1661,\t,sm:1.01\n",
      "step: 990,\t,class_loss:0.1526,\t,sm:0.95\n",
      "step: 991,\t,class_loss:0.1203,\t,sm:0.88\n",
      "step: 992,\t,class_loss:0.1528,\t,sm:0.87\n",
      "step: 993,\t,class_loss:0.1263,\t,sm:0.81\n",
      "step: 994,\t,class_loss:0.0760,\t,sm:0.80\n",
      "step: 995,\t,class_loss:0.1149,\t,sm:0.84\n",
      "step: 996,\t,class_loss:0.0701,\t,sm:0.93\n",
      "step: 997,\t,class_loss:0.0933,\t,sm:0.92\n",
      "step: 998,\t,class_loss:0.0898,\t,sm:0.89\n",
      "iter: 00999, \t precision: 0.8474,\t best_acc:0.8514\n",
      "step: 999,\t,class_loss:0.0919,\t,sm:0.92\n",
      "step: 1000,\t,class_loss:0.0975,\t,sm:0.99\n",
      "step: 1001,\t,class_loss:0.1671,\t,sm:0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1002,\t,class_loss:0.0870,\t,sm:0.93\n",
      "step: 1003,\t,class_loss:0.1255,\t,sm:0.93\n",
      "step: 1004,\t,class_loss:0.2083,\t,sm:0.89\n",
      "step: 1005,\t,class_loss:0.0724,\t,sm:0.83\n",
      "step: 1006,\t,class_loss:0.1751,\t,sm:0.79\n",
      "step: 1007,\t,class_loss:0.1002,\t,sm:0.97\n",
      "step: 1008,\t,class_loss:0.1314,\t,sm:1.08\n",
      "step: 1009,\t,class_loss:0.2790,\t,sm:1.03\n",
      "step: 1010,\t,class_loss:0.0960,\t,sm:0.90\n",
      "step: 1011,\t,class_loss:0.0975,\t,sm:0.84\n",
      "step: 1012,\t,class_loss:0.2114,\t,sm:0.84\n",
      "step: 1013,\t,class_loss:0.1252,\t,sm:1.00\n",
      "step: 1014,\t,class_loss:0.0803,\t,sm:1.10\n",
      "step: 1015,\t,class_loss:0.1223,\t,sm:1.00\n",
      "step: 1016,\t,class_loss:0.0363,\t,sm:0.89\n",
      "step: 1017,\t,class_loss:0.0334,\t,sm:0.85\n",
      "step: 1018,\t,class_loss:0.1473,\t,sm:0.85\n",
      "step: 1019,\t,class_loss:0.1195,\t,sm:0.83\n",
      "step: 1020,\t,class_loss:0.1346,\t,sm:0.87\n",
      "step: 1021,\t,class_loss:0.0805,\t,sm:0.84\n",
      "step: 1022,\t,class_loss:0.1680,\t,sm:0.87\n",
      "step: 1023,\t,class_loss:0.2438,\t,sm:0.93\n",
      "step: 1024,\t,class_loss:0.1191,\t,sm:0.99\n",
      "step: 1025,\t,class_loss:0.1087,\t,sm:1.06\n",
      "step: 1026,\t,class_loss:0.0736,\t,sm:0.93\n",
      "step: 1027,\t,class_loss:0.0852,\t,sm:1.00\n",
      "step: 1028,\t,class_loss:0.0831,\t,sm:0.87\n",
      "step: 1029,\t,class_loss:0.1512,\t,sm:0.88\n",
      "step: 1030,\t,class_loss:0.0799,\t,sm:0.90\n",
      "step: 1031,\t,class_loss:0.0679,\t,sm:0.81\n",
      "step: 1032,\t,class_loss:0.0678,\t,sm:0.74\n",
      "step: 1033,\t,class_loss:0.0323,\t,sm:0.77\n",
      "step: 1034,\t,class_loss:0.1122,\t,sm:0.84\n",
      "step: 1035,\t,class_loss:0.0411,\t,sm:0.83\n",
      "step: 1036,\t,class_loss:0.1859,\t,sm:0.79\n",
      "step: 1037,\t,class_loss:0.1189,\t,sm:0.84\n",
      "step: 1038,\t,class_loss:0.0588,\t,sm:0.87\n",
      "step: 1039,\t,class_loss:0.1022,\t,sm:0.90\n",
      "step: 1040,\t,class_loss:0.1470,\t,sm:0.86\n",
      "step: 1041,\t,class_loss:0.1065,\t,sm:0.90\n",
      "step: 1042,\t,class_loss:0.0689,\t,sm:0.79\n",
      "step: 1043,\t,class_loss:0.0425,\t,sm:0.78\n",
      "step: 1044,\t,class_loss:0.0519,\t,sm:0.80\n",
      "step: 1045,\t,class_loss:0.0671,\t,sm:0.96\n",
      "step: 1046,\t,class_loss:0.1390,\t,sm:1.07\n",
      "step: 1047,\t,class_loss:0.1371,\t,sm:1.18\n",
      "step: 1048,\t,class_loss:0.0678,\t,sm:1.00\n",
      "iter: 01049, \t precision: 0.8514,\t best_acc:0.8514\n",
      "step: 1049,\t,class_loss:0.1023,\t,sm:0.95\n",
      "step: 1050,\t,class_loss:0.0637,\t,sm:0.87\n",
      "step: 1051,\t,class_loss:0.1220,\t,sm:0.79\n",
      "step: 1052,\t,class_loss:0.2038,\t,sm:0.80\n",
      "step: 1053,\t,class_loss:0.1101,\t,sm:0.87\n",
      "step: 1054,\t,class_loss:0.1072,\t,sm:0.90\n",
      "step: 1055,\t,class_loss:0.1481,\t,sm:1.01\n",
      "step: 1056,\t,class_loss:0.0745,\t,sm:0.93\n",
      "step: 1057,\t,class_loss:0.1086,\t,sm:0.87\n",
      "step: 1058,\t,class_loss:0.1355,\t,sm:0.87\n",
      "step: 1059,\t,class_loss:0.0460,\t,sm:0.94\n",
      "step: 1060,\t,class_loss:0.0742,\t,sm:0.93\n",
      "step: 1061,\t,class_loss:0.0475,\t,sm:0.84\n",
      "step: 1062,\t,class_loss:0.0652,\t,sm:0.88\n",
      "step: 1063,\t,class_loss:0.1764,\t,sm:0.86\n",
      "step: 1064,\t,class_loss:0.1149,\t,sm:0.83\n",
      "step: 1065,\t,class_loss:0.0869,\t,sm:0.87\n",
      "step: 1066,\t,class_loss:0.1415,\t,sm:1.00\n",
      "step: 1067,\t,class_loss:0.1310,\t,sm:1.08\n",
      "step: 1068,\t,class_loss:0.2316,\t,sm:0.84\n",
      "step: 1069,\t,class_loss:0.0336,\t,sm:0.79\n",
      "step: 1070,\t,class_loss:0.0331,\t,sm:0.77\n",
      "step: 1071,\t,class_loss:0.0695,\t,sm:0.89\n",
      "step: 1072,\t,class_loss:0.0714,\t,sm:0.95\n",
      "step: 1073,\t,class_loss:0.0946,\t,sm:0.87\n",
      "step: 1074,\t,class_loss:0.0960,\t,sm:0.82\n",
      "step: 1075,\t,class_loss:0.1398,\t,sm:1.01\n",
      "step: 1076,\t,class_loss:0.1558,\t,sm:1.01\n",
      "step: 1077,\t,class_loss:0.1207,\t,sm:0.93\n",
      "step: 1078,\t,class_loss:0.0788,\t,sm:0.86\n",
      "step: 1079,\t,class_loss:0.0907,\t,sm:0.90\n",
      "step: 1080,\t,class_loss:0.1634,\t,sm:0.89\n",
      "step: 1081,\t,class_loss:0.0757,\t,sm:0.88\n",
      "step: 1082,\t,class_loss:0.1812,\t,sm:0.89\n",
      "step: 1083,\t,class_loss:0.2193,\t,sm:0.98\n",
      "step: 1084,\t,class_loss:0.0942,\t,sm:0.99\n",
      "step: 1085,\t,class_loss:0.0860,\t,sm:0.90\n",
      "step: 1086,\t,class_loss:0.1292,\t,sm:0.89\n",
      "step: 1087,\t,class_loss:0.1337,\t,sm:0.82\n",
      "step: 1088,\t,class_loss:0.0402,\t,sm:0.76\n",
      "step: 1089,\t,class_loss:0.1023,\t,sm:0.76\n",
      "step: 1090,\t,class_loss:0.1140,\t,sm:0.85\n",
      "step: 1091,\t,class_loss:0.1082,\t,sm:0.88\n",
      "step: 1092,\t,class_loss:0.1310,\t,sm:0.98\n",
      "step: 1093,\t,class_loss:0.1285,\t,sm:1.03\n",
      "step: 1094,\t,class_loss:0.0611,\t,sm:0.93\n",
      "step: 1095,\t,class_loss:0.0921,\t,sm:0.86\n",
      "step: 1096,\t,class_loss:0.0735,\t,sm:0.80\n",
      "step: 1097,\t,class_loss:0.1083,\t,sm:0.74\n",
      "step: 1098,\t,class_loss:0.1709,\t,sm:0.85\n",
      "iter: 01099, \t precision: 0.8514,\t best_acc:0.8514\n",
      "step: 1099,\t,class_loss:0.0917,\t,sm:0.87\n",
      "step: 1100,\t,class_loss:0.0655,\t,sm:0.76\n",
      "step: 1101,\t,class_loss:0.1692,\t,sm:0.88\n",
      "step: 1102,\t,class_loss:0.0761,\t,sm:0.93\n",
      "step: 1103,\t,class_loss:0.1884,\t,sm:0.82\n",
      "step: 1104,\t,class_loss:0.0803,\t,sm:0.83\n",
      "step: 1105,\t,class_loss:0.0846,\t,sm:0.86\n",
      "step: 1106,\t,class_loss:0.0463,\t,sm:0.94\n",
      "step: 1107,\t,class_loss:0.0505,\t,sm:0.91\n",
      "step: 1108,\t,class_loss:0.2185,\t,sm:1.03\n",
      "step: 1109,\t,class_loss:0.1408,\t,sm:0.95\n",
      "step: 1110,\t,class_loss:0.1318,\t,sm:0.90\n",
      "step: 1111,\t,class_loss:0.1459,\t,sm:0.79\n",
      "step: 1112,\t,class_loss:0.1433,\t,sm:0.75\n",
      "step: 1113,\t,class_loss:0.0736,\t,sm:0.79\n",
      "step: 1114,\t,class_loss:0.1849,\t,sm:0.77\n",
      "step: 1115,\t,class_loss:0.0644,\t,sm:0.80\n",
      "step: 1116,\t,class_loss:0.1278,\t,sm:0.76\n",
      "step: 1117,\t,class_loss:0.0342,\t,sm:0.74\n",
      "step: 1118,\t,class_loss:0.0598,\t,sm:0.86\n",
      "step: 1119,\t,class_loss:0.2512,\t,sm:0.88\n",
      "step: 1120,\t,class_loss:0.2642,\t,sm:0.86\n",
      "step: 1121,\t,class_loss:0.0503,\t,sm:0.81\n",
      "step: 1122,\t,class_loss:0.1108,\t,sm:0.83\n",
      "step: 1123,\t,class_loss:0.0464,\t,sm:0.82\n",
      "step: 1124,\t,class_loss:0.1090,\t,sm:0.88\n",
      "step: 1125,\t,class_loss:0.0460,\t,sm:0.87\n",
      "step: 1126,\t,class_loss:0.0879,\t,sm:0.79\n",
      "step: 1127,\t,class_loss:0.0544,\t,sm:0.81\n",
      "step: 1128,\t,class_loss:0.0435,\t,sm:0.74\n",
      "step: 1129,\t,class_loss:0.1089,\t,sm:0.71\n",
      "step: 1130,\t,class_loss:0.0619,\t,sm:0.74\n",
      "step: 1131,\t,class_loss:0.0330,\t,sm:0.75\n",
      "step: 1132,\t,class_loss:0.0573,\t,sm:0.82\n",
      "step: 1133,\t,class_loss:0.0858,\t,sm:0.84\n",
      "step: 1134,\t,class_loss:0.0472,\t,sm:0.85\n",
      "step: 1135,\t,class_loss:0.0221,\t,sm:0.71\n",
      "step: 1136,\t,class_loss:0.0530,\t,sm:0.81\n",
      "step: 1137,\t,class_loss:0.1051,\t,sm:0.88\n",
      "step: 1138,\t,class_loss:0.0594,\t,sm:0.82\n",
      "step: 1139,\t,class_loss:0.0915,\t,sm:0.77\n",
      "step: 1140,\t,class_loss:0.0938,\t,sm:0.70\n",
      "step: 1141,\t,class_loss:0.0248,\t,sm:0.72\n",
      "step: 1142,\t,class_loss:0.1397,\t,sm:0.68\n",
      "step: 1143,\t,class_loss:0.0581,\t,sm:0.74\n",
      "step: 1144,\t,class_loss:0.1253,\t,sm:0.83\n",
      "step: 1145,\t,class_loss:0.0715,\t,sm:0.87\n",
      "step: 1146,\t,class_loss:0.1331,\t,sm:0.86\n",
      "step: 1147,\t,class_loss:0.0531,\t,sm:0.87\n",
      "step: 1148,\t,class_loss:0.0280,\t,sm:0.79\n",
      "iter: 01149, \t precision: 0.8514,\t best_acc:0.8514\n",
      "step: 1149,\t,class_loss:0.1519,\t,sm:0.87\n",
      "step: 1150,\t,class_loss:0.0511,\t,sm:0.82\n",
      "step: 1151,\t,class_loss:0.1474,\t,sm:0.91\n",
      "step: 1152,\t,class_loss:0.0582,\t,sm:0.77\n",
      "step: 1153,\t,class_loss:0.0653,\t,sm:0.73\n",
      "step: 1154,\t,class_loss:0.0959,\t,sm:0.80\n",
      "step: 1155,\t,class_loss:0.0392,\t,sm:0.72\n",
      "step: 1156,\t,class_loss:0.1082,\t,sm:0.67\n",
      "step: 1157,\t,class_loss:0.2156,\t,sm:0.82\n",
      "step: 1158,\t,class_loss:0.0786,\t,sm:0.82\n",
      "step: 1159,\t,class_loss:0.0891,\t,sm:0.74\n",
      "step: 1160,\t,class_loss:0.0368,\t,sm:0.65\n",
      "step: 1161,\t,class_loss:0.1074,\t,sm:0.68\n",
      "step: 1162,\t,class_loss:0.0424,\t,sm:0.65\n",
      "step: 1163,\t,class_loss:0.0472,\t,sm:0.66\n",
      "step: 1164,\t,class_loss:0.0816,\t,sm:0.70\n",
      "step: 1165,\t,class_loss:0.0770,\t,sm:0.78\n",
      "step: 1166,\t,class_loss:0.0960,\t,sm:0.81\n",
      "step: 1167,\t,class_loss:0.1050,\t,sm:0.73\n",
      "step: 1168,\t,class_loss:0.1377,\t,sm:0.78\n",
      "step: 1169,\t,class_loss:0.0834,\t,sm:0.83\n",
      "step: 1170,\t,class_loss:0.1035,\t,sm:0.81\n",
      "step: 1171,\t,class_loss:0.0793,\t,sm:0.70\n",
      "step: 1172,\t,class_loss:0.1042,\t,sm:0.83\n",
      "step: 1173,\t,class_loss:0.0666,\t,sm:0.74\n",
      "step: 1174,\t,class_loss:0.0612,\t,sm:0.74\n",
      "step: 1175,\t,class_loss:0.0677,\t,sm:0.69\n",
      "step: 1176,\t,class_loss:0.1552,\t,sm:0.64\n",
      "step: 1177,\t,class_loss:0.0557,\t,sm:0.64\n",
      "step: 1178,\t,class_loss:0.0265,\t,sm:0.67\n",
      "step: 1179,\t,class_loss:0.1481,\t,sm:0.74\n",
      "step: 1180,\t,class_loss:0.1366,\t,sm:0.77\n",
      "step: 1181,\t,class_loss:0.1672,\t,sm:0.70\n",
      "step: 1182,\t,class_loss:0.0671,\t,sm:0.73\n",
      "step: 1183,\t,class_loss:0.0313,\t,sm:0.73\n",
      "step: 1184,\t,class_loss:0.0341,\t,sm:0.80\n",
      "step: 1185,\t,class_loss:0.0934,\t,sm:0.77\n",
      "step: 1186,\t,class_loss:0.0468,\t,sm:0.82\n",
      "step: 1187,\t,class_loss:0.0433,\t,sm:0.77\n",
      "step: 1188,\t,class_loss:0.1472,\t,sm:0.83\n",
      "step: 1189,\t,class_loss:0.0516,\t,sm:0.85\n",
      "step: 1190,\t,class_loss:0.0378,\t,sm:0.79\n",
      "step: 1191,\t,class_loss:0.1191,\t,sm:0.79\n",
      "step: 1192,\t,class_loss:0.0937,\t,sm:0.72\n",
      "step: 1193,\t,class_loss:0.0795,\t,sm:0.81\n",
      "step: 1194,\t,class_loss:0.2221,\t,sm:0.78\n",
      "step: 1195,\t,class_loss:0.0566,\t,sm:0.74\n",
      "step: 1196,\t,class_loss:0.0927,\t,sm:0.73\n",
      "step: 1197,\t,class_loss:0.0816,\t,sm:0.83\n",
      "step: 1198,\t,class_loss:0.0819,\t,sm:0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 01199, \t precision: 0.8514,\t best_acc:0.8514\n",
      "step: 1199,\t,class_loss:0.1728,\t,sm:0.80\n",
      "step: 1200,\t,class_loss:0.1564,\t,sm:1.12\n",
      "step: 1201,\t,class_loss:0.0435,\t,sm:1.03\n",
      "step: 1202,\t,class_loss:0.1452,\t,sm:0.82\n",
      "step: 1203,\t,class_loss:0.0543,\t,sm:0.92\n",
      "step: 1204,\t,class_loss:0.0759,\t,sm:0.81\n",
      "step: 1205,\t,class_loss:0.0418,\t,sm:0.81\n",
      "step: 1206,\t,class_loss:0.0866,\t,sm:0.78\n",
      "step: 1207,\t,class_loss:0.0494,\t,sm:0.74\n",
      "step: 1208,\t,class_loss:0.1377,\t,sm:0.75\n",
      "step: 1209,\t,class_loss:0.0485,\t,sm:0.80\n",
      "step: 1210,\t,class_loss:0.0833,\t,sm:0.89\n",
      "step: 1211,\t,class_loss:0.0644,\t,sm:0.83\n",
      "step: 1212,\t,class_loss:0.0296,\t,sm:0.78\n",
      "step: 1213,\t,class_loss:0.0535,\t,sm:0.70\n",
      "step: 1214,\t,class_loss:0.0625,\t,sm:0.68\n",
      "step: 1215,\t,class_loss:0.1399,\t,sm:0.83\n",
      "step: 1216,\t,class_loss:0.0885,\t,sm:0.79\n",
      "step: 1217,\t,class_loss:0.0585,\t,sm:0.79\n",
      "step: 1218,\t,class_loss:0.0385,\t,sm:0.72\n",
      "step: 1219,\t,class_loss:0.1394,\t,sm:0.78\n",
      "step: 1220,\t,class_loss:0.0796,\t,sm:0.76\n",
      "step: 1221,\t,class_loss:0.0375,\t,sm:0.71\n",
      "step: 1222,\t,class_loss:0.0615,\t,sm:0.70\n",
      "step: 1223,\t,class_loss:0.0421,\t,sm:0.81\n",
      "step: 1224,\t,class_loss:0.0480,\t,sm:0.83\n",
      "step: 1225,\t,class_loss:0.1445,\t,sm:0.89\n",
      "step: 1226,\t,class_loss:0.0693,\t,sm:0.78\n",
      "step: 1227,\t,class_loss:0.0639,\t,sm:0.81\n",
      "step: 1228,\t,class_loss:0.0539,\t,sm:0.79\n",
      "step: 1229,\t,class_loss:0.0453,\t,sm:0.76\n",
      "step: 1230,\t,class_loss:0.1010,\t,sm:0.78\n",
      "step: 1231,\t,class_loss:0.0329,\t,sm:0.70\n",
      "step: 1232,\t,class_loss:0.0598,\t,sm:0.69\n",
      "step: 1233,\t,class_loss:0.0791,\t,sm:0.71\n",
      "step: 1234,\t,class_loss:0.0675,\t,sm:0.73\n",
      "step: 1235,\t,class_loss:0.0423,\t,sm:0.58\n",
      "step: 1236,\t,class_loss:0.0648,\t,sm:0.61\n",
      "step: 1237,\t,class_loss:0.1727,\t,sm:0.65\n",
      "step: 1238,\t,class_loss:0.0971,\t,sm:0.73\n",
      "step: 1239,\t,class_loss:0.1145,\t,sm:0.75\n",
      "step: 1240,\t,class_loss:0.1158,\t,sm:0.73\n",
      "step: 1241,\t,class_loss:0.0898,\t,sm:0.78\n",
      "step: 1242,\t,class_loss:0.1814,\t,sm:0.80\n",
      "step: 1243,\t,class_loss:0.1061,\t,sm:0.79\n",
      "step: 1244,\t,class_loss:0.0643,\t,sm:0.77\n",
      "step: 1245,\t,class_loss:0.0723,\t,sm:0.74\n",
      "step: 1246,\t,class_loss:0.0395,\t,sm:0.69\n",
      "step: 1247,\t,class_loss:0.0486,\t,sm:0.75\n",
      "step: 1248,\t,class_loss:0.0485,\t,sm:0.66\n",
      "iter: 01249, \t precision: 0.8474,\t best_acc:0.8514\n",
      "step: 1249,\t,class_loss:0.0537,\t,sm:0.72\n",
      "step: 1250,\t,class_loss:0.1671,\t,sm:0.83\n",
      "step: 1251,\t,class_loss:0.0378,\t,sm:0.82\n",
      "step: 1252,\t,class_loss:0.0378,\t,sm:0.72\n",
      "step: 1253,\t,class_loss:0.0958,\t,sm:0.75\n",
      "step: 1254,\t,class_loss:0.0485,\t,sm:0.70\n",
      "step: 1255,\t,class_loss:0.0745,\t,sm:0.72\n",
      "step: 1256,\t,class_loss:0.1115,\t,sm:0.70\n",
      "step: 1257,\t,class_loss:0.0246,\t,sm:0.69\n",
      "step: 1258,\t,class_loss:0.1089,\t,sm:0.69\n",
      "step: 1259,\t,class_loss:0.0529,\t,sm:0.71\n",
      "step: 1260,\t,class_loss:0.0581,\t,sm:0.66\n",
      "step: 1261,\t,class_loss:0.0320,\t,sm:0.68\n",
      "step: 1262,\t,class_loss:0.0590,\t,sm:0.67\n",
      "step: 1263,\t,class_loss:0.0756,\t,sm:0.77\n",
      "step: 1264,\t,class_loss:0.2071,\t,sm:0.93\n",
      "step: 1265,\t,class_loss:0.1055,\t,sm:0.85\n",
      "step: 1266,\t,class_loss:0.0418,\t,sm:0.83\n",
      "step: 1267,\t,class_loss:0.0448,\t,sm:0.89\n",
      "step: 1268,\t,class_loss:0.0589,\t,sm:0.87\n",
      "step: 1269,\t,class_loss:0.0753,\t,sm:0.85\n",
      "step: 1270,\t,class_loss:0.0371,\t,sm:0.82\n",
      "step: 1271,\t,class_loss:0.0689,\t,sm:0.79\n",
      "step: 1272,\t,class_loss:0.1125,\t,sm:0.80\n",
      "step: 1273,\t,class_loss:0.1317,\t,sm:0.75\n",
      "step: 1274,\t,class_loss:0.0584,\t,sm:0.79\n",
      "step: 1275,\t,class_loss:0.1292,\t,sm:0.78\n",
      "step: 1276,\t,class_loss:0.0444,\t,sm:0.72\n",
      "step: 1277,\t,class_loss:0.1002,\t,sm:0.79\n",
      "step: 1278,\t,class_loss:0.0988,\t,sm:0.77\n",
      "step: 1279,\t,class_loss:0.0436,\t,sm:0.78\n",
      "step: 1280,\t,class_loss:0.1719,\t,sm:0.77\n",
      "step: 1281,\t,class_loss:0.2701,\t,sm:0.76\n",
      "step: 1282,\t,class_loss:0.0549,\t,sm:0.81\n",
      "step: 1283,\t,class_loss:0.0852,\t,sm:0.83\n",
      "step: 1284,\t,class_loss:0.0827,\t,sm:0.82\n",
      "step: 1285,\t,class_loss:0.0920,\t,sm:0.84\n",
      "step: 1286,\t,class_loss:0.0522,\t,sm:0.83\n",
      "step: 1287,\t,class_loss:0.0657,\t,sm:0.76\n",
      "step: 1288,\t,class_loss:0.0754,\t,sm:0.79\n",
      "step: 1289,\t,class_loss:0.0513,\t,sm:0.72\n",
      "step: 1290,\t,class_loss:0.0301,\t,sm:0.71\n",
      "step: 1291,\t,class_loss:0.1737,\t,sm:0.61\n",
      "step: 1292,\t,class_loss:0.0783,\t,sm:0.65\n",
      "step: 1293,\t,class_loss:0.0488,\t,sm:0.68\n",
      "step: 1294,\t,class_loss:0.0917,\t,sm:0.71\n",
      "step: 1295,\t,class_loss:0.0716,\t,sm:0.75\n",
      "step: 1296,\t,class_loss:0.0430,\t,sm:0.84\n",
      "step: 1297,\t,class_loss:0.0506,\t,sm:0.70\n",
      "step: 1298,\t,class_loss:0.1018,\t,sm:0.80\n",
      "iter: 01299, \t precision: 0.8534,\t best_acc:0.8534\n",
      "step: 1299,\t,class_loss:0.1029,\t,sm:0.70\n",
      "step: 1300,\t,class_loss:0.0635,\t,sm:0.73\n",
      "step: 1301,\t,class_loss:0.0453,\t,sm:0.71\n",
      "step: 1302,\t,class_loss:0.0242,\t,sm:0.65\n",
      "step: 1303,\t,class_loss:0.0712,\t,sm:0.76\n",
      "step: 1304,\t,class_loss:0.0293,\t,sm:0.72\n",
      "step: 1305,\t,class_loss:0.1433,\t,sm:0.80\n",
      "step: 1306,\t,class_loss:0.0989,\t,sm:0.70\n",
      "step: 1307,\t,class_loss:0.0903,\t,sm:0.73\n",
      "step: 1308,\t,class_loss:0.0887,\t,sm:0.70\n",
      "step: 1309,\t,class_loss:0.0193,\t,sm:0.70\n",
      "step: 1310,\t,class_loss:0.0498,\t,sm:0.64\n",
      "step: 1311,\t,class_loss:0.1306,\t,sm:0.61\n",
      "step: 1312,\t,class_loss:0.0330,\t,sm:0.60\n",
      "step: 1313,\t,class_loss:0.1061,\t,sm:0.60\n",
      "step: 1314,\t,class_loss:0.0425,\t,sm:0.68\n",
      "step: 1315,\t,class_loss:0.0480,\t,sm:0.75\n",
      "step: 1316,\t,class_loss:0.1429,\t,sm:0.82\n",
      "step: 1317,\t,class_loss:0.0374,\t,sm:0.78\n",
      "step: 1318,\t,class_loss:0.0513,\t,sm:0.76\n",
      "step: 1319,\t,class_loss:0.0279,\t,sm:0.75\n",
      "step: 1320,\t,class_loss:0.0401,\t,sm:0.76\n",
      "step: 1321,\t,class_loss:0.1650,\t,sm:0.74\n",
      "step: 1322,\t,class_loss:0.2072,\t,sm:0.77\n",
      "step: 1323,\t,class_loss:0.0431,\t,sm:0.80\n",
      "step: 1324,\t,class_loss:0.0538,\t,sm:0.77\n",
      "step: 1325,\t,class_loss:0.0838,\t,sm:0.82\n",
      "step: 1326,\t,class_loss:0.0851,\t,sm:0.83\n",
      "step: 1327,\t,class_loss:0.0393,\t,sm:0.73\n",
      "step: 1328,\t,class_loss:0.0172,\t,sm:0.72\n",
      "step: 1329,\t,class_loss:0.0423,\t,sm:0.77\n",
      "step: 1330,\t,class_loss:0.0302,\t,sm:0.75\n",
      "step: 1331,\t,class_loss:0.1122,\t,sm:0.72\n",
      "step: 1332,\t,class_loss:0.0337,\t,sm:0.68\n",
      "step: 1333,\t,class_loss:0.0309,\t,sm:0.63\n",
      "step: 1334,\t,class_loss:0.1434,\t,sm:0.61\n",
      "step: 1335,\t,class_loss:0.0408,\t,sm:0.60\n",
      "step: 1336,\t,class_loss:0.0444,\t,sm:0.63\n",
      "step: 1337,\t,class_loss:0.0970,\t,sm:0.62\n",
      "step: 1338,\t,class_loss:0.0723,\t,sm:0.73\n",
      "step: 1339,\t,class_loss:0.0453,\t,sm:0.64\n",
      "step: 1340,\t,class_loss:0.0355,\t,sm:0.68\n",
      "step: 1341,\t,class_loss:0.0868,\t,sm:0.70\n",
      "step: 1342,\t,class_loss:0.1098,\t,sm:0.71\n",
      "step: 1343,\t,class_loss:0.0644,\t,sm:0.69\n",
      "step: 1344,\t,class_loss:0.1082,\t,sm:0.67\n",
      "step: 1345,\t,class_loss:0.0284,\t,sm:0.73\n",
      "step: 1346,\t,class_loss:0.0280,\t,sm:0.70\n",
      "step: 1347,\t,class_loss:0.1668,\t,sm:0.70\n",
      "step: 1348,\t,class_loss:0.0373,\t,sm:0.71\n",
      "iter: 01349, \t precision: 0.8574,\t best_acc:0.8574\n",
      "step: 1349,\t,class_loss:0.0800,\t,sm:0.66\n",
      "step: 1350,\t,class_loss:0.0353,\t,sm:0.68\n",
      "step: 1351,\t,class_loss:0.1377,\t,sm:0.66\n",
      "step: 1352,\t,class_loss:0.0550,\t,sm:0.65\n",
      "step: 1353,\t,class_loss:0.0229,\t,sm:0.69\n",
      "step: 1354,\t,class_loss:0.0351,\t,sm:0.64\n",
      "step: 1355,\t,class_loss:0.0751,\t,sm:0.63\n",
      "step: 1356,\t,class_loss:0.0589,\t,sm:0.70\n",
      "step: 1357,\t,class_loss:0.0387,\t,sm:0.61\n",
      "step: 1358,\t,class_loss:0.0496,\t,sm:0.65\n",
      "step: 1359,\t,class_loss:0.0471,\t,sm:0.67\n",
      "step: 1360,\t,class_loss:0.0404,\t,sm:0.64\n",
      "step: 1361,\t,class_loss:0.0671,\t,sm:0.62\n",
      "step: 1362,\t,class_loss:0.1217,\t,sm:0.65\n",
      "step: 1363,\t,class_loss:0.0589,\t,sm:0.62\n",
      "step: 1364,\t,class_loss:0.0933,\t,sm:0.58\n",
      "step: 1365,\t,class_loss:0.0385,\t,sm:0.72\n",
      "step: 1366,\t,class_loss:0.0465,\t,sm:0.75\n",
      "step: 1367,\t,class_loss:0.0881,\t,sm:0.73\n",
      "step: 1368,\t,class_loss:0.1077,\t,sm:0.78\n",
      "step: 1369,\t,class_loss:0.0361,\t,sm:0.73\n",
      "step: 1370,\t,class_loss:0.0480,\t,sm:0.67\n",
      "step: 1371,\t,class_loss:0.0558,\t,sm:0.65\n",
      "step: 1372,\t,class_loss:0.0954,\t,sm:0.66\n",
      "step: 1373,\t,class_loss:0.0470,\t,sm:0.64\n",
      "step: 1374,\t,class_loss:0.0677,\t,sm:0.59\n",
      "step: 1375,\t,class_loss:0.0581,\t,sm:0.56\n",
      "step: 1376,\t,class_loss:0.0599,\t,sm:0.55\n",
      "step: 1377,\t,class_loss:0.0661,\t,sm:0.60\n",
      "step: 1378,\t,class_loss:0.0264,\t,sm:0.58\n",
      "step: 1379,\t,class_loss:0.0390,\t,sm:0.57\n",
      "step: 1380,\t,class_loss:0.1154,\t,sm:0.55\n",
      "step: 1381,\t,class_loss:0.0459,\t,sm:0.62\n",
      "step: 1382,\t,class_loss:0.0532,\t,sm:0.67\n",
      "step: 1383,\t,class_loss:0.0408,\t,sm:0.64\n",
      "step: 1384,\t,class_loss:0.0521,\t,sm:0.71\n",
      "step: 1385,\t,class_loss:0.0618,\t,sm:0.69\n",
      "step: 1386,\t,class_loss:0.0358,\t,sm:0.70\n",
      "step: 1387,\t,class_loss:0.0659,\t,sm:0.76\n",
      "step: 1388,\t,class_loss:0.0582,\t,sm:0.66\n",
      "step: 1389,\t,class_loss:0.0501,\t,sm:0.69\n",
      "step: 1390,\t,class_loss:0.0329,\t,sm:0.58\n",
      "step: 1391,\t,class_loss:0.0881,\t,sm:0.62\n",
      "step: 1392,\t,class_loss:0.0994,\t,sm:0.60\n",
      "step: 1393,\t,class_loss:0.1277,\t,sm:0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1394,\t,class_loss:0.1954,\t,sm:0.76\n",
      "step: 1395,\t,class_loss:0.0779,\t,sm:0.70\n",
      "step: 1396,\t,class_loss:0.0811,\t,sm:0.69\n",
      "step: 1397,\t,class_loss:0.0914,\t,sm:0.73\n",
      "step: 1398,\t,class_loss:0.0758,\t,sm:0.74\n",
      "iter: 01399, \t precision: 0.8594,\t best_acc:0.8594\n",
      "step: 1399,\t,class_loss:0.0775,\t,sm:0.75\n",
      "step: 1400,\t,class_loss:0.0415,\t,sm:0.71\n",
      "step: 1401,\t,class_loss:0.1364,\t,sm:0.66\n",
      "step: 1402,\t,class_loss:0.0643,\t,sm:0.72\n",
      "step: 1403,\t,class_loss:0.0380,\t,sm:0.79\n",
      "step: 1404,\t,class_loss:0.1220,\t,sm:0.75\n",
      "step: 1405,\t,class_loss:0.0262,\t,sm:0.78\n",
      "step: 1406,\t,class_loss:0.0515,\t,sm:0.69\n",
      "step: 1407,\t,class_loss:0.0765,\t,sm:0.62\n",
      "step: 1408,\t,class_loss:0.0579,\t,sm:0.67\n",
      "step: 1409,\t,class_loss:0.0746,\t,sm:0.68\n",
      "step: 1410,\t,class_loss:0.0773,\t,sm:0.70\n",
      "step: 1411,\t,class_loss:0.1240,\t,sm:0.73\n",
      "step: 1412,\t,class_loss:0.0751,\t,sm:0.83\n",
      "step: 1413,\t,class_loss:0.0541,\t,sm:0.73\n",
      "step: 1414,\t,class_loss:0.0902,\t,sm:0.68\n",
      "step: 1415,\t,class_loss:0.0222,\t,sm:0.70\n",
      "step: 1416,\t,class_loss:0.0452,\t,sm:0.67\n",
      "step: 1417,\t,class_loss:0.0971,\t,sm:0.67\n",
      "step: 1418,\t,class_loss:0.0615,\t,sm:0.78\n",
      "step: 1419,\t,class_loss:0.0308,\t,sm:0.64\n",
      "step: 1420,\t,class_loss:0.0661,\t,sm:0.71\n",
      "step: 1421,\t,class_loss:0.0489,\t,sm:0.62\n",
      "step: 1422,\t,class_loss:0.0464,\t,sm:0.56\n",
      "step: 1423,\t,class_loss:0.0681,\t,sm:0.59\n",
      "step: 1424,\t,class_loss:0.1447,\t,sm:0.65\n",
      "step: 1425,\t,class_loss:0.0399,\t,sm:0.78\n",
      "step: 1426,\t,class_loss:0.0710,\t,sm:0.77\n",
      "step: 1427,\t,class_loss:0.1025,\t,sm:0.69\n",
      "step: 1428,\t,class_loss:0.0461,\t,sm:0.66\n",
      "step: 1429,\t,class_loss:0.0878,\t,sm:0.62\n",
      "step: 1430,\t,class_loss:0.1000,\t,sm:0.78\n",
      "step: 1431,\t,class_loss:0.0559,\t,sm:0.80\n",
      "step: 1432,\t,class_loss:0.0793,\t,sm:0.72\n",
      "step: 1433,\t,class_loss:0.0374,\t,sm:0.55\n",
      "step: 1434,\t,class_loss:0.0696,\t,sm:0.58\n",
      "step: 1435,\t,class_loss:0.0810,\t,sm:0.59\n",
      "step: 1436,\t,class_loss:0.0334,\t,sm:0.66\n",
      "step: 1437,\t,class_loss:0.0398,\t,sm:0.74\n",
      "step: 1438,\t,class_loss:0.1356,\t,sm:0.67\n",
      "step: 1439,\t,class_loss:0.0568,\t,sm:0.67\n",
      "step: 1440,\t,class_loss:0.0626,\t,sm:0.72\n",
      "step: 1441,\t,class_loss:0.0847,\t,sm:0.72\n",
      "step: 1442,\t,class_loss:0.0602,\t,sm:0.67\n",
      "step: 1443,\t,class_loss:0.0447,\t,sm:0.61\n",
      "step: 1444,\t,class_loss:0.0472,\t,sm:0.61\n",
      "step: 1445,\t,class_loss:0.0338,\t,sm:0.61\n",
      "step: 1446,\t,class_loss:0.0872,\t,sm:0.69\n",
      "step: 1447,\t,class_loss:0.0288,\t,sm:0.64\n",
      "step: 1448,\t,class_loss:0.0387,\t,sm:0.61\n",
      "iter: 01449, \t precision: 0.8614,\t best_acc:0.8614\n",
      "step: 1449,\t,class_loss:0.0412,\t,sm:0.63\n",
      "step: 1450,\t,class_loss:0.0455,\t,sm:0.63\n",
      "step: 1451,\t,class_loss:0.0559,\t,sm:0.63\n",
      "step: 1452,\t,class_loss:0.0631,\t,sm:0.59\n",
      "step: 1453,\t,class_loss:0.0269,\t,sm:0.66\n",
      "step: 1454,\t,class_loss:0.0718,\t,sm:0.60\n",
      "step: 1455,\t,class_loss:0.0465,\t,sm:0.70\n",
      "step: 1456,\t,class_loss:0.0761,\t,sm:0.70\n",
      "step: 1457,\t,class_loss:0.0399,\t,sm:0.80\n",
      "step: 1458,\t,class_loss:0.0353,\t,sm:0.73\n",
      "step: 1459,\t,class_loss:0.0259,\t,sm:0.66\n",
      "step: 1460,\t,class_loss:0.0407,\t,sm:0.67\n",
      "step: 1461,\t,class_loss:0.0556,\t,sm:0.59\n",
      "step: 1462,\t,class_loss:0.0641,\t,sm:0.68\n",
      "step: 1463,\t,class_loss:0.0844,\t,sm:0.65\n",
      "step: 1464,\t,class_loss:0.0532,\t,sm:0.64\n",
      "step: 1465,\t,class_loss:0.0290,\t,sm:0.62\n",
      "step: 1466,\t,class_loss:0.0236,\t,sm:0.60\n",
      "step: 1467,\t,class_loss:0.0346,\t,sm:0.63\n",
      "step: 1468,\t,class_loss:0.0266,\t,sm:0.62\n",
      "step: 1469,\t,class_loss:0.1000,\t,sm:0.66\n",
      "step: 1470,\t,class_loss:0.1812,\t,sm:0.73\n",
      "step: 1471,\t,class_loss:0.1224,\t,sm:0.68\n",
      "step: 1472,\t,class_loss:0.0328,\t,sm:0.64\n",
      "step: 1473,\t,class_loss:0.0391,\t,sm:0.52\n",
      "step: 1474,\t,class_loss:0.0771,\t,sm:0.50\n",
      "step: 1475,\t,class_loss:0.0226,\t,sm:0.56\n",
      "step: 1476,\t,class_loss:0.0663,\t,sm:0.67\n",
      "step: 1477,\t,class_loss:0.0403,\t,sm:0.64\n",
      "step: 1478,\t,class_loss:0.0798,\t,sm:0.68\n",
      "step: 1479,\t,class_loss:0.1978,\t,sm:0.73\n",
      "step: 1480,\t,class_loss:0.0619,\t,sm:0.70\n",
      "step: 1481,\t,class_loss:0.0537,\t,sm:0.75\n",
      "step: 1482,\t,class_loss:0.1084,\t,sm:0.77\n",
      "step: 1483,\t,class_loss:0.0527,\t,sm:0.79\n",
      "step: 1484,\t,class_loss:0.0730,\t,sm:0.75\n",
      "step: 1485,\t,class_loss:0.0662,\t,sm:0.67\n",
      "step: 1486,\t,class_loss:0.1390,\t,sm:0.68\n",
      "step: 1487,\t,class_loss:0.0832,\t,sm:0.61\n",
      "step: 1488,\t,class_loss:0.0545,\t,sm:0.69\n",
      "step: 1489,\t,class_loss:0.0644,\t,sm:0.63\n",
      "step: 1490,\t,class_loss:0.1252,\t,sm:0.67\n",
      "step: 1491,\t,class_loss:0.0358,\t,sm:0.59\n",
      "step: 1492,\t,class_loss:0.0887,\t,sm:0.51\n",
      "step: 1493,\t,class_loss:0.0801,\t,sm:0.62\n",
      "step: 1494,\t,class_loss:0.0319,\t,sm:0.68\n",
      "step: 1495,\t,class_loss:0.0244,\t,sm:0.71\n",
      "step: 1496,\t,class_loss:0.0543,\t,sm:0.66\n",
      "step: 1497,\t,class_loss:0.0781,\t,sm:0.71\n",
      "step: 1498,\t,class_loss:0.0521,\t,sm:0.69\n",
      "iter: 01499, \t precision: 0.8635,\t best_acc:0.8635\n",
      "step: 1499,\t,class_loss:0.0485,\t,sm:0.65\n",
      "step: 1500,\t,class_loss:0.0417,\t,sm:0.62\n",
      "step: 1501,\t,class_loss:0.0747,\t,sm:0.66\n",
      "step: 1502,\t,class_loss:0.0289,\t,sm:0.59\n",
      "step: 1503,\t,class_loss:0.0944,\t,sm:0.55\n",
      "step: 1504,\t,class_loss:0.0549,\t,sm:0.54\n",
      "step: 1505,\t,class_loss:0.0171,\t,sm:0.52\n",
      "step: 1506,\t,class_loss:0.0439,\t,sm:0.55\n",
      "step: 1507,\t,class_loss:0.1932,\t,sm:0.55\n",
      "step: 1508,\t,class_loss:0.0249,\t,sm:0.58\n",
      "step: 1509,\t,class_loss:0.0406,\t,sm:0.65\n",
      "step: 1510,\t,class_loss:0.0525,\t,sm:0.61\n",
      "step: 1511,\t,class_loss:0.0255,\t,sm:0.61\n",
      "step: 1512,\t,class_loss:0.0371,\t,sm:0.66\n",
      "step: 1513,\t,class_loss:0.0470,\t,sm:0.56\n",
      "step: 1514,\t,class_loss:0.0654,\t,sm:0.54\n",
      "step: 1515,\t,class_loss:0.0977,\t,sm:0.59\n",
      "step: 1516,\t,class_loss:0.0391,\t,sm:0.63\n",
      "step: 1517,\t,class_loss:0.0816,\t,sm:0.67\n",
      "step: 1518,\t,class_loss:0.1334,\t,sm:0.62\n",
      "step: 1519,\t,class_loss:0.0869,\t,sm:0.63\n",
      "step: 1520,\t,class_loss:0.1127,\t,sm:0.65\n",
      "step: 1521,\t,class_loss:0.0901,\t,sm:0.67\n",
      "step: 1522,\t,class_loss:0.0290,\t,sm:0.65\n",
      "step: 1523,\t,class_loss:0.0438,\t,sm:0.67\n",
      "step: 1524,\t,class_loss:0.0453,\t,sm:0.67\n",
      "step: 1525,\t,class_loss:0.0266,\t,sm:0.61\n",
      "step: 1526,\t,class_loss:0.0675,\t,sm:0.65\n",
      "step: 1527,\t,class_loss:0.0768,\t,sm:0.71\n",
      "step: 1528,\t,class_loss:0.0442,\t,sm:0.60\n",
      "step: 1529,\t,class_loss:0.0696,\t,sm:0.60\n",
      "step: 1530,\t,class_loss:0.0324,\t,sm:0.68\n",
      "step: 1531,\t,class_loss:0.0732,\t,sm:0.61\n",
      "step: 1532,\t,class_loss:0.1170,\t,sm:0.65\n",
      "step: 1533,\t,class_loss:0.0628,\t,sm:0.65\n",
      "step: 1534,\t,class_loss:0.0505,\t,sm:0.66\n",
      "step: 1535,\t,class_loss:0.1726,\t,sm:0.61\n",
      "step: 1536,\t,class_loss:0.0241,\t,sm:0.58\n",
      "step: 1537,\t,class_loss:0.0170,\t,sm:0.59\n",
      "step: 1538,\t,class_loss:0.0600,\t,sm:0.63\n",
      "step: 1539,\t,class_loss:0.0677,\t,sm:0.62\n",
      "step: 1540,\t,class_loss:0.0632,\t,sm:0.55\n",
      "step: 1541,\t,class_loss:0.0876,\t,sm:0.50\n",
      "step: 1542,\t,class_loss:0.0568,\t,sm:0.54\n",
      "step: 1543,\t,class_loss:0.0591,\t,sm:0.57\n",
      "step: 1544,\t,class_loss:0.0982,\t,sm:0.64\n",
      "step: 1545,\t,class_loss:0.0529,\t,sm:0.62\n",
      "step: 1546,\t,class_loss:0.0852,\t,sm:0.61\n",
      "step: 1547,\t,class_loss:0.0299,\t,sm:0.61\n",
      "step: 1548,\t,class_loss:0.0712,\t,sm:0.58\n",
      "iter: 01549, \t precision: 0.8594,\t best_acc:0.8635\n",
      "step: 1549,\t,class_loss:0.0597,\t,sm:0.59\n",
      "step: 1550,\t,class_loss:0.0885,\t,sm:0.61\n",
      "step: 1551,\t,class_loss:0.1152,\t,sm:0.56\n",
      "step: 1552,\t,class_loss:0.0243,\t,sm:0.65\n",
      "step: 1553,\t,class_loss:0.0597,\t,sm:0.59\n",
      "step: 1554,\t,class_loss:0.0689,\t,sm:0.54\n",
      "step: 1555,\t,class_loss:0.0712,\t,sm:0.57\n",
      "step: 1556,\t,class_loss:0.0752,\t,sm:0.57\n",
      "step: 1557,\t,class_loss:0.1668,\t,sm:0.55\n",
      "step: 1558,\t,class_loss:0.0894,\t,sm:0.51\n",
      "step: 1559,\t,class_loss:0.0322,\t,sm:0.55\n",
      "step: 1560,\t,class_loss:0.0361,\t,sm:0.65\n",
      "step: 1561,\t,class_loss:0.0547,\t,sm:0.67\n",
      "step: 1562,\t,class_loss:0.0729,\t,sm:0.71\n",
      "step: 1563,\t,class_loss:0.0816,\t,sm:0.76\n",
      "step: 1564,\t,class_loss:0.0424,\t,sm:0.61\n",
      "step: 1565,\t,class_loss:0.0355,\t,sm:0.59\n",
      "step: 1566,\t,class_loss:0.1218,\t,sm:0.59\n",
      "step: 1567,\t,class_loss:0.0317,\t,sm:0.55\n",
      "step: 1568,\t,class_loss:0.0312,\t,sm:0.53\n",
      "step: 1569,\t,class_loss:0.0979,\t,sm:0.67\n",
      "step: 1570,\t,class_loss:0.0444,\t,sm:0.65\n",
      "step: 1571,\t,class_loss:0.0389,\t,sm:0.67\n",
      "step: 1572,\t,class_loss:0.0249,\t,sm:0.72\n",
      "step: 1573,\t,class_loss:0.0376,\t,sm:0.70\n",
      "step: 1574,\t,class_loss:0.1094,\t,sm:0.65\n",
      "step: 1575,\t,class_loss:0.0417,\t,sm:0.67\n",
      "step: 1576,\t,class_loss:0.1212,\t,sm:0.64\n",
      "step: 1577,\t,class_loss:0.0329,\t,sm:0.64\n",
      "step: 1578,\t,class_loss:0.0307,\t,sm:0.58\n",
      "step: 1579,\t,class_loss:0.0204,\t,sm:0.56\n",
      "step: 1580,\t,class_loss:0.0498,\t,sm:0.51\n",
      "step: 1581,\t,class_loss:0.0172,\t,sm:0.56\n",
      "step: 1582,\t,class_loss:0.0401,\t,sm:0.53\n",
      "step: 1583,\t,class_loss:0.0335,\t,sm:0.52\n",
      "step: 1584,\t,class_loss:0.0389,\t,sm:0.56\n",
      "step: 1585,\t,class_loss:0.0556,\t,sm:0.56\n",
      "step: 1586,\t,class_loss:0.1385,\t,sm:0.58\n",
      "step: 1587,\t,class_loss:0.0411,\t,sm:0.63\n",
      "step: 1588,\t,class_loss:0.0218,\t,sm:0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1589,\t,class_loss:0.0393,\t,sm:0.59\n",
      "step: 1590,\t,class_loss:0.1035,\t,sm:0.60\n",
      "step: 1591,\t,class_loss:0.0235,\t,sm:0.60\n",
      "step: 1592,\t,class_loss:0.0730,\t,sm:0.61\n",
      "step: 1593,\t,class_loss:0.0155,\t,sm:0.51\n",
      "step: 1594,\t,class_loss:0.0217,\t,sm:0.47\n",
      "step: 1595,\t,class_loss:0.0354,\t,sm:0.51\n",
      "step: 1596,\t,class_loss:0.0207,\t,sm:0.53\n",
      "step: 1597,\t,class_loss:0.0443,\t,sm:0.56\n",
      "step: 1598,\t,class_loss:0.0722,\t,sm:0.59\n",
      "iter: 01599, \t precision: 0.8614,\t best_acc:0.8635\n",
      "step: 1599,\t,class_loss:0.1055,\t,sm:0.60\n",
      "step: 1600,\t,class_loss:0.0776,\t,sm:0.64\n",
      "step: 1601,\t,class_loss:0.1311,\t,sm:0.56\n",
      "step: 1602,\t,class_loss:0.1516,\t,sm:0.58\n",
      "step: 1603,\t,class_loss:0.0325,\t,sm:0.55\n",
      "step: 1604,\t,class_loss:0.0754,\t,sm:0.52\n",
      "step: 1605,\t,class_loss:0.1978,\t,sm:0.57\n",
      "step: 1606,\t,class_loss:0.0439,\t,sm:0.56\n",
      "step: 1607,\t,class_loss:0.0584,\t,sm:0.66\n",
      "step: 1608,\t,class_loss:0.0680,\t,sm:0.66\n",
      "step: 1609,\t,class_loss:0.0157,\t,sm:0.55\n",
      "step: 1610,\t,class_loss:0.0830,\t,sm:0.66\n",
      "step: 1611,\t,class_loss:0.0482,\t,sm:0.61\n",
      "step: 1612,\t,class_loss:0.1010,\t,sm:0.76\n",
      "step: 1613,\t,class_loss:0.0486,\t,sm:0.66\n",
      "step: 1614,\t,class_loss:0.0694,\t,sm:0.68\n",
      "step: 1615,\t,class_loss:0.0335,\t,sm:0.59\n",
      "step: 1616,\t,class_loss:0.0456,\t,sm:0.56\n",
      "step: 1617,\t,class_loss:0.0794,\t,sm:0.64\n",
      "step: 1618,\t,class_loss:0.1342,\t,sm:0.62\n",
      "step: 1619,\t,class_loss:0.0654,\t,sm:0.65\n",
      "step: 1620,\t,class_loss:0.0440,\t,sm:0.59\n",
      "step: 1621,\t,class_loss:0.1255,\t,sm:0.58\n",
      "step: 1622,\t,class_loss:0.0497,\t,sm:0.70\n",
      "step: 1623,\t,class_loss:0.0408,\t,sm:0.64\n",
      "step: 1624,\t,class_loss:0.0326,\t,sm:0.62\n",
      "step: 1625,\t,class_loss:0.0805,\t,sm:0.72\n",
      "step: 1626,\t,class_loss:0.0276,\t,sm:0.72\n",
      "step: 1627,\t,class_loss:0.0678,\t,sm:0.62\n",
      "step: 1628,\t,class_loss:0.0512,\t,sm:0.61\n",
      "step: 1629,\t,class_loss:0.0295,\t,sm:0.54\n",
      "step: 1630,\t,class_loss:0.0260,\t,sm:0.47\n",
      "step: 1631,\t,class_loss:0.0355,\t,sm:0.49\n",
      "step: 1632,\t,class_loss:0.0536,\t,sm:0.54\n",
      "step: 1633,\t,class_loss:0.0339,\t,sm:0.58\n",
      "step: 1634,\t,class_loss:0.0824,\t,sm:0.61\n",
      "step: 1635,\t,class_loss:0.0474,\t,sm:0.54\n",
      "step: 1636,\t,class_loss:0.0767,\t,sm:0.49\n",
      "step: 1637,\t,class_loss:0.0787,\t,sm:0.59\n",
      "step: 1638,\t,class_loss:0.0304,\t,sm:0.63\n",
      "step: 1639,\t,class_loss:0.0657,\t,sm:0.61\n",
      "step: 1640,\t,class_loss:0.1002,\t,sm:0.62\n",
      "step: 1641,\t,class_loss:0.0294,\t,sm:0.62\n",
      "step: 1642,\t,class_loss:0.0249,\t,sm:0.59\n",
      "step: 1643,\t,class_loss:0.0386,\t,sm:0.51\n",
      "step: 1644,\t,class_loss:0.0507,\t,sm:0.64\n",
      "step: 1645,\t,class_loss:0.0328,\t,sm:0.56\n",
      "step: 1646,\t,class_loss:0.0842,\t,sm:0.53\n",
      "step: 1647,\t,class_loss:0.0392,\t,sm:0.49\n",
      "step: 1648,\t,class_loss:0.0606,\t,sm:0.47\n",
      "iter: 01649, \t precision: 0.8635,\t best_acc:0.8635\n",
      "step: 1649,\t,class_loss:0.0591,\t,sm:0.53\n",
      "step: 1650,\t,class_loss:0.0527,\t,sm:0.48\n",
      "step: 1651,\t,class_loss:0.0585,\t,sm:0.61\n",
      "step: 1652,\t,class_loss:0.0277,\t,sm:0.52\n",
      "step: 1653,\t,class_loss:0.1049,\t,sm:0.60\n",
      "step: 1654,\t,class_loss:0.0431,\t,sm:0.57\n",
      "step: 1655,\t,class_loss:0.1041,\t,sm:0.61\n",
      "step: 1656,\t,class_loss:0.0499,\t,sm:0.55\n",
      "step: 1657,\t,class_loss:0.0376,\t,sm:0.56\n",
      "step: 1658,\t,class_loss:0.0436,\t,sm:0.58\n",
      "step: 1659,\t,class_loss:0.0464,\t,sm:0.58\n",
      "step: 1660,\t,class_loss:0.0627,\t,sm:0.52\n",
      "step: 1661,\t,class_loss:0.0352,\t,sm:0.48\n",
      "step: 1662,\t,class_loss:0.0266,\t,sm:0.50\n",
      "step: 1663,\t,class_loss:0.0650,\t,sm:0.51\n",
      "step: 1664,\t,class_loss:0.0262,\t,sm:0.58\n",
      "step: 1665,\t,class_loss:0.0842,\t,sm:0.54\n",
      "step: 1666,\t,class_loss:0.0360,\t,sm:0.59\n",
      "step: 1667,\t,class_loss:0.0558,\t,sm:0.67\n",
      "step: 1668,\t,class_loss:0.0566,\t,sm:0.64\n",
      "step: 1669,\t,class_loss:0.0477,\t,sm:0.62\n",
      "step: 1670,\t,class_loss:0.0482,\t,sm:0.57\n",
      "step: 1671,\t,class_loss:0.0216,\t,sm:0.53\n",
      "step: 1672,\t,class_loss:0.0195,\t,sm:0.49\n",
      "step: 1673,\t,class_loss:0.0538,\t,sm:0.51\n",
      "step: 1674,\t,class_loss:0.0608,\t,sm:0.57\n",
      "step: 1675,\t,class_loss:0.0578,\t,sm:0.58\n",
      "step: 1676,\t,class_loss:0.0470,\t,sm:0.62\n",
      "step: 1677,\t,class_loss:0.0498,\t,sm:0.59\n",
      "step: 1678,\t,class_loss:0.0297,\t,sm:0.57\n",
      "step: 1679,\t,class_loss:0.0605,\t,sm:0.64\n",
      "step: 1680,\t,class_loss:0.0968,\t,sm:0.62\n",
      "step: 1681,\t,class_loss:0.0459,\t,sm:0.52\n",
      "step: 1682,\t,class_loss:0.0797,\t,sm:0.66\n",
      "step: 1683,\t,class_loss:0.0312,\t,sm:0.61\n",
      "step: 1684,\t,class_loss:0.0341,\t,sm:0.66\n",
      "step: 1685,\t,class_loss:0.0834,\t,sm:0.64\n",
      "step: 1686,\t,class_loss:0.0750,\t,sm:0.61\n",
      "step: 1687,\t,class_loss:0.1216,\t,sm:0.63\n",
      "step: 1688,\t,class_loss:0.0312,\t,sm:0.57\n",
      "step: 1689,\t,class_loss:0.0302,\t,sm:0.58\n",
      "step: 1690,\t,class_loss:0.0710,\t,sm:0.62\n",
      "step: 1691,\t,class_loss:0.0455,\t,sm:0.62\n",
      "step: 1692,\t,class_loss:0.0656,\t,sm:0.73\n",
      "step: 1693,\t,class_loss:0.0265,\t,sm:0.66\n",
      "step: 1694,\t,class_loss:0.0938,\t,sm:0.64\n",
      "step: 1695,\t,class_loss:0.0420,\t,sm:0.53\n",
      "step: 1696,\t,class_loss:0.0340,\t,sm:0.53\n",
      "step: 1697,\t,class_loss:0.0566,\t,sm:0.53\n",
      "step: 1698,\t,class_loss:0.0564,\t,sm:0.49\n",
      "iter: 01699, \t precision: 0.8635,\t best_acc:0.8635\n",
      "step: 1699,\t,class_loss:0.0342,\t,sm:0.58\n",
      "step: 1700,\t,class_loss:0.0313,\t,sm:0.63\n",
      "step: 1701,\t,class_loss:0.0791,\t,sm:0.56\n",
      "step: 1702,\t,class_loss:0.0783,\t,sm:0.56\n",
      "step: 1703,\t,class_loss:0.1039,\t,sm:0.61\n",
      "step: 1704,\t,class_loss:0.0735,\t,sm:0.54\n",
      "step: 1705,\t,class_loss:0.0425,\t,sm:0.60\n",
      "step: 1706,\t,class_loss:0.0427,\t,sm:0.62\n",
      "step: 1707,\t,class_loss:0.0544,\t,sm:0.59\n",
      "step: 1708,\t,class_loss:0.0572,\t,sm:0.60\n",
      "step: 1709,\t,class_loss:0.0283,\t,sm:0.56\n",
      "step: 1710,\t,class_loss:0.0831,\t,sm:0.65\n",
      "step: 1711,\t,class_loss:0.0713,\t,sm:0.57\n",
      "step: 1712,\t,class_loss:0.0490,\t,sm:0.60\n",
      "step: 1713,\t,class_loss:0.0609,\t,sm:0.54\n",
      "step: 1714,\t,class_loss:0.0247,\t,sm:0.62\n",
      "step: 1715,\t,class_loss:0.0977,\t,sm:0.53\n",
      "step: 1716,\t,class_loss:0.0366,\t,sm:0.50\n",
      "step: 1717,\t,class_loss:0.1082,\t,sm:0.72\n",
      "step: 1718,\t,class_loss:0.0345,\t,sm:0.66\n",
      "step: 1719,\t,class_loss:0.0171,\t,sm:0.65\n",
      "step: 1720,\t,class_loss:0.0246,\t,sm:0.61\n",
      "step: 1721,\t,class_loss:0.0335,\t,sm:0.55\n",
      "step: 1722,\t,class_loss:0.1088,\t,sm:0.61\n",
      "step: 1723,\t,class_loss:0.0577,\t,sm:0.63\n",
      "step: 1724,\t,class_loss:0.0199,\t,sm:0.54\n",
      "step: 1725,\t,class_loss:0.0783,\t,sm:0.50\n",
      "step: 1726,\t,class_loss:0.0198,\t,sm:0.47\n",
      "step: 1727,\t,class_loss:0.0251,\t,sm:0.54\n",
      "step: 1728,\t,class_loss:0.0472,\t,sm:0.51\n",
      "step: 1729,\t,class_loss:0.0565,\t,sm:0.48\n",
      "step: 1730,\t,class_loss:0.0216,\t,sm:0.65\n",
      "step: 1731,\t,class_loss:0.0407,\t,sm:0.55\n",
      "step: 1732,\t,class_loss:0.0846,\t,sm:0.56\n",
      "step: 1733,\t,class_loss:0.0733,\t,sm:0.67\n",
      "step: 1734,\t,class_loss:0.0258,\t,sm:0.58\n",
      "step: 1735,\t,class_loss:0.0486,\t,sm:0.53\n",
      "step: 1736,\t,class_loss:0.0280,\t,sm:0.47\n",
      "step: 1737,\t,class_loss:0.1171,\t,sm:0.46\n",
      "step: 1738,\t,class_loss:0.0317,\t,sm:0.51\n",
      "step: 1739,\t,class_loss:0.0408,\t,sm:0.54\n",
      "step: 1740,\t,class_loss:0.0247,\t,sm:0.51\n",
      "step: 1741,\t,class_loss:0.0385,\t,sm:0.54\n",
      "step: 1742,\t,class_loss:0.0769,\t,sm:0.58\n",
      "step: 1743,\t,class_loss:0.0481,\t,sm:0.64\n",
      "step: 1744,\t,class_loss:0.0409,\t,sm:0.58\n",
      "step: 1745,\t,class_loss:0.0377,\t,sm:0.57\n",
      "step: 1746,\t,class_loss:0.0610,\t,sm:0.59\n",
      "step: 1747,\t,class_loss:0.0308,\t,sm:0.52\n",
      "step: 1748,\t,class_loss:0.0360,\t,sm:0.52\n",
      "iter: 01749, \t precision: 0.8594,\t best_acc:0.8635\n",
      "step: 1749,\t,class_loss:0.0296,\t,sm:0.57\n",
      "step: 1750,\t,class_loss:0.0775,\t,sm:0.54\n",
      "step: 1751,\t,class_loss:0.0400,\t,sm:0.54\n",
      "step: 1752,\t,class_loss:0.0470,\t,sm:0.54\n",
      "step: 1753,\t,class_loss:0.0287,\t,sm:0.51\n",
      "step: 1754,\t,class_loss:0.0439,\t,sm:0.50\n",
      "step: 1755,\t,class_loss:0.0512,\t,sm:0.54\n",
      "step: 1756,\t,class_loss:0.0305,\t,sm:0.63\n",
      "step: 1757,\t,class_loss:0.0388,\t,sm:0.59\n",
      "step: 1758,\t,class_loss:0.0244,\t,sm:0.56\n",
      "step: 1759,\t,class_loss:0.0540,\t,sm:0.57\n",
      "step: 1760,\t,class_loss:0.0407,\t,sm:0.53\n",
      "step: 1761,\t,class_loss:0.0144,\t,sm:0.53\n",
      "step: 1762,\t,class_loss:0.0672,\t,sm:0.62\n",
      "step: 1763,\t,class_loss:0.0349,\t,sm:0.50\n",
      "step: 1764,\t,class_loss:0.0201,\t,sm:0.44\n",
      "step: 1765,\t,class_loss:0.0290,\t,sm:0.50\n",
      "step: 1766,\t,class_loss:0.0451,\t,sm:0.55\n",
      "step: 1767,\t,class_loss:0.0455,\t,sm:0.57\n",
      "step: 1768,\t,class_loss:0.0646,\t,sm:0.59\n",
      "step: 1769,\t,class_loss:0.0428,\t,sm:0.61\n",
      "step: 1770,\t,class_loss:0.0370,\t,sm:0.59\n",
      "step: 1771,\t,class_loss:0.0555,\t,sm:0.71\n",
      "step: 1772,\t,class_loss:0.0396,\t,sm:0.69\n",
      "step: 1773,\t,class_loss:0.0412,\t,sm:0.62\n",
      "step: 1774,\t,class_loss:0.0193,\t,sm:0.59\n",
      "step: 1775,\t,class_loss:0.0586,\t,sm:0.55\n",
      "step: 1776,\t,class_loss:0.0151,\t,sm:0.51\n",
      "step: 1777,\t,class_loss:0.0411,\t,sm:0.55\n",
      "step: 1778,\t,class_loss:0.0628,\t,sm:0.51\n",
      "step: 1779,\t,class_loss:0.1262,\t,sm:0.50\n",
      "step: 1780,\t,class_loss:0.0524,\t,sm:0.52\n",
      "step: 1781,\t,class_loss:0.1172,\t,sm:0.61\n",
      "step: 1782,\t,class_loss:0.0713,\t,sm:0.55\n",
      "step: 1783,\t,class_loss:0.0413,\t,sm:0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1784,\t,class_loss:0.0150,\t,sm:0.57\n",
      "step: 1785,\t,class_loss:0.0452,\t,sm:0.56\n",
      "step: 1786,\t,class_loss:0.0263,\t,sm:0.62\n",
      "step: 1787,\t,class_loss:0.0624,\t,sm:0.61\n",
      "step: 1788,\t,class_loss:0.0118,\t,sm:0.69\n",
      "step: 1789,\t,class_loss:0.0666,\t,sm:0.57\n",
      "step: 1790,\t,class_loss:0.1742,\t,sm:0.57\n",
      "step: 1791,\t,class_loss:0.0730,\t,sm:0.51\n",
      "step: 1792,\t,class_loss:0.0267,\t,sm:0.51\n",
      "step: 1793,\t,class_loss:0.0490,\t,sm:0.50\n",
      "step: 1794,\t,class_loss:0.0353,\t,sm:0.64\n",
      "step: 1795,\t,class_loss:0.0347,\t,sm:0.59\n",
      "step: 1796,\t,class_loss:0.0382,\t,sm:0.58\n",
      "step: 1797,\t,class_loss:0.0412,\t,sm:0.59\n",
      "step: 1798,\t,class_loss:0.0525,\t,sm:0.56\n",
      "iter: 01799, \t precision: 0.8594,\t best_acc:0.8635\n",
      "step: 1799,\t,class_loss:0.0260,\t,sm:0.55\n",
      "step: 1800,\t,class_loss:0.0957,\t,sm:0.59\n",
      "step: 1801,\t,class_loss:0.0347,\t,sm:0.54\n",
      "step: 1802,\t,class_loss:0.0352,\t,sm:0.48\n",
      "step: 1803,\t,class_loss:0.0374,\t,sm:0.51\n",
      "step: 1804,\t,class_loss:0.0505,\t,sm:0.52\n",
      "step: 1805,\t,class_loss:0.0735,\t,sm:0.55\n",
      "step: 1806,\t,class_loss:0.0986,\t,sm:0.59\n",
      "step: 1807,\t,class_loss:0.0220,\t,sm:0.62\n",
      "step: 1808,\t,class_loss:0.0411,\t,sm:0.61\n",
      "step: 1809,\t,class_loss:0.1463,\t,sm:0.62\n",
      "step: 1810,\t,class_loss:0.0235,\t,sm:0.59\n",
      "step: 1811,\t,class_loss:0.0333,\t,sm:0.51\n",
      "step: 1812,\t,class_loss:0.1330,\t,sm:0.49\n",
      "step: 1813,\t,class_loss:0.1097,\t,sm:0.52\n",
      "step: 1814,\t,class_loss:0.0564,\t,sm:0.54\n",
      "step: 1815,\t,class_loss:0.0140,\t,sm:0.53\n",
      "step: 1816,\t,class_loss:0.0187,\t,sm:0.47\n",
      "step: 1817,\t,class_loss:0.0519,\t,sm:0.54\n",
      "step: 1818,\t,class_loss:0.0649,\t,sm:0.55\n",
      "step: 1819,\t,class_loss:0.0532,\t,sm:0.52\n",
      "step: 1820,\t,class_loss:0.0504,\t,sm:0.53\n",
      "step: 1821,\t,class_loss:0.0685,\t,sm:0.60\n",
      "step: 1822,\t,class_loss:0.0333,\t,sm:0.55\n",
      "step: 1823,\t,class_loss:0.0313,\t,sm:0.57\n",
      "step: 1824,\t,class_loss:0.0417,\t,sm:0.51\n",
      "step: 1825,\t,class_loss:0.0175,\t,sm:0.50\n",
      "step: 1826,\t,class_loss:0.0270,\t,sm:0.50\n",
      "step: 1827,\t,class_loss:0.1527,\t,sm:0.53\n",
      "step: 1828,\t,class_loss:0.0364,\t,sm:0.43\n",
      "step: 1829,\t,class_loss:0.0525,\t,sm:0.51\n",
      "step: 1830,\t,class_loss:0.0415,\t,sm:0.46\n",
      "step: 1831,\t,class_loss:0.0160,\t,sm:0.42\n",
      "step: 1832,\t,class_loss:0.0462,\t,sm:0.50\n",
      "step: 1833,\t,class_loss:0.1097,\t,sm:0.52\n",
      "step: 1834,\t,class_loss:0.0609,\t,sm:0.54\n",
      "step: 1835,\t,class_loss:0.0349,\t,sm:0.52\n",
      "step: 1836,\t,class_loss:0.0616,\t,sm:0.55\n",
      "step: 1837,\t,class_loss:0.0515,\t,sm:0.54\n",
      "step: 1838,\t,class_loss:0.0417,\t,sm:0.54\n",
      "step: 1839,\t,class_loss:0.0561,\t,sm:0.54\n",
      "step: 1840,\t,class_loss:0.0251,\t,sm:0.57\n",
      "step: 1841,\t,class_loss:0.0662,\t,sm:0.57\n",
      "step: 1842,\t,class_loss:0.0387,\t,sm:0.50\n",
      "step: 1843,\t,class_loss:0.0600,\t,sm:0.51\n",
      "step: 1844,\t,class_loss:0.0382,\t,sm:0.48\n",
      "step: 1845,\t,class_loss:0.0641,\t,sm:0.43\n",
      "step: 1846,\t,class_loss:0.0399,\t,sm:0.48\n",
      "step: 1847,\t,class_loss:0.0383,\t,sm:0.56\n",
      "step: 1848,\t,class_loss:0.0332,\t,sm:0.53\n",
      "iter: 01849, \t precision: 0.8574,\t best_acc:0.8635\n",
      "step: 1849,\t,class_loss:0.1017,\t,sm:0.51\n",
      "step: 1850,\t,class_loss:0.0122,\t,sm:0.54\n",
      "step: 1851,\t,class_loss:0.0216,\t,sm:0.57\n",
      "step: 1852,\t,class_loss:0.0296,\t,sm:0.50\n",
      "step: 1853,\t,class_loss:0.0313,\t,sm:0.51\n",
      "step: 1854,\t,class_loss:0.0147,\t,sm:0.49\n",
      "step: 1855,\t,class_loss:0.0563,\t,sm:0.54\n",
      "step: 1856,\t,class_loss:0.0231,\t,sm:0.50\n",
      "step: 1857,\t,class_loss:0.0218,\t,sm:0.56\n",
      "step: 1858,\t,class_loss:0.0545,\t,sm:0.54\n",
      "step: 1859,\t,class_loss:0.0290,\t,sm:0.55\n",
      "step: 1860,\t,class_loss:0.0825,\t,sm:0.53\n",
      "step: 1861,\t,class_loss:0.0318,\t,sm:0.48\n",
      "step: 1862,\t,class_loss:0.0580,\t,sm:0.48\n",
      "step: 1863,\t,class_loss:0.0740,\t,sm:0.52\n",
      "step: 1864,\t,class_loss:0.0308,\t,sm:0.54\n",
      "step: 1865,\t,class_loss:0.0179,\t,sm:0.51\n",
      "step: 1866,\t,class_loss:0.0532,\t,sm:0.49\n",
      "step: 1867,\t,class_loss:0.0311,\t,sm:0.46\n",
      "step: 1868,\t,class_loss:0.0542,\t,sm:0.46\n",
      "step: 1869,\t,class_loss:0.0437,\t,sm:0.45\n",
      "step: 1870,\t,class_loss:0.0973,\t,sm:0.48\n",
      "step: 1871,\t,class_loss:0.0599,\t,sm:0.48\n",
      "step: 1872,\t,class_loss:0.0225,\t,sm:0.55\n",
      "step: 1873,\t,class_loss:0.0733,\t,sm:0.56\n",
      "step: 1874,\t,class_loss:0.0176,\t,sm:0.49\n",
      "step: 1875,\t,class_loss:0.1038,\t,sm:0.53\n",
      "step: 1876,\t,class_loss:0.0354,\t,sm:0.52\n",
      "step: 1877,\t,class_loss:0.0423,\t,sm:0.55\n",
      "step: 1878,\t,class_loss:0.0506,\t,sm:0.52\n",
      "step: 1879,\t,class_loss:0.0432,\t,sm:0.50\n",
      "step: 1880,\t,class_loss:0.0261,\t,sm:0.52\n",
      "step: 1881,\t,class_loss:0.0784,\t,sm:0.54\n",
      "step: 1882,\t,class_loss:0.0280,\t,sm:0.51\n",
      "step: 1883,\t,class_loss:0.0790,\t,sm:0.50\n",
      "step: 1884,\t,class_loss:0.0267,\t,sm:0.48\n",
      "step: 1885,\t,class_loss:0.0411,\t,sm:0.50\n",
      "step: 1886,\t,class_loss:0.0895,\t,sm:0.58\n",
      "step: 1887,\t,class_loss:0.0283,\t,sm:0.53\n",
      "step: 1888,\t,class_loss:0.0392,\t,sm:0.48\n",
      "step: 1889,\t,class_loss:0.0868,\t,sm:0.50\n",
      "step: 1890,\t,class_loss:0.0213,\t,sm:0.54\n",
      "step: 1891,\t,class_loss:0.0633,\t,sm:0.50\n",
      "step: 1892,\t,class_loss:0.0349,\t,sm:0.50\n",
      "step: 1893,\t,class_loss:0.0247,\t,sm:0.50\n",
      "step: 1894,\t,class_loss:0.0136,\t,sm:0.50\n",
      "step: 1895,\t,class_loss:0.0257,\t,sm:0.47\n",
      "step: 1896,\t,class_loss:0.0584,\t,sm:0.44\n",
      "step: 1897,\t,class_loss:0.0597,\t,sm:0.49\n",
      "step: 1898,\t,class_loss:0.0221,\t,sm:0.47\n",
      "iter: 01899, \t precision: 0.8594,\t best_acc:0.8635\n",
      "step: 1899,\t,class_loss:0.0577,\t,sm:0.40\n",
      "step: 1900,\t,class_loss:0.0448,\t,sm:0.44\n",
      "step: 1901,\t,class_loss:0.0318,\t,sm:0.49\n",
      "step: 1902,\t,class_loss:0.0184,\t,sm:0.49\n",
      "step: 1903,\t,class_loss:0.0378,\t,sm:0.49\n",
      "step: 1904,\t,class_loss:0.0357,\t,sm:0.47\n",
      "step: 1905,\t,class_loss:0.0245,\t,sm:0.52\n",
      "step: 1906,\t,class_loss:0.1533,\t,sm:0.54\n",
      "step: 1907,\t,class_loss:0.0548,\t,sm:0.51\n",
      "step: 1908,\t,class_loss:0.0397,\t,sm:0.54\n",
      "step: 1909,\t,class_loss:0.0342,\t,sm:0.49\n",
      "step: 1910,\t,class_loss:0.0210,\t,sm:0.50\n",
      "step: 1911,\t,class_loss:0.0339,\t,sm:0.48\n",
      "step: 1912,\t,class_loss:0.0223,\t,sm:0.45\n",
      "step: 1913,\t,class_loss:0.0188,\t,sm:0.43\n",
      "step: 1914,\t,class_loss:0.0606,\t,sm:0.46\n",
      "step: 1915,\t,class_loss:0.0383,\t,sm:0.44\n",
      "step: 1916,\t,class_loss:0.0259,\t,sm:0.44\n",
      "step: 1917,\t,class_loss:0.0344,\t,sm:0.44\n",
      "step: 1918,\t,class_loss:0.0312,\t,sm:0.48\n",
      "step: 1919,\t,class_loss:0.0602,\t,sm:0.46\n",
      "step: 1920,\t,class_loss:0.0712,\t,sm:0.47\n",
      "step: 1921,\t,class_loss:0.1068,\t,sm:0.56\n",
      "step: 1922,\t,class_loss:0.0289,\t,sm:0.49\n",
      "step: 1923,\t,class_loss:0.0387,\t,sm:0.47\n",
      "step: 1924,\t,class_loss:0.0148,\t,sm:0.47\n",
      "step: 1925,\t,class_loss:0.0277,\t,sm:0.50\n",
      "step: 1926,\t,class_loss:0.0763,\t,sm:0.49\n",
      "step: 1927,\t,class_loss:0.0350,\t,sm:0.46\n",
      "step: 1928,\t,class_loss:0.0387,\t,sm:0.44\n",
      "step: 1929,\t,class_loss:0.0566,\t,sm:0.52\n",
      "step: 1930,\t,class_loss:0.0157,\t,sm:0.56\n",
      "step: 1931,\t,class_loss:0.0243,\t,sm:0.48\n",
      "step: 1932,\t,class_loss:0.0534,\t,sm:0.50\n",
      "step: 1933,\t,class_loss:0.0761,\t,sm:0.42\n",
      "step: 1934,\t,class_loss:0.0361,\t,sm:0.46\n",
      "step: 1935,\t,class_loss:0.0774,\t,sm:0.58\n",
      "step: 1936,\t,class_loss:0.0260,\t,sm:0.59\n",
      "step: 1937,\t,class_loss:0.0509,\t,sm:0.46\n",
      "step: 1938,\t,class_loss:0.0239,\t,sm:0.42\n",
      "step: 1939,\t,class_loss:0.0425,\t,sm:0.48\n",
      "step: 1940,\t,class_loss:0.1280,\t,sm:0.49\n",
      "step: 1941,\t,class_loss:0.0366,\t,sm:0.49\n",
      "step: 1942,\t,class_loss:0.0632,\t,sm:0.50\n",
      "step: 1943,\t,class_loss:0.0932,\t,sm:0.53\n",
      "step: 1944,\t,class_loss:0.0582,\t,sm:0.49\n",
      "step: 1945,\t,class_loss:0.1406,\t,sm:0.49\n",
      "step: 1946,\t,class_loss:0.0336,\t,sm:0.46\n",
      "step: 1947,\t,class_loss:0.0203,\t,sm:0.52\n",
      "step: 1948,\t,class_loss:0.0358,\t,sm:0.55\n",
      "iter: 01949, \t precision: 0.8614,\t best_acc:0.8635\n",
      "step: 1949,\t,class_loss:0.0307,\t,sm:0.50\n",
      "step: 1950,\t,class_loss:0.0228,\t,sm:0.48\n",
      "step: 1951,\t,class_loss:0.0260,\t,sm:0.49\n",
      "step: 1952,\t,class_loss:0.0641,\t,sm:0.54\n",
      "step: 1953,\t,class_loss:0.0169,\t,sm:0.46\n",
      "step: 1954,\t,class_loss:0.0185,\t,sm:0.41\n",
      "step: 1955,\t,class_loss:0.0209,\t,sm:0.40\n",
      "step: 1956,\t,class_loss:0.0156,\t,sm:0.44\n",
      "step: 1957,\t,class_loss:0.0139,\t,sm:0.48\n",
      "step: 1958,\t,class_loss:0.0396,\t,sm:0.50\n",
      "step: 1959,\t,class_loss:0.0331,\t,sm:0.48\n",
      "step: 1960,\t,class_loss:0.0732,\t,sm:0.48\n",
      "step: 1961,\t,class_loss:0.0308,\t,sm:0.50\n",
      "step: 1962,\t,class_loss:0.0238,\t,sm:0.48\n",
      "step: 1963,\t,class_loss:0.0315,\t,sm:0.48\n",
      "step: 1964,\t,class_loss:0.0625,\t,sm:0.57\n",
      "step: 1965,\t,class_loss:0.0607,\t,sm:0.60\n",
      "step: 1966,\t,class_loss:0.0229,\t,sm:0.53\n",
      "step: 1967,\t,class_loss:0.0512,\t,sm:0.49\n",
      "step: 1968,\t,class_loss:0.0930,\t,sm:0.45\n",
      "step: 1969,\t,class_loss:0.0528,\t,sm:0.41\n",
      "step: 1970,\t,class_loss:0.0310,\t,sm:0.43\n",
      "step: 1971,\t,class_loss:0.0140,\t,sm:0.46\n",
      "step: 1972,\t,class_loss:0.0211,\t,sm:0.44\n",
      "step: 1973,\t,class_loss:0.0245,\t,sm:0.47\n",
      "step: 1974,\t,class_loss:0.0161,\t,sm:0.45\n",
      "step: 1975,\t,class_loss:0.0246,\t,sm:0.46\n",
      "step: 1976,\t,class_loss:0.0811,\t,sm:0.51\n",
      "step: 1977,\t,class_loss:0.0153,\t,sm:0.48\n",
      "step: 1978,\t,class_loss:0.0244,\t,sm:0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1979,\t,class_loss:0.1487,\t,sm:0.53\n",
      "step: 1980,\t,class_loss:0.0366,\t,sm:0.44\n",
      "step: 1981,\t,class_loss:0.0747,\t,sm:0.51\n",
      "step: 1982,\t,class_loss:0.0436,\t,sm:0.49\n",
      "step: 1983,\t,class_loss:0.0478,\t,sm:0.47\n",
      "step: 1984,\t,class_loss:0.0143,\t,sm:0.41\n",
      "step: 1985,\t,class_loss:0.0297,\t,sm:0.39\n",
      "step: 1986,\t,class_loss:0.0208,\t,sm:0.36\n",
      "step: 1987,\t,class_loss:0.0556,\t,sm:0.43\n",
      "step: 1988,\t,class_loss:0.0620,\t,sm:0.54\n",
      "step: 1989,\t,class_loss:0.0320,\t,sm:0.63\n",
      "step: 1990,\t,class_loss:0.1071,\t,sm:0.61\n",
      "step: 1991,\t,class_loss:0.0536,\t,sm:0.62\n",
      "step: 1992,\t,class_loss:0.0519,\t,sm:0.49\n",
      "step: 1993,\t,class_loss:0.0305,\t,sm:0.52\n",
      "step: 1994,\t,class_loss:0.0213,\t,sm:0.52\n",
      "step: 1995,\t,class_loss:0.0393,\t,sm:0.52\n",
      "step: 1996,\t,class_loss:0.0430,\t,sm:0.56\n",
      "step: 1997,\t,class_loss:0.0506,\t,sm:0.52\n",
      "step: 1998,\t,class_loss:0.0610,\t,sm:0.48\n",
      "iter: 01999, \t precision: 0.8614,\t best_acc:0.8635\n",
      "step: 1999,\t,class_loss:0.0365,\t,sm:0.43\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import network\n",
    "import loss\n",
    "import pre_process as prep\n",
    "import lr_schedule\n",
    "from pre_process import ImageList, image_classification_test\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Code for RSDA-MSTN')\n",
    "    parser.add_argument('--gpu_id', type=str, nargs='?', default='0', help=\"device id to run\")\n",
    "    parser.add_argument('--source', type=str, default='amazon',choices=[\"amazon\", \"dslr\",\"webcam\"])\n",
    "    parser.add_argument('--target', type=str, default='dslr', choices=[\"amazon\", \"dslr\", \"webcam\"])\n",
    "    parser.add_argument('--test_interval', type=int, default=50, help=\"interval of two continuous test phase\")\n",
    "    parser.add_argument('--snapshot_interval', type=int, default=1000, help=\"interval of two continuous output model\")\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help=\"learning rate\")\n",
    "    parser.add_argument('--stages', type=int, default=6, help=\"training stages\")\n",
    "    args = parser.parse_args([])\n",
    "    s_dset_path = 'data/office/' + args.source + '_list.txt' #'../../data/office/' + args.source + '_list.txt'\n",
    "    t_dset_path = 'data/office/' + args.target + '_list.txt' #'../../data/office/' + args.target + '_list.txt'\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
    "    config = {}\n",
    "    config[\"source\"] = args.source\n",
    "    config[\"target\"] = args.target\n",
    "    config[\"gpu\"] = args.gpu_id\n",
    "    config[\"test_interval\"] = args.test_interval\n",
    "    config[\"snapshot_interval\"] = args.snapshot_interval\n",
    "    config[\"output_for_test\"] = True\n",
    "    config[\"output_path\"] = \"snapshot/init\"\n",
    "    if not osp.exists(config[\"output_path\"]):\n",
    "        os.makedirs(config[\"output_path\"])\n",
    "    config[\"out_file\"] = open(osp.join(config[\"output_path\"],args.source+\"_\"+args.target+ \"no_transfer_log.txt\"), \"w\")\n",
    "\n",
    "    config[\"prep\"] = {'params':{\"resize_size\":256, \"crop_size\":224}}\n",
    "    config[\"network\"] = {\"name\":network.ResNet50, \\\n",
    "            \"params\":{\"new_cls\":True,\"feature_dim\":256,\"class_num\":31} }\n",
    "    config[\"optimizer\"] = {\"type\":optim.SGD, \"optim_params\":{'lr':args.lr, \"momentum\":0.9, \\\n",
    "                           \"weight_decay\":0.0005, \"nesterov\":True}, \"lr_type\":\"inv\", \\\n",
    "                           \"lr_param\":{\"lr\":args.lr, \"gamma\":0.001, \"power\":0.75} }\n",
    "    config[\"data\"] = {\"source\":{\"list_path\":s_dset_path, \"batch_size\":36}, \\\n",
    "                      \"target\":{\"list_path\":t_dset_path, \"batch_size\":36}, \\\n",
    "                      \"test\":{\"list_path\":t_dset_path, \"batch_size\":72}}\n",
    "    config[\"out_file\"].flush()\n",
    "    if config[\"source\"] == \"amazon\" and config[\"target\"] == \"dslr\":\n",
    "        config[\"iterations\"] = 2000\n",
    "        seed = 0\n",
    "    elif config[\"source\"] == \"amazon\" and config[\"target\"] == \"webcam\":\n",
    "        config[\"iterations\"] = 2000\n",
    "        seed = 0\n",
    "    else:\n",
    "        config[\"iterations\"] = 4000\n",
    "        seed = 1\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    config[\"out_file\"].write('\\n--- initialization ---\\n')\n",
    "    source = config[\"source\"]\n",
    "    target = config[\"target\"]\n",
    "    prep_dict = {}\n",
    "    prep_dict[\"source\"] = prep.image_train(**config[\"prep\"]['params'])\n",
    "    prep_dict[\"target\"] = prep.image_train(**config[\"prep\"]['params'])\n",
    "\n",
    "    prep_dict[\"test\"] = prep.image_test(**config[\"prep\"]['params'])\n",
    "\n",
    "    ## prepare data\n",
    "    dsets = {}\n",
    "    dset_loaders = {}\n",
    "    data_config = config[\"data\"]\n",
    "    train_bs = data_config[\"source\"][\"batch_size\"]\n",
    "    test_bs = data_config[\"test\"][\"batch_size\"]\n",
    "    dsets[\"source\"] = ImageList(open(data_config[\"source\"][\"list_path\"]).readlines(), \\\n",
    "                                transform=prep_dict[\"source\"])\n",
    "    dset_loaders[\"source\"] = DataLoader(dsets[\"source\"], batch_size=train_bs, \\\n",
    "                                        shuffle=True, num_workers=0, drop_last=True)\n",
    "    dsets[\"target\"] = ImageList(open(data_config[\"target\"][\"list_path\"]).readlines(), \\\n",
    "                                transform=prep_dict[\"target\"])\n",
    "    dset_loaders[\"target\"] = DataLoader(dsets[\"target\"], batch_size=train_bs, \\\n",
    "                                        shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "    dsets[\"test\"] = ImageList(open(data_config[\"test\"][\"list_path\"]).readlines(), \\\n",
    "                              transform=prep_dict[\"test\"])\n",
    "    dset_loaders[\"test\"] = DataLoader(dsets[\"test\"], batch_size=test_bs, \\\n",
    "                                      shuffle=False, num_workers=0)\n",
    "\n",
    "    class_num = config[\"network\"][\"params\"][\"class_num\"]\n",
    "\n",
    "    ## set base network\n",
    "    net_config = config[\"network\"]\n",
    "    base_network = net_config[\"name\"](**net_config[\"params\"])\n",
    "    base_network = base_network.cuda()\n",
    "\n",
    "    ## add additional network for some methods\n",
    "    ad_net = network.AdversarialNetwork(base_network.output_num(), 1024)\n",
    "    ad_net = ad_net.cuda()\n",
    "\n",
    "    gpus = config['gpu'].split(',')\n",
    "    if len(gpus) > 1:\n",
    "        ad_net = nn.DataParallel(ad_net)\n",
    "        base_network = nn.DataParallel(base_network)\n",
    "\n",
    "    parameter_classifier = [base_network.get_parameters()[1]]\n",
    "    parameter_feature = base_network.get_parameters()[0:1] + ad_net.get_parameters()\n",
    "\n",
    "    ## set optimizer\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer_classfier = optimizer_config[\"type\"](parameter_classifier, \\\n",
    "                                                   **(optimizer_config[\"optim_params\"]))\n",
    "    optimizer_feature = optimizer_config[\"type\"](parameter_feature, \\\n",
    "                                                 **(optimizer_config[\"optim_params\"]))\n",
    "    param_lr = []\n",
    "    for param_group in optimizer_feature.param_groups:\n",
    "        param_lr.append(param_group[\"lr\"])\n",
    "    param_lr.append(optimizer_classfier.param_groups[0][\"lr\"])\n",
    "    schedule_param = optimizer_config[\"lr_param\"]\n",
    "    lr_scheduler = lr_schedule.schedule_dict[optimizer_config[\"lr_type\"]]\n",
    "\n",
    "    ## train\n",
    "    len_train_source = len(dset_loaders[\"source\"])\n",
    "    len_train_target = len(dset_loaders[\"target\"])\n",
    "    best_acc = 0.0\n",
    "    best_model = copy.deepcopy(base_network)\n",
    "\n",
    "    Cs_memory=torch.zeros(class_num,256).cuda()\n",
    "    Ct_memory=torch.zeros(class_num,256).cuda()\n",
    "\n",
    "\n",
    "    for i in range(config[\"iterations\"]):\n",
    "        if i % config[\"test_interval\"] == config[\"test_interval\"] - 1:\n",
    "            base_network.train(False)\n",
    "            temp_acc = image_classification_test(dset_loaders,base_network)\n",
    "            temp_model = base_network\n",
    "            if temp_acc > best_acc:\n",
    "                best_acc = temp_acc\n",
    "                best_model = copy.deepcopy(temp_model)\n",
    "            log_str = \"iter: {:05d}, \\t precision: {:.4f},\\t best_acc:{:.4f}\".format(i, temp_acc, best_acc)\n",
    "            config[\"out_file\"].write(log_str + \"\\n\")\n",
    "            config[\"out_file\"].flush()\n",
    "            print(log_str)\n",
    "        if (i + 1) % config[\"snapshot_interval\"] == 0:\n",
    "            if not os.path.exists(\"save/init_model\"):\n",
    "                os.makedirs(\"save/init_model\")\n",
    "            torch.save(best_model, 'save/init_model/' + source + '_' + target + 'no_transfer.pkl')\n",
    "\n",
    "        ## train one iter\n",
    "        base_network.train(True)\n",
    "        ad_net.train(True)\n",
    "        optimizer_classfier = lr_scheduler(optimizer_classfier, i, **schedule_param)\n",
    "        optimizer_feature = lr_scheduler(optimizer_feature, i, **schedule_param)\n",
    "\n",
    "        if i % len_train_source == 0:\n",
    "            iter_source = iter(dset_loaders[\"source\"])\n",
    "        if i % len_train_target == 0:\n",
    "            iter_target = iter(dset_loaders[\"target\"])\n",
    "        inputs_source, labels_source = iter_source.next()\n",
    "        inputs_target, labels_target = iter_target.next()\n",
    "        inputs_source, inputs_target, labels_source = inputs_source.cuda(), inputs_target.cuda(), labels_source.cuda()\n",
    "        features_source, outputs_source = base_network(inputs_source)\n",
    "        features_target, outputs_target = base_network(inputs_target)\n",
    "        pseu_labels_target=torch.argmax(outputs_target,dim=1)\n",
    "        \n",
    "        loss_sm,Cs_memory,Ct_memory=loss.SM(features_source,features_target,labels_source,pseu_labels_target,\n",
    "                                            Cs_memory,Ct_memory)\n",
    "        gamma=network.calc_coeff(i)\n",
    "        classifier_loss = nn.CrossEntropyLoss()(outputs_source, labels_source)\n",
    "\n",
    "        loss_total = classifier_loss  + gamma * (loss_sm)\n",
    "\n",
    "        optimizer_classfier.zero_grad()\n",
    "        optimizer_feature.zero_grad()\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer_feature.step()\n",
    "        optimizer_classfier.step()\n",
    "\n",
    "        print('step:{: d},\\t,class_loss:{:.4f},\\t,sm:{:.2f}'\n",
    "              ''.format(i, classifier_loss.item(),loss_sm.item()))\n",
    "        Cs_memory.detach_()\n",
    "        Ct_memory.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
